{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3e55c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul  7 16:06:12 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.129.06   Driver Version: 470.129.06   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| 90%   24C    P8    27W / 350W |      7MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:81:00.0 Off |                  N/A |\n",
      "| 90%   28C    P8    26W / 350W |      7MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  On   | 00000000:82:00.0 Off |                  N/A |\n",
      "| 90%   25C    P8    19W / 350W |      7MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  On   | 00000000:C1:00.0 Off |                  N/A |\n",
      "| 90%   25C    P8    23W / 350W |     19MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d1eebb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.6.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
      "\u001b[K     |████████████████████████████████| 749 kB 53.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 24.2 MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.12.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 53.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.8)\n",
      "Installing collected packages: tokenizers, regex, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.8.1 regex-2022.6.2 tokenizers-0.12.1 transformers-4.20.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Cloning into 'SimKGC'...\n",
      "remote: Enumerating objects: 156, done.\u001b[K\n",
      "remote: Counting objects: 100% (156/156), done.\u001b[K\n",
      "remote: Compressing objects: 100% (101/101), done.\u001b[K\n",
      "remote: Total 156 (delta 81), reused 129 (delta 54), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (156/156), 15.67 MiB | 20.06 MiB/s, done.\n",
      "Resolving deltas: 100% (81/81), done.\n",
      "Cloning into 'CAKE'...\n",
      "remote: Enumerating objects: 110, done.\u001b[K\n",
      "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
      "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
      "remote: Total 110 (delta 0), reused 0 (delta 0), pack-reused 107\u001b[K\n",
      "Receiving objects: 100% (110/110), 51.26 MiB | 26.04 MiB/s, done.\n",
      "Resolving deltas: 100% (23/23), done.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "# !pip install -U adapter-transformers\n",
    "!git clone https://github.com/intfloat/SimKGC.git\n",
    "!git clone https://github.com/ngl567/CAKE.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22224224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/SimKGC\n",
      "+ set -e\n",
      "+ TASK=WN18RR\n",
      "+ [[ 1 -ge 1 ]]\n",
      "+ TASK=FB15k237\n",
      "+ shift\n",
      "+ python3 -u preprocess.py --task FB15k237 --train-path ./data/FB15k237/train.txt --valid-path ./data/FB15k237/valid.txt --test-path ./data/FB15k237/test.txt\n",
      "Process ./data/FB15k237/train.txt...\n",
      "Load 14904 entity descriptions from ./data/FB15k237/FB15k_mid2description.txt\n",
      "No desc found for /m/02vxfw_\n",
      "No desc found for /m/02jxk\n",
      "No desc found for /m/03m3nzf\n",
      "No desc found for /m/04_1l0v\n",
      "No desc found for /m/09x_r\n",
      "No desc found for /m/0bytsc\n",
      "No desc found for /m/07_bv_\n",
      "No desc found for /m/03lsz8h\n",
      "No desc found for /m/05xf75\n",
      "No desc found for /m/01dy7j\n",
      "No desc found for /m/09ly2r6\n",
      "No desc found for /m/015zql\n",
      "No desc found for /m/047vp20\n",
      "No desc found for /m/0hk18\n",
      "No desc found for /m/061zc_\n",
      "No desc found for /m/0cfywh\n",
      "No desc found for /m/03tp4\n",
      "No desc found for /m/029cpw\n",
      "No desc found for /m/0lmb5\n",
      "No desc found for /m/0m6x4\n",
      "No desc found for /m/09l65\n",
      "No desc found for /m/0147fv\n",
      "No desc found for /m/0kvrb\n",
      "No desc found for /m/03gwg4w\n",
      "No desc found for /m/0bm39zf\n",
      "No desc found for /m/08mbj32\n",
      "No desc found for /m/068bs\n",
      "No desc found for /m/01xzb6\n",
      "No desc found for /m/07djnx\n",
      "No desc found for /m/02q_plc\n",
      "No desc found for /m/02cjrp\n",
      "No desc found for /m/01xsbh\n",
      "No desc found for /m/03bx017\n",
      "No desc found for /m/01sy5c\n",
      "No desc found for /m/0854hr\n",
      "No desc found for /m/0h005\n",
      "No desc found for /m/01fkv0\n",
      "No desc found for /m/05h4fjx\n",
      "No desc found for /m/05ry0p\n",
      "No desc found for /m/01dvms\n",
      "No desc found for /m/04686_j\n",
      "No desc found for /m/08chdb\n",
      "No desc found for /m/05zvq6g\n",
      "No desc found for /m/0288crq\n",
      "No desc found for /m/01my929\n",
      "No desc found for /m/07t_l23\n",
      "No desc found for /m/07s4911\n",
      "Load 14951 entity names from ./data/FB15k237/FB15k_mid2name.txt\n",
      "Save 237 relations to ./data/FB15k237/relations.json\n",
      "Save 272115 examples to ./data/FB15k237/train.txt.json\n",
      "Process ./data/FB15k237/valid.txt...\n",
      "Save 17535 examples to ./data/FB15k237/valid.txt.json\n",
      "Process ./data/FB15k237/test.txt...\n",
      "Save 20466 examples to ./data/FB15k237/test.txt.json\n",
      "Get 14541 entities, 237 relations in total\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "%cd SimKGC\n",
    "!bash scripts/preprocess.sh FB15k237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b24ba2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ set -e\n",
      "+ TASK=FB15k237\n",
      "+++ dirname scripts/train_fb.sh\n",
      "++ cd scripts\n",
      "++ cd ..\n",
      "++ pwd\n",
      "+ DIR=/workspace/SimKGC\n",
      "+ echo 'working directory: /workspace/SimKGC'\n",
      "working directory: /workspace/SimKGC\n",
      "+ '[' -z ./checkpoint/fb15k237/ ']'\n",
      "+ '[' -z '' ']'\n",
      "+ DATA_DIR=/workspace/SimKGC/data/FB15k237\n",
      "+ python3 -u main.py --model-dir ./checkpoint/fb15k237/ --pretrained-model bert-base-uncased --pooling mean --lr 5e-5 --use-link-graph --train-path /workspace/SimKGC/data/FB15k237/train.txt.json --valid-path /workspace/SimKGC/data/FB15k237/valid.txt.json --task FB15k237 --batch-size 960 --print-freq 20 --additive-margin 0.02 --use-amp --use-self-negative --finetune-t --pre-batch 2 --epochs 10 --workers 4 --max-to-keep 5\n",
      "[2022-07-07 16:08:30,756 INFO] Load 14541 entities from /workspace/SimKGC/data/FB15k237/entities.json\n",
      "[2022-07-07 16:08:30,756 INFO] Triplets path: ['/workspace/SimKGC/data/FB15k237/train.txt.json']\n",
      "[2022-07-07 16:08:32,233 INFO] Triplet statistics: 474 relations, 544230 triplets\n",
      "[2022-07-07 16:08:32,234 INFO] Start to build link graph from /workspace/SimKGC/data/FB15k237/train.txt.json\n",
      "[2022-07-07 16:08:33,026 INFO] Done build link graph with 14505 nodes\n",
      "[2022-07-07 16:08:33,140 INFO] Use 4 gpus for training\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 28.3kB/s]\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 583kB/s]\n",
      "Downloading: 100%|████████████████████████████| 226k/226k [00:00<00:00, 693kB/s]\n",
      "Downloading: 100%|███████████████████████████| 455k/455k [00:00<00:00, 1.36MB/s]\n",
      "[2022-07-07 16:08:38,770 INFO] Build tokenizer from bert-base-uncased\n",
      "[2022-07-07 16:08:38,771 INFO] => creating model\n",
      "[2022-07-07 16:08:40,571 INFO] CustomBertModel(\n",
      "  (hr_bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (tail_bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[2022-07-07 16:08:41,566 INFO] module.log_inv_t: 1.0\n",
      "[2022-07-07 16:08:41,566 INFO] module.hr_bert.embeddings.word_embeddings.weight: 23440896\n",
      "[2022-07-07 16:08:41,566 INFO] module.hr_bert.embeddings.position_embeddings.weight: 393216\n",
      "[2022-07-07 16:08:41,567 INFO] module.hr_bert.embeddings.token_type_embeddings.weight: 1536\n",
      "[2022-07-07 16:08:41,567 INFO] module.hr_bert.embeddings.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,567 INFO] module.hr_bert.embeddings.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,567 INFO] module.hr_bert.encoder.layer.0.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,567 INFO] module.hr_bert.encoder.layer.0.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,567 INFO] module.hr_bert.encoder.layer.0.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,567 INFO] module.hr_bert.encoder.layer.0.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,567 INFO] module.hr_bert.encoder.layer.0.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,567 INFO] module.hr_bert.encoder.layer.0.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,567 INFO] module.hr_bert.encoder.layer.0.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,567 INFO] module.hr_bert.encoder.layer.0.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,567 INFO] module.hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,567 INFO] module.hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,567 INFO] module.hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,567 INFO] module.hr_bert.encoder.layer.0.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,567 INFO] module.hr_bert.encoder.layer.0.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,567 INFO] module.hr_bert.encoder.layer.0.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,567 INFO] module.hr_bert.encoder.layer.0.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,567 INFO] module.hr_bert.encoder.layer.0.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,568 INFO] module.hr_bert.encoder.layer.1.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,568 INFO] module.hr_bert.encoder.layer.1.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,568 INFO] module.hr_bert.encoder.layer.1.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,568 INFO] module.hr_bert.encoder.layer.1.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,568 INFO] module.hr_bert.encoder.layer.1.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,568 INFO] module.hr_bert.encoder.layer.1.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,568 INFO] module.hr_bert.encoder.layer.1.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,568 INFO] module.hr_bert.encoder.layer.1.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,568 INFO] module.hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,568 INFO] module.hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,568 INFO] module.hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,568 INFO] module.hr_bert.encoder.layer.1.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,568 INFO] module.hr_bert.encoder.layer.1.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,568 INFO] module.hr_bert.encoder.layer.1.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,568 INFO] module.hr_bert.encoder.layer.1.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,568 INFO] module.hr_bert.encoder.layer.1.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,568 INFO] module.hr_bert.encoder.layer.2.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,568 INFO] module.hr_bert.encoder.layer.2.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,568 INFO] module.hr_bert.encoder.layer.2.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,568 INFO] module.hr_bert.encoder.layer.2.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,569 INFO] module.hr_bert.encoder.layer.2.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,569 INFO] module.hr_bert.encoder.layer.2.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,569 INFO] module.hr_bert.encoder.layer.2.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,569 INFO] module.hr_bert.encoder.layer.2.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,569 INFO] module.hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,569 INFO] module.hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,569 INFO] module.hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,569 INFO] module.hr_bert.encoder.layer.2.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,569 INFO] module.hr_bert.encoder.layer.2.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,569 INFO] module.hr_bert.encoder.layer.2.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,569 INFO] module.hr_bert.encoder.layer.2.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,569 INFO] module.hr_bert.encoder.layer.2.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,569 INFO] module.hr_bert.encoder.layer.3.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,569 INFO] module.hr_bert.encoder.layer.3.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,569 INFO] module.hr_bert.encoder.layer.3.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,569 INFO] module.hr_bert.encoder.layer.3.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,569 INFO] module.hr_bert.encoder.layer.3.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,569 INFO] module.hr_bert.encoder.layer.3.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,569 INFO] module.hr_bert.encoder.layer.3.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,570 INFO] module.hr_bert.encoder.layer.3.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,570 INFO] module.hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,570 INFO] module.hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,570 INFO] module.hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,570 INFO] module.hr_bert.encoder.layer.3.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,570 INFO] module.hr_bert.encoder.layer.3.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,570 INFO] module.hr_bert.encoder.layer.3.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,570 INFO] module.hr_bert.encoder.layer.3.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,570 INFO] module.hr_bert.encoder.layer.3.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,570 INFO] module.hr_bert.encoder.layer.4.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,570 INFO] module.hr_bert.encoder.layer.4.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,570 INFO] module.hr_bert.encoder.layer.4.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,570 INFO] module.hr_bert.encoder.layer.4.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,570 INFO] module.hr_bert.encoder.layer.4.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,570 INFO] module.hr_bert.encoder.layer.4.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,570 INFO] module.hr_bert.encoder.layer.4.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,570 INFO] module.hr_bert.encoder.layer.4.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,570 INFO] module.hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,570 INFO] module.hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,570 INFO] module.hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,571 INFO] module.hr_bert.encoder.layer.4.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,571 INFO] module.hr_bert.encoder.layer.4.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,571 INFO] module.hr_bert.encoder.layer.4.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,571 INFO] module.hr_bert.encoder.layer.4.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,571 INFO] module.hr_bert.encoder.layer.4.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,571 INFO] module.hr_bert.encoder.layer.5.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,571 INFO] module.hr_bert.encoder.layer.5.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,571 INFO] module.hr_bert.encoder.layer.5.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,571 INFO] module.hr_bert.encoder.layer.5.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,571 INFO] module.hr_bert.encoder.layer.5.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,571 INFO] module.hr_bert.encoder.layer.5.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,571 INFO] module.hr_bert.encoder.layer.5.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,571 INFO] module.hr_bert.encoder.layer.5.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,571 INFO] module.hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,571 INFO] module.hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,571 INFO] module.hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,571 INFO] module.hr_bert.encoder.layer.5.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,571 INFO] module.hr_bert.encoder.layer.5.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,571 INFO] module.hr_bert.encoder.layer.5.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,572 INFO] module.hr_bert.encoder.layer.5.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,572 INFO] module.hr_bert.encoder.layer.5.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,572 INFO] module.hr_bert.encoder.layer.6.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,572 INFO] module.hr_bert.encoder.layer.6.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,572 INFO] module.hr_bert.encoder.layer.6.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,572 INFO] module.hr_bert.encoder.layer.6.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,572 INFO] module.hr_bert.encoder.layer.6.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,572 INFO] module.hr_bert.encoder.layer.6.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,572 INFO] module.hr_bert.encoder.layer.6.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,572 INFO] module.hr_bert.encoder.layer.6.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,572 INFO] module.hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,572 INFO] module.hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,572 INFO] module.hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,572 INFO] module.hr_bert.encoder.layer.6.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,572 INFO] module.hr_bert.encoder.layer.6.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,572 INFO] module.hr_bert.encoder.layer.6.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,572 INFO] module.hr_bert.encoder.layer.6.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,572 INFO] module.hr_bert.encoder.layer.6.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,572 INFO] module.hr_bert.encoder.layer.7.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,572 INFO] module.hr_bert.encoder.layer.7.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,573 INFO] module.hr_bert.encoder.layer.7.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,573 INFO] module.hr_bert.encoder.layer.7.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,573 INFO] module.hr_bert.encoder.layer.7.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,573 INFO] module.hr_bert.encoder.layer.7.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,573 INFO] module.hr_bert.encoder.layer.7.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,573 INFO] module.hr_bert.encoder.layer.7.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,573 INFO] module.hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,573 INFO] module.hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,573 INFO] module.hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,573 INFO] module.hr_bert.encoder.layer.7.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,573 INFO] module.hr_bert.encoder.layer.7.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,573 INFO] module.hr_bert.encoder.layer.7.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,573 INFO] module.hr_bert.encoder.layer.7.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,573 INFO] module.hr_bert.encoder.layer.7.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,573 INFO] module.hr_bert.encoder.layer.8.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,573 INFO] module.hr_bert.encoder.layer.8.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,573 INFO] module.hr_bert.encoder.layer.8.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,573 INFO] module.hr_bert.encoder.layer.8.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,573 INFO] module.hr_bert.encoder.layer.8.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,573 INFO] module.hr_bert.encoder.layer.8.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,574 INFO] module.hr_bert.encoder.layer.8.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,574 INFO] module.hr_bert.encoder.layer.8.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,574 INFO] module.hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,574 INFO] module.hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,574 INFO] module.hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,574 INFO] module.hr_bert.encoder.layer.8.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,574 INFO] module.hr_bert.encoder.layer.8.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,574 INFO] module.hr_bert.encoder.layer.8.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,574 INFO] module.hr_bert.encoder.layer.8.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,574 INFO] module.hr_bert.encoder.layer.8.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,574 INFO] module.hr_bert.encoder.layer.9.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,574 INFO] module.hr_bert.encoder.layer.9.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,574 INFO] module.hr_bert.encoder.layer.9.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,574 INFO] module.hr_bert.encoder.layer.9.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,574 INFO] module.hr_bert.encoder.layer.9.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,574 INFO] module.hr_bert.encoder.layer.9.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,574 INFO] module.hr_bert.encoder.layer.9.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,574 INFO] module.hr_bert.encoder.layer.9.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,574 INFO] module.hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,575 INFO] module.hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,575 INFO] module.hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,575 INFO] module.hr_bert.encoder.layer.9.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,575 INFO] module.hr_bert.encoder.layer.9.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,575 INFO] module.hr_bert.encoder.layer.9.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,575 INFO] module.hr_bert.encoder.layer.9.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,575 INFO] module.hr_bert.encoder.layer.9.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,575 INFO] module.hr_bert.encoder.layer.10.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,575 INFO] module.hr_bert.encoder.layer.10.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,575 INFO] module.hr_bert.encoder.layer.10.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,575 INFO] module.hr_bert.encoder.layer.10.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,575 INFO] module.hr_bert.encoder.layer.10.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,575 INFO] module.hr_bert.encoder.layer.10.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,575 INFO] module.hr_bert.encoder.layer.10.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,575 INFO] module.hr_bert.encoder.layer.10.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,575 INFO] module.hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,575 INFO] module.hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,575 INFO] module.hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,575 INFO] module.hr_bert.encoder.layer.10.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,575 INFO] module.hr_bert.encoder.layer.10.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,576 INFO] module.hr_bert.encoder.layer.10.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,576 INFO] module.hr_bert.encoder.layer.10.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,576 INFO] module.hr_bert.encoder.layer.10.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,576 INFO] module.hr_bert.encoder.layer.11.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,576 INFO] module.hr_bert.encoder.layer.11.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,576 INFO] module.hr_bert.encoder.layer.11.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,576 INFO] module.hr_bert.encoder.layer.11.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,576 INFO] module.hr_bert.encoder.layer.11.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,576 INFO] module.hr_bert.encoder.layer.11.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,576 INFO] module.hr_bert.encoder.layer.11.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,576 INFO] module.hr_bert.encoder.layer.11.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,576 INFO] module.hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,576 INFO] module.hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,576 INFO] module.hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,576 INFO] module.hr_bert.encoder.layer.11.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,576 INFO] module.hr_bert.encoder.layer.11.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,576 INFO] module.hr_bert.encoder.layer.11.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,576 INFO] module.hr_bert.encoder.layer.11.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,576 INFO] module.hr_bert.encoder.layer.11.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,577 INFO] module.hr_bert.pooler.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,577 INFO] module.hr_bert.pooler.dense.bias: 768\n",
      "[2022-07-07 16:08:41,577 INFO] module.tail_bert.embeddings.word_embeddings.weight: 23440896\n",
      "[2022-07-07 16:08:41,577 INFO] module.tail_bert.embeddings.position_embeddings.weight: 393216\n",
      "[2022-07-07 16:08:41,577 INFO] module.tail_bert.embeddings.token_type_embeddings.weight: 1536\n",
      "[2022-07-07 16:08:41,577 INFO] module.tail_bert.embeddings.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,577 INFO] module.tail_bert.embeddings.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,577 INFO] module.tail_bert.encoder.layer.0.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,577 INFO] module.tail_bert.encoder.layer.0.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,577 INFO] module.tail_bert.encoder.layer.0.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,577 INFO] module.tail_bert.encoder.layer.0.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,577 INFO] module.tail_bert.encoder.layer.0.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,577 INFO] module.tail_bert.encoder.layer.0.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,577 INFO] module.tail_bert.encoder.layer.0.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,577 INFO] module.tail_bert.encoder.layer.0.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,577 INFO] module.tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,577 INFO] module.tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,577 INFO] module.tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,577 INFO] module.tail_bert.encoder.layer.0.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,577 INFO] module.tail_bert.encoder.layer.0.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,578 INFO] module.tail_bert.encoder.layer.0.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,578 INFO] module.tail_bert.encoder.layer.0.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,578 INFO] module.tail_bert.encoder.layer.0.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,578 INFO] module.tail_bert.encoder.layer.1.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,578 INFO] module.tail_bert.encoder.layer.1.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,578 INFO] module.tail_bert.encoder.layer.1.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,578 INFO] module.tail_bert.encoder.layer.1.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,578 INFO] module.tail_bert.encoder.layer.1.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,578 INFO] module.tail_bert.encoder.layer.1.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,578 INFO] module.tail_bert.encoder.layer.1.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,578 INFO] module.tail_bert.encoder.layer.1.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,578 INFO] module.tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,578 INFO] module.tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,578 INFO] module.tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,578 INFO] module.tail_bert.encoder.layer.1.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,578 INFO] module.tail_bert.encoder.layer.1.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,578 INFO] module.tail_bert.encoder.layer.1.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,578 INFO] module.tail_bert.encoder.layer.1.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,578 INFO] module.tail_bert.encoder.layer.1.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,579 INFO] module.tail_bert.encoder.layer.2.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,579 INFO] module.tail_bert.encoder.layer.2.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,579 INFO] module.tail_bert.encoder.layer.2.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,579 INFO] module.tail_bert.encoder.layer.2.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,579 INFO] module.tail_bert.encoder.layer.2.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,579 INFO] module.tail_bert.encoder.layer.2.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,579 INFO] module.tail_bert.encoder.layer.2.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,579 INFO] module.tail_bert.encoder.layer.2.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,579 INFO] module.tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,579 INFO] module.tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,579 INFO] module.tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,579 INFO] module.tail_bert.encoder.layer.2.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,579 INFO] module.tail_bert.encoder.layer.2.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,579 INFO] module.tail_bert.encoder.layer.2.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,579 INFO] module.tail_bert.encoder.layer.2.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,579 INFO] module.tail_bert.encoder.layer.2.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,579 INFO] module.tail_bert.encoder.layer.3.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,579 INFO] module.tail_bert.encoder.layer.3.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,579 INFO] module.tail_bert.encoder.layer.3.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,579 INFO] module.tail_bert.encoder.layer.3.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,580 INFO] module.tail_bert.encoder.layer.3.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,580 INFO] module.tail_bert.encoder.layer.3.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,580 INFO] module.tail_bert.encoder.layer.3.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,580 INFO] module.tail_bert.encoder.layer.3.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,580 INFO] module.tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,580 INFO] module.tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,580 INFO] module.tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,580 INFO] module.tail_bert.encoder.layer.3.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,580 INFO] module.tail_bert.encoder.layer.3.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,580 INFO] module.tail_bert.encoder.layer.3.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,580 INFO] module.tail_bert.encoder.layer.3.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,580 INFO] module.tail_bert.encoder.layer.3.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,580 INFO] module.tail_bert.encoder.layer.4.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,580 INFO] module.tail_bert.encoder.layer.4.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,580 INFO] module.tail_bert.encoder.layer.4.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,580 INFO] module.tail_bert.encoder.layer.4.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,580 INFO] module.tail_bert.encoder.layer.4.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,580 INFO] module.tail_bert.encoder.layer.4.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,580 INFO] module.tail_bert.encoder.layer.4.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,580 INFO] module.tail_bert.encoder.layer.4.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,581 INFO] module.tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,581 INFO] module.tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,581 INFO] module.tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,581 INFO] module.tail_bert.encoder.layer.4.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,581 INFO] module.tail_bert.encoder.layer.4.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,581 INFO] module.tail_bert.encoder.layer.4.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,581 INFO] module.tail_bert.encoder.layer.4.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,581 INFO] module.tail_bert.encoder.layer.4.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,581 INFO] module.tail_bert.encoder.layer.5.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,581 INFO] module.tail_bert.encoder.layer.5.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,581 INFO] module.tail_bert.encoder.layer.5.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,581 INFO] module.tail_bert.encoder.layer.5.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,581 INFO] module.tail_bert.encoder.layer.5.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,581 INFO] module.tail_bert.encoder.layer.5.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,581 INFO] module.tail_bert.encoder.layer.5.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,581 INFO] module.tail_bert.encoder.layer.5.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,581 INFO] module.tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,581 INFO] module.tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,581 INFO] module.tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,582 INFO] module.tail_bert.encoder.layer.5.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,582 INFO] module.tail_bert.encoder.layer.5.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,582 INFO] module.tail_bert.encoder.layer.5.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,582 INFO] module.tail_bert.encoder.layer.5.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,582 INFO] module.tail_bert.encoder.layer.5.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,582 INFO] module.tail_bert.encoder.layer.6.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,582 INFO] module.tail_bert.encoder.layer.6.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,582 INFO] module.tail_bert.encoder.layer.6.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,582 INFO] module.tail_bert.encoder.layer.6.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,582 INFO] module.tail_bert.encoder.layer.6.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,582 INFO] module.tail_bert.encoder.layer.6.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,582 INFO] module.tail_bert.encoder.layer.6.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,582 INFO] module.tail_bert.encoder.layer.6.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,582 INFO] module.tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,582 INFO] module.tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,582 INFO] module.tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,582 INFO] module.tail_bert.encoder.layer.6.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,582 INFO] module.tail_bert.encoder.layer.6.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,582 INFO] module.tail_bert.encoder.layer.6.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,582 INFO] module.tail_bert.encoder.layer.6.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,583 INFO] module.tail_bert.encoder.layer.6.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,583 INFO] module.tail_bert.encoder.layer.7.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,583 INFO] module.tail_bert.encoder.layer.7.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,583 INFO] module.tail_bert.encoder.layer.7.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,583 INFO] module.tail_bert.encoder.layer.7.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,583 INFO] module.tail_bert.encoder.layer.7.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,583 INFO] module.tail_bert.encoder.layer.7.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,583 INFO] module.tail_bert.encoder.layer.7.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,583 INFO] module.tail_bert.encoder.layer.7.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,583 INFO] module.tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,583 INFO] module.tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,583 INFO] module.tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,583 INFO] module.tail_bert.encoder.layer.7.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,583 INFO] module.tail_bert.encoder.layer.7.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,583 INFO] module.tail_bert.encoder.layer.7.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,583 INFO] module.tail_bert.encoder.layer.7.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,583 INFO] module.tail_bert.encoder.layer.7.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,583 INFO] module.tail_bert.encoder.layer.8.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,583 INFO] module.tail_bert.encoder.layer.8.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,584 INFO] module.tail_bert.encoder.layer.8.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,584 INFO] module.tail_bert.encoder.layer.8.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,584 INFO] module.tail_bert.encoder.layer.8.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,584 INFO] module.tail_bert.encoder.layer.8.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,584 INFO] module.tail_bert.encoder.layer.8.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,584 INFO] module.tail_bert.encoder.layer.8.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,584 INFO] module.tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,584 INFO] module.tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,584 INFO] module.tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,584 INFO] module.tail_bert.encoder.layer.8.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,584 INFO] module.tail_bert.encoder.layer.8.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,584 INFO] module.tail_bert.encoder.layer.8.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,584 INFO] module.tail_bert.encoder.layer.8.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,584 INFO] module.tail_bert.encoder.layer.8.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,584 INFO] module.tail_bert.encoder.layer.9.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,584 INFO] module.tail_bert.encoder.layer.9.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,584 INFO] module.tail_bert.encoder.layer.9.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,584 INFO] module.tail_bert.encoder.layer.9.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,584 INFO] module.tail_bert.encoder.layer.9.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,584 INFO] module.tail_bert.encoder.layer.9.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,585 INFO] module.tail_bert.encoder.layer.9.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,585 INFO] module.tail_bert.encoder.layer.9.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,585 INFO] module.tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,585 INFO] module.tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,585 INFO] module.tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,585 INFO] module.tail_bert.encoder.layer.9.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,585 INFO] module.tail_bert.encoder.layer.9.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,585 INFO] module.tail_bert.encoder.layer.9.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,585 INFO] module.tail_bert.encoder.layer.9.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,585 INFO] module.tail_bert.encoder.layer.9.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,585 INFO] module.tail_bert.encoder.layer.10.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,585 INFO] module.tail_bert.encoder.layer.10.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,585 INFO] module.tail_bert.encoder.layer.10.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,585 INFO] module.tail_bert.encoder.layer.10.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,585 INFO] module.tail_bert.encoder.layer.10.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,585 INFO] module.tail_bert.encoder.layer.10.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,585 INFO] module.tail_bert.encoder.layer.10.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,585 INFO] module.tail_bert.encoder.layer.10.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,585 INFO] module.tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,586 INFO] module.tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,586 INFO] module.tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,586 INFO] module.tail_bert.encoder.layer.10.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,586 INFO] module.tail_bert.encoder.layer.10.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,586 INFO] module.tail_bert.encoder.layer.10.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,586 INFO] module.tail_bert.encoder.layer.10.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,586 INFO] module.tail_bert.encoder.layer.10.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,586 INFO] module.tail_bert.encoder.layer.11.attention.self.query.weight: 589824\n",
      "[2022-07-07 16:08:41,586 INFO] module.tail_bert.encoder.layer.11.attention.self.query.bias: 768\n",
      "[2022-07-07 16:08:41,586 INFO] module.tail_bert.encoder.layer.11.attention.self.key.weight: 589824\n",
      "[2022-07-07 16:08:41,586 INFO] module.tail_bert.encoder.layer.11.attention.self.key.bias: 768\n",
      "[2022-07-07 16:08:41,586 INFO] module.tail_bert.encoder.layer.11.attention.self.value.weight: 589824\n",
      "[2022-07-07 16:08:41,586 INFO] module.tail_bert.encoder.layer.11.attention.self.value.bias: 768\n",
      "[2022-07-07 16:08:41,586 INFO] module.tail_bert.encoder.layer.11.attention.output.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,586 INFO] module.tail_bert.encoder.layer.11.attention.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,586 INFO] module.tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,586 INFO] module.tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,586 INFO] module.tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,586 INFO] module.tail_bert.encoder.layer.11.intermediate.dense.bias: 3072\n",
      "[2022-07-07 16:08:41,586 INFO] module.tail_bert.encoder.layer.11.output.dense.weight: 2359296\n",
      "[2022-07-07 16:08:41,587 INFO] module.tail_bert.encoder.layer.11.output.dense.bias: 768\n",
      "[2022-07-07 16:08:41,587 INFO] module.tail_bert.encoder.layer.11.output.LayerNorm.weight: 768\n",
      "[2022-07-07 16:08:41,587 INFO] module.tail_bert.encoder.layer.11.output.LayerNorm.bias: 768\n",
      "[2022-07-07 16:08:41,587 INFO] module.tail_bert.pooler.dense.weight: 589824\n",
      "[2022-07-07 16:08:41,587 INFO] module.tail_bert.pooler.dense.bias: 768\n",
      "[2022-07-07 16:08:41,587 INFO] Number of parameters: 218.0M\n",
      "[2022-07-07 16:08:41,587 INFO] In test mode: False\n",
      "[2022-07-07 16:08:42,039 INFO] Load 272115 examples from /workspace/SimKGC/data/FB15k237/train.txt.json\n",
      "[2022-07-07 16:08:43,666 INFO] In test mode: False\n",
      "[2022-07-07 16:08:43,695 INFO] Load 17535 examples from /workspace/SimKGC/data/FB15k237/valid.txt.json\n",
      "[2022-07-07 16:08:44,065 INFO] Total training steps: 5669, warmup steps: 400\n",
      "[2022-07-07 16:08:44,065 INFO] Args={\n",
      "    \"pretrained_model\": \"bert-base-uncased\",\n",
      "    \"task\": \"FB15k237\",\n",
      "    \"train_path\": \"/workspace/SimKGC/data/FB15k237/train.txt.json\",\n",
      "    \"valid_path\": \"/workspace/SimKGC/data/FB15k237/valid.txt.json\",\n",
      "    \"model_dir\": \"./checkpoint/fb15k237/\",\n",
      "    \"warmup\": 400,\n",
      "    \"max_to_keep\": 5,\n",
      "    \"grad_clip\": 10.0,\n",
      "    \"pooling\": \"mean\",\n",
      "    \"dropout\": 0.1,\n",
      "    \"use_amp\": true,\n",
      "    \"t\": 0.05,\n",
      "    \"use_link_graph\": true,\n",
      "    \"eval_every_n_step\": 10000,\n",
      "    \"pre_batch\": 2,\n",
      "    \"pre_batch_weight\": 0.5,\n",
      "    \"additive_margin\": 0.02,\n",
      "    \"finetune_t\": true,\n",
      "    \"max_num_tokens\": 50,\n",
      "    \"use_self_negative\": true,\n",
      "    \"workers\": 4,\n",
      "    \"epochs\": 10,\n",
      "    \"batch_size\": 960,\n",
      "    \"lr\": 5e-05,\n",
      "    \"lr_scheduler\": \"linear\",\n",
      "    \"weight_decay\": 0.0001,\n",
      "    \"print_freq\": 20,\n",
      "    \"seed\": null,\n",
      "    \"is_test\": false,\n",
      "    \"rerank_n_hop\": 2,\n",
      "    \"neighbor_weight\": 0.0,\n",
      "    \"eval_model_path\": \"\"\n",
      "}\n",
      "[2022-07-07 16:08:51,534 INFO] Epoch: [0][  0/566]\tLoss 14.48 (14.48)\tInvT  20.00 ( 20.00)\tAcc@1   0.00 (  0.00)\tAcc@3   0.00 (  0.00)\n",
      "[2022-07-07 16:09:20,613 INFO] Epoch: [0][ 20/566]\tLoss 14.44 (14.46)\tInvT  20.00 ( 20.00)\tAcc@1   0.00 (  0.00)\tAcc@3   0.00 (  0.00)\n",
      "[2022-07-07 16:09:50,277 INFO] Epoch: [0][ 40/566]\tLoss 13.92 (14.35)\tInvT  20.00 ( 20.00)\tAcc@1   0.73 (  0.10)\tAcc@3   1.15 (  0.16)\n",
      "[2022-07-07 16:10:20,189 INFO] Epoch: [0][ 60/566]\tLoss 13.43 (14.14)\tInvT  20.00 ( 20.00)\tAcc@1   0.62 (  0.25)\tAcc@3   1.04 (  0.39)\n",
      "[2022-07-07 16:10:49,681 INFO] Epoch: [0][ 80/566]\tLoss 13.3 (13.95)\tInvT  20.00 ( 20.00)\tAcc@1   1.04 (  0.33)\tAcc@3   1.67 (  0.58)\n",
      "[2022-07-07 16:11:19,366 INFO] Epoch: [0][100/566]\tLoss 12.72 (13.76)\tInvT  19.99 ( 20.00)\tAcc@1   0.42 (  0.38)\tAcc@3   2.40 (  0.76)\n",
      "[2022-07-07 16:11:48,725 INFO] Epoch: [0][120/566]\tLoss 12.61 (13.6)\tInvT  19.99 ( 20.00)\tAcc@1   0.62 (  0.44)\tAcc@3   2.40 (  0.97)\n",
      "[2022-07-07 16:12:18,431 INFO] Epoch: [0][140/566]\tLoss 12.25 (13.42)\tInvT  19.99 ( 20.00)\tAcc@1   1.25 (  0.54)\tAcc@3   2.71 (  1.25)\n",
      "[2022-07-07 16:12:47,948 INFO] Epoch: [0][160/566]\tLoss 11.93 (13.27)\tInvT  19.99 ( 20.00)\tAcc@1   2.29 (  0.68)\tAcc@3   4.79 (  1.57)\n",
      "[2022-07-07 16:13:17,508 INFO] Epoch: [0][180/566]\tLoss 11.84 (13.1)\tInvT  19.99 ( 19.99)\tAcc@1   2.92 (  0.91)\tAcc@3   6.15 (  1.98)\n",
      "[2022-07-07 16:13:47,001 INFO] Epoch: [0][200/566]\tLoss 11.72 (12.93)\tInvT  19.99 ( 19.99)\tAcc@1   4.27 (  1.18)\tAcc@3   7.19 (  2.46)\n",
      "[2022-07-07 16:14:16,763 INFO] Epoch: [0][220/566]\tLoss 10.79 (12.77)\tInvT  19.98 ( 19.99)\tAcc@1   4.90 (  1.45)\tAcc@3   8.23 (  2.90)\n",
      "[2022-07-07 16:14:46,409 INFO] Epoch: [0][240/566]\tLoss 10.72 (12.6)\tInvT  19.98 ( 19.99)\tAcc@1   5.42 (  1.77)\tAcc@3  11.15 (  3.43)\n",
      "[2022-07-07 16:15:15,973 INFO] Epoch: [0][260/566]\tLoss 10.7 (12.45)\tInvT  19.98 ( 19.99)\tAcc@1   4.90 (  2.05)\tAcc@3   8.75 (  3.91)\n",
      "[2022-07-07 16:15:45,577 INFO] Epoch: [0][280/566]\tLoss 9.952 (12.3)\tInvT  19.98 ( 19.99)\tAcc@1   7.81 (  2.36)\tAcc@3  11.56 (  4.43)\n",
      "[2022-07-07 16:16:15,316 INFO] Epoch: [0][300/566]\tLoss 9.59 (12.13)\tInvT  19.97 ( 19.99)\tAcc@1   7.71 (  2.69)\tAcc@3  14.69 (  4.97)\n",
      "[2022-07-07 16:16:45,051 INFO] Epoch: [0][320/566]\tLoss 9.435 (11.98)\tInvT  19.97 ( 19.99)\tAcc@1   8.96 (  2.98)\tAcc@3  15.10 (  5.48)\n",
      "[2022-07-07 16:17:14,578 INFO] Epoch: [0][340/566]\tLoss 9.703 (11.84)\tInvT  19.96 ( 19.99)\tAcc@1   7.60 (  3.29)\tAcc@3  14.17 (  6.01)\n",
      "[2022-07-07 16:17:44,165 INFO] Epoch: [0][360/566]\tLoss 9.79 (11.72)\tInvT  19.96 ( 19.99)\tAcc@1   9.06 (  3.55)\tAcc@3  14.69 (  6.42)\n",
      "[2022-07-07 16:18:13,701 INFO] Epoch: [0][380/566]\tLoss 9.097 (11.6)\tInvT  19.95 ( 19.98)\tAcc@1   8.02 (  3.80)\tAcc@3  14.17 (  6.84)\n",
      "[2022-07-07 16:18:43,132 INFO] Epoch: [0][400/566]\tLoss 9.523 (11.49)\tInvT  19.94 ( 19.98)\tAcc@1   6.88 (  4.01)\tAcc@3  13.54 (  7.24)\n",
      "[2022-07-07 16:19:12,763 INFO] Epoch: [0][420/566]\tLoss 9.336 (11.38)\tInvT  19.94 ( 19.98)\tAcc@1  10.00 (  4.26)\tAcc@3  16.67 (  7.68)\n",
      "[2022-07-07 16:19:42,436 INFO] Epoch: [0][440/566]\tLoss 9.009 (11.26)\tInvT  19.93 ( 19.98)\tAcc@1   8.65 (  4.52)\tAcc@3  15.10 (  8.13)\n",
      "[2022-07-07 16:20:12,097 INFO] Epoch: [0][460/566]\tLoss 8.468 (11.15)\tInvT  19.92 ( 19.98)\tAcc@1  10.21 (  4.75)\tAcc@3  19.27 (  8.55)\n",
      "[2022-07-07 16:20:41,430 INFO] Epoch: [0][480/566]\tLoss 8.605 (11.03)\tInvT  19.90 ( 19.97)\tAcc@1  10.00 (  5.00)\tAcc@3  18.75 (  9.00)\n",
      "[2022-07-07 16:21:11,072 INFO] Epoch: [0][500/566]\tLoss 8.154 (10.92)\tInvT  19.89 ( 19.97)\tAcc@1  10.83 (  5.26)\tAcc@3  19.48 (  9.45)\n",
      "[2022-07-07 16:21:40,795 INFO] Epoch: [0][520/566]\tLoss 7.913 (10.81)\tInvT  19.88 ( 19.97)\tAcc@1  12.71 (  5.51)\tAcc@3  22.81 (  9.90)\n",
      "[2022-07-07 16:22:10,462 INFO] Epoch: [0][540/566]\tLoss 7.885 (10.71)\tInvT  19.86 ( 19.96)\tAcc@1  11.98 (  5.75)\tAcc@3  22.71 ( 10.34)\n",
      "[2022-07-07 16:22:40,307 INFO] Epoch: [0][560/566]\tLoss 7.811 (10.61)\tInvT  19.85 ( 19.96)\tAcc@1  10.94 (  5.98)\tAcc@3  23.33 ( 10.76)\n",
      "[2022-07-07 16:22:47,733 INFO] Learning rate: 4.842474852913266e-05\n",
      "[2022-07-07 16:23:14,435 INFO] Epoch 0, valid metric: {\"Acc@1\": 16.752, \"Acc@3\": 25.361, \"loss\": 4.009}\n",
      "[2022-07-07 16:23:19,063 INFO] Epoch: [1][  0/566]\tLoss 7.51 (7.51)\tInvT  19.84 ( 19.84)\tAcc@1  14.38 ( 14.38)\tAcc@3  25.10 ( 25.10)\n",
      "[2022-07-07 16:23:48,819 INFO] Epoch: [1][ 20/566]\tLoss 7.593 (7.664)\tInvT  19.82 ( 19.83)\tAcc@1  15.10 ( 13.16)\tAcc@3  25.21 ( 24.16)\n",
      "[2022-07-07 16:24:18,456 INFO] Epoch: [1][ 40/566]\tLoss 7.464 (7.59)\tInvT  19.81 ( 19.82)\tAcc@1  13.65 ( 13.44)\tAcc@3  25.62 ( 24.74)\n",
      "[2022-07-07 16:24:48,090 INFO] Epoch: [1][ 60/566]\tLoss 7.427 (7.53)\tInvT  19.79 ( 19.81)\tAcc@1  14.48 ( 13.67)\tAcc@3  24.58 ( 24.96)\n",
      "[2022-07-07 16:25:17,804 INFO] Epoch: [1][ 80/566]\tLoss 7.241 (7.486)\tInvT  19.77 ( 19.81)\tAcc@1  15.52 ( 13.90)\tAcc@3  27.71 ( 25.35)\n",
      "[2022-07-07 16:25:47,523 INFO] Epoch: [1][100/566]\tLoss 7.227 (7.445)\tInvT  19.75 ( 19.80)\tAcc@1  14.27 ( 14.18)\tAcc@3  29.27 ( 25.79)\n",
      "[2022-07-07 16:26:17,157 INFO] Epoch: [1][120/566]\tLoss 7.138 (7.409)\tInvT  19.73 ( 19.79)\tAcc@1  14.90 ( 14.37)\tAcc@3  27.40 ( 26.14)\n",
      "[2022-07-07 16:26:46,749 INFO] Epoch: [1][140/566]\tLoss 7.008 (7.373)\tInvT  19.71 ( 19.78)\tAcc@1  17.19 ( 14.62)\tAcc@3  29.17 ( 26.55)\n",
      "[2022-07-07 16:27:16,375 INFO] Epoch: [1][160/566]\tLoss 7.141 (7.334)\tInvT  19.69 ( 19.77)\tAcc@1  14.58 ( 14.85)\tAcc@3  28.65 ( 26.92)\n",
      "[2022-07-07 16:27:46,059 INFO] Epoch: [1][180/566]\tLoss 6.975 (7.296)\tInvT  19.67 ( 19.76)\tAcc@1  17.29 ( 15.06)\tAcc@3  32.50 ( 27.28)\n",
      "[2022-07-07 16:28:15,674 INFO] Epoch: [1][200/566]\tLoss 6.847 (7.258)\tInvT  19.66 ( 19.75)\tAcc@1  16.35 ( 15.25)\tAcc@3  31.67 ( 27.67)\n",
      "[2022-07-07 16:28:45,392 INFO] Epoch: [1][220/566]\tLoss 6.721 (7.219)\tInvT  19.64 ( 19.74)\tAcc@1  18.33 ( 15.44)\tAcc@3  33.54 ( 28.07)\n",
      "[2022-07-07 16:29:15,087 INFO] Epoch: [1][240/566]\tLoss 6.714 (7.185)\tInvT  19.62 ( 19.73)\tAcc@1  16.98 ( 15.62)\tAcc@3  33.02 ( 28.40)\n",
      "[2022-07-07 16:29:44,836 INFO] Epoch: [1][260/566]\tLoss 6.781 (7.15)\tInvT  19.60 ( 19.72)\tAcc@1  18.65 ( 15.84)\tAcc@3  32.50 ( 28.75)\n",
      "[2022-07-07 16:30:14,543 INFO] Epoch: [1][280/566]\tLoss 6.471 (7.116)\tInvT  19.58 ( 19.71)\tAcc@1  19.27 ( 16.03)\tAcc@3  35.31 ( 29.07)\n",
      "[2022-07-07 16:30:44,339 INFO] Epoch: [1][300/566]\tLoss 6.917 (7.085)\tInvT  19.56 ( 19.70)\tAcc@1  17.71 ( 16.19)\tAcc@3  32.19 ( 29.39)\n",
      "[2022-07-07 16:31:14,132 INFO] Epoch: [1][320/566]\tLoss 6.57 (7.056)\tInvT  19.54 ( 19.69)\tAcc@1  19.17 ( 16.37)\tAcc@3  35.10 ( 29.75)\n",
      "[2022-07-07 16:31:43,789 INFO] Epoch: [1][340/566]\tLoss 6.626 (7.027)\tInvT  19.52 ( 19.68)\tAcc@1  18.12 ( 16.52)\tAcc@3  34.06 ( 30.06)\n",
      "[2022-07-07 16:32:13,377 INFO] Epoch: [1][360/566]\tLoss 6.761 (6.998)\tInvT  19.50 ( 19.67)\tAcc@1  18.65 ( 16.69)\tAcc@3  33.75 ( 30.38)\n",
      "[2022-07-07 16:32:43,200 INFO] Epoch: [1][380/566]\tLoss 6.315 (6.971)\tInvT  19.48 ( 19.66)\tAcc@1  18.75 ( 16.85)\tAcc@3  36.67 ( 30.69)\n",
      "[2022-07-07 16:33:13,061 INFO] Epoch: [1][400/566]\tLoss 6.349 (6.943)\tInvT  19.46 ( 19.65)\tAcc@1  22.50 ( 17.02)\tAcc@3  38.54 ( 31.00)\n",
      "[2022-07-07 16:33:42,809 INFO] Epoch: [1][420/566]\tLoss 6.424 (6.915)\tInvT  19.44 ( 19.64)\tAcc@1  19.90 ( 17.19)\tAcc@3  36.04 ( 31.31)\n",
      "[2022-07-07 16:34:12,276 INFO] Epoch: [1][440/566]\tLoss 6.088 (6.888)\tInvT  19.42 ( 19.63)\tAcc@1  22.81 ( 17.36)\tAcc@3  40.62 ( 31.62)\n",
      "[2022-07-07 16:34:42,094 INFO] Epoch: [1][460/566]\tLoss 6.182 (6.863)\tInvT  19.40 ( 19.62)\tAcc@1  21.88 ( 17.52)\tAcc@3  39.69 ( 31.92)\n",
      "[2022-07-07 16:35:11,924 INFO] Epoch: [1][480/566]\tLoss 6.387 (6.837)\tInvT  19.38 ( 19.61)\tAcc@1  22.08 ( 17.68)\tAcc@3  41.04 ( 32.22)\n",
      "[2022-07-07 16:35:41,656 INFO] Epoch: [1][500/566]\tLoss 6.344 (6.812)\tInvT  19.36 ( 19.60)\tAcc@1  19.27 ( 17.85)\tAcc@3  39.06 ( 32.52)\n",
      "[2022-07-07 16:36:11,270 INFO] Epoch: [1][520/566]\tLoss 6.256 (6.787)\tInvT  19.34 ( 19.60)\tAcc@1  20.83 ( 18.03)\tAcc@3  40.52 ( 32.82)\n",
      "[2022-07-07 16:36:41,039 INFO] Epoch: [1][540/566]\tLoss 6.227 (6.764)\tInvT  19.33 ( 19.59)\tAcc@1  21.35 ( 18.17)\tAcc@3  41.77 ( 33.11)\n",
      "[2022-07-07 16:37:10,764 INFO] Epoch: [1][560/566]\tLoss 6.063 (6.738)\tInvT  19.31 ( 19.58)\tAcc@1  23.23 ( 18.34)\tAcc@3  40.83 ( 33.41)\n",
      "[2022-07-07 16:37:18,243 INFO] Learning rate: 4.3053710381476566e-05\n",
      "[2022-07-07 16:37:44,366 INFO] Epoch 1, valid metric: {\"Acc@1\": 24.197, \"Acc@3\": 36.806, \"loss\": 3.418}\n",
      "[2022-07-07 16:37:50,160 INFO] Epoch: [2][  0/566]\tLoss 5.94 (5.94)\tInvT  19.30 ( 19.30)\tAcc@1  24.79 ( 24.79)\tAcc@3  44.17 ( 44.17)\n",
      "[2022-07-07 16:38:19,821 INFO] Epoch: [2][ 20/566]\tLoss 5.842 (5.837)\tInvT  19.29 ( 19.30)\tAcc@1  23.96 ( 24.53)\tAcc@3  44.17 ( 44.60)\n",
      "[2022-07-07 16:38:49,547 INFO] Epoch: [2][ 40/566]\tLoss 5.763 (5.851)\tInvT  19.27 ( 19.29)\tAcc@1  25.31 ( 24.74)\tAcc@3  43.54 ( 44.31)\n",
      "[2022-07-07 16:39:19,366 INFO] Epoch: [2][ 60/566]\tLoss 5.618 (5.833)\tInvT  19.26 ( 19.28)\tAcc@1  26.98 ( 24.91)\tAcc@3  46.56 ( 44.52)\n",
      "[2022-07-07 16:39:49,108 INFO] Epoch: [2][ 80/566]\tLoss 5.798 (5.822)\tInvT  19.24 ( 19.27)\tAcc@1  25.62 ( 25.02)\tAcc@3  45.31 ( 44.67)\n",
      "[2022-07-07 16:40:18,688 INFO] Epoch: [2][100/566]\tLoss 5.839 (5.815)\tInvT  19.23 ( 19.27)\tAcc@1  25.94 ( 25.05)\tAcc@3  46.56 ( 44.82)\n",
      "[2022-07-07 16:40:48,417 INFO] Epoch: [2][120/566]\tLoss 5.676 (5.802)\tInvT  19.21 ( 19.26)\tAcc@1  25.10 ( 25.19)\tAcc@3  46.56 ( 44.93)\n",
      "[2022-07-07 16:41:18,027 INFO] Epoch: [2][140/566]\tLoss 5.721 (5.787)\tInvT  19.20 ( 19.25)\tAcc@1  25.42 ( 25.35)\tAcc@3  45.62 ( 45.08)\n",
      "[2022-07-07 16:41:47,641 INFO] Epoch: [2][160/566]\tLoss 5.572 (5.777)\tInvT  19.18 ( 19.24)\tAcc@1  28.02 ( 25.50)\tAcc@3  48.44 ( 45.16)\n",
      "[2022-07-07 16:42:17,489 INFO] Epoch: [2][180/566]\tLoss 5.892 (5.768)\tInvT  19.17 ( 19.24)\tAcc@1  23.96 ( 25.62)\tAcc@3  43.44 ( 45.30)\n",
      "[2022-07-07 16:42:47,062 INFO] Epoch: [2][200/566]\tLoss 5.687 (5.757)\tInvT  19.16 ( 19.23)\tAcc@1  27.29 ( 25.68)\tAcc@3  48.54 ( 45.44)\n",
      "[2022-07-07 16:43:16,778 INFO] Epoch: [2][220/566]\tLoss 5.613 (5.75)\tInvT  19.14 ( 19.22)\tAcc@1  27.19 ( 25.77)\tAcc@3  46.56 ( 45.54)\n",
      "[2022-07-07 16:43:46,444 INFO] Epoch: [2][240/566]\tLoss 5.296 (5.735)\tInvT  19.13 ( 19.21)\tAcc@1  30.94 ( 25.91)\tAcc@3  48.85 ( 45.71)\n",
      "[2022-07-07 16:44:16,166 INFO] Epoch: [2][260/566]\tLoss 5.622 (5.725)\tInvT  19.11 ( 19.21)\tAcc@1  26.56 ( 25.99)\tAcc@3  44.90 ( 45.82)\n",
      "[2022-07-07 16:44:46,029 INFO] Epoch: [2][280/566]\tLoss 5.602 (5.718)\tInvT  19.10 ( 19.20)\tAcc@1  28.33 ( 26.08)\tAcc@3  49.38 ( 45.93)\n",
      "[2022-07-07 16:45:15,787 INFO] Epoch: [2][300/566]\tLoss 5.686 (5.709)\tInvT  19.09 ( 19.19)\tAcc@1  27.60 ( 26.15)\tAcc@3  47.29 ( 46.04)\n",
      "[2022-07-07 16:45:45,129 INFO] Epoch: [2][320/566]\tLoss 5.474 (5.696)\tInvT  19.07 ( 19.19)\tAcc@1  26.15 ( 26.25)\tAcc@3  47.50 ( 46.18)\n",
      "[2022-07-07 16:46:14,873 INFO] Epoch: [2][340/566]\tLoss 5.679 (5.69)\tInvT  19.06 ( 19.18)\tAcc@1  26.04 ( 26.32)\tAcc@3  46.56 ( 46.27)\n",
      "[2022-07-07 16:46:44,699 INFO] Epoch: [2][360/566]\tLoss 5.378 (5.678)\tInvT  19.04 ( 19.17)\tAcc@1  27.50 ( 26.41)\tAcc@3  49.06 ( 46.43)\n",
      "[2022-07-07 16:47:14,475 INFO] Epoch: [2][380/566]\tLoss 5.3 (5.669)\tInvT  19.03 ( 19.16)\tAcc@1  28.65 ( 26.49)\tAcc@3  50.73 ( 46.55)\n",
      "[2022-07-07 16:47:44,228 INFO] Epoch: [2][400/566]\tLoss 5.565 (5.663)\tInvT  19.02 ( 19.16)\tAcc@1  29.17 ( 26.57)\tAcc@3  48.85 ( 46.63)\n",
      "[2022-07-07 16:48:13,991 INFO] Epoch: [2][420/566]\tLoss 5.709 (5.655)\tInvT  19.01 ( 19.15)\tAcc@1  25.94 ( 26.65)\tAcc@3  45.62 ( 46.73)\n",
      "[2022-07-07 16:48:43,941 INFO] Epoch: [2][440/566]\tLoss 5.407 (5.647)\tInvT  18.99 ( 19.14)\tAcc@1  28.75 ( 26.74)\tAcc@3  48.75 ( 46.83)\n",
      "[2022-07-07 16:49:13,632 INFO] Epoch: [2][460/566]\tLoss 5.544 (5.637)\tInvT  18.98 ( 19.14)\tAcc@1  30.00 ( 26.83)\tAcc@3  49.48 ( 46.96)\n",
      "[2022-07-07 16:49:43,292 INFO] Epoch: [2][480/566]\tLoss 5.363 (5.627)\tInvT  18.97 ( 19.13)\tAcc@1  30.83 ( 26.93)\tAcc@3  49.17 ( 47.09)\n",
      "[2022-07-07 16:50:12,876 INFO] Epoch: [2][500/566]\tLoss 5.426 (5.62)\tInvT  18.95 ( 19.12)\tAcc@1  28.96 ( 27.01)\tAcc@3  50.10 ( 47.18)\n",
      "[2022-07-07 16:50:42,523 INFO] Epoch: [2][520/566]\tLoss 5.797 (5.613)\tInvT  18.94 ( 19.12)\tAcc@1  26.77 ( 27.08)\tAcc@3  45.21 ( 47.27)\n",
      "[2022-07-07 16:51:12,228 INFO] Epoch: [2][540/566]\tLoss 5.424 (5.602)\tInvT  18.93 ( 19.11)\tAcc@1  28.12 ( 27.19)\tAcc@3  49.69 ( 47.40)\n",
      "[2022-07-07 16:51:42,187 INFO] Epoch: [2][560/566]\tLoss 5.562 (5.594)\tInvT  18.92 ( 19.10)\tAcc@1  27.71 ( 27.27)\tAcc@3  49.58 ( 47.50)\n",
      "[2022-07-07 16:51:49,659 INFO] Learning rate: 3.768267223382046e-05\n",
      "[2022-07-07 16:52:15,832 INFO] Epoch 2, valid metric: {\"Acc@1\": 26.855, \"Acc@3\": 40.188, \"loss\": 3.269}\n",
      "[2022-07-07 16:52:21,459 INFO] Epoch: [3][  0/566]\tLoss 4.84 (4.84)\tInvT  18.91 ( 18.91)\tAcc@1  33.12 ( 33.12)\tAcc@3  54.48 ( 54.48)\n",
      "[2022-07-07 16:52:51,187 INFO] Epoch: [3][ 20/566]\tLoss 5.141 (5.065)\tInvT  18.91 ( 18.91)\tAcc@1  32.40 ( 32.42)\tAcc@3  53.75 ( 53.90)\n",
      "[2022-07-07 16:53:20,816 INFO] Epoch: [3][ 40/566]\tLoss 4.765 (5.071)\tInvT  18.90 ( 18.91)\tAcc@1  34.38 ( 32.34)\tAcc@3  56.88 ( 53.80)\n",
      "[2022-07-07 16:53:50,518 INFO] Epoch: [3][ 60/566]\tLoss 5.412 (5.087)\tInvT  18.89 ( 18.90)\tAcc@1  30.42 ( 32.30)\tAcc@3  51.04 ( 53.55)\n",
      "[2022-07-07 16:54:20,061 INFO] Epoch: [3][ 80/566]\tLoss 5.081 (5.086)\tInvT  18.89 ( 18.90)\tAcc@1  31.88 ( 32.38)\tAcc@3  53.44 ( 53.60)\n",
      "[2022-07-07 16:54:49,722 INFO] Epoch: [3][100/566]\tLoss 5.157 (5.086)\tInvT  18.88 ( 18.90)\tAcc@1  31.88 ( 32.42)\tAcc@3  52.29 ( 53.72)\n",
      "[2022-07-07 16:55:19,511 INFO] Epoch: [3][120/566]\tLoss 5.349 (5.082)\tInvT  18.88 ( 18.89)\tAcc@1  30.10 ( 32.42)\tAcc@3  51.25 ( 53.81)\n",
      "[2022-07-07 16:55:49,297 INFO] Epoch: [3][140/566]\tLoss 5.087 (5.075)\tInvT  18.87 ( 18.89)\tAcc@1  34.48 ( 32.54)\tAcc@3  53.02 ( 53.89)\n",
      "[2022-07-07 16:56:19,140 INFO] Epoch: [3][160/566]\tLoss 5.108 (5.074)\tInvT  18.87 ( 18.89)\tAcc@1  35.00 ( 32.65)\tAcc@3  54.48 ( 53.91)\n",
      "[2022-07-07 16:56:48,962 INFO] Epoch: [3][180/566]\tLoss 5.466 (5.069)\tInvT  18.86 ( 18.89)\tAcc@1  30.73 ( 32.75)\tAcc@3  49.38 ( 53.99)\n",
      "[2022-07-07 16:57:18,770 INFO] Epoch: [3][200/566]\tLoss 5.049 (5.068)\tInvT  18.85 ( 18.88)\tAcc@1  32.19 ( 32.76)\tAcc@3  55.10 ( 54.02)\n",
      "[2022-07-07 16:57:48,421 INFO] Epoch: [3][220/566]\tLoss 4.913 (5.061)\tInvT  18.85 ( 18.88)\tAcc@1  34.58 ( 32.86)\tAcc@3  55.94 ( 54.12)\n",
      "[2022-07-07 16:58:18,079 INFO] Epoch: [3][240/566]\tLoss 5.197 (5.058)\tInvT  18.84 ( 18.88)\tAcc@1  32.50 ( 32.89)\tAcc@3  51.15 ( 54.17)\n",
      "[2022-07-07 16:58:47,482 INFO] Epoch: [3][260/566]\tLoss 4.984 (5.057)\tInvT  18.84 ( 18.87)\tAcc@1  34.17 ( 32.98)\tAcc@3  55.31 ( 54.20)\n",
      "[2022-07-07 16:59:17,224 INFO] Epoch: [3][280/566]\tLoss 4.813 (5.056)\tInvT  18.83 ( 18.87)\tAcc@1  36.56 ( 33.02)\tAcc@3  55.52 ( 54.25)\n",
      "[2022-07-07 16:59:47,106 INFO] Epoch: [3][300/566]\tLoss 5.265 (5.052)\tInvT  18.83 ( 18.87)\tAcc@1  33.54 ( 33.04)\tAcc@3  52.08 ( 54.30)\n",
      "[2022-07-07 17:00:16,869 INFO] Epoch: [3][320/566]\tLoss 5.031 (5.048)\tInvT  18.82 ( 18.87)\tAcc@1  30.94 ( 33.13)\tAcc@3  54.17 ( 54.36)\n",
      "[2022-07-07 17:00:46,490 INFO] Epoch: [3][340/566]\tLoss 5.074 (5.047)\tInvT  18.82 ( 18.86)\tAcc@1  34.48 ( 33.19)\tAcc@3  53.85 ( 54.38)\n",
      "[2022-07-07 17:01:16,218 INFO] Epoch: [3][360/566]\tLoss 4.812 (5.04)\tInvT  18.81 ( 18.86)\tAcc@1  34.38 ( 33.26)\tAcc@3  58.33 ( 54.51)\n",
      "[2022-07-07 17:01:45,946 INFO] Epoch: [3][380/566]\tLoss 4.9 (5.037)\tInvT  18.81 ( 18.86)\tAcc@1  33.75 ( 33.29)\tAcc@3  55.00 ( 54.57)\n",
      "[2022-07-07 17:02:15,741 INFO] Epoch: [3][400/566]\tLoss 4.795 (5.031)\tInvT  18.81 ( 18.86)\tAcc@1  35.31 ( 33.38)\tAcc@3  56.77 ( 54.64)\n",
      "[2022-07-07 17:02:45,415 INFO] Epoch: [3][420/566]\tLoss 5.005 (5.026)\tInvT  18.80 ( 18.85)\tAcc@1  35.62 ( 33.44)\tAcc@3  56.46 ( 54.69)\n",
      "[2022-07-07 17:03:15,230 INFO] Epoch: [3][440/566]\tLoss 5.053 (5.023)\tInvT  18.80 ( 18.85)\tAcc@1  33.02 ( 33.51)\tAcc@3  54.48 ( 54.74)\n",
      "[2022-07-07 17:03:45,029 INFO] Epoch: [3][460/566]\tLoss 4.98 (5.019)\tInvT  18.80 ( 18.85)\tAcc@1  34.58 ( 33.55)\tAcc@3  55.52 ( 54.80)\n",
      "[2022-07-07 17:04:14,640 INFO] Epoch: [3][480/566]\tLoss 4.768 (5.016)\tInvT  18.79 ( 18.85)\tAcc@1  36.88 ( 33.59)\tAcc@3  57.71 ( 54.84)\n",
      "[2022-07-07 17:04:44,426 INFO] Epoch: [3][500/566]\tLoss 5.117 (5.014)\tInvT  18.79 ( 18.84)\tAcc@1  32.71 ( 33.62)\tAcc@3  55.62 ( 54.88)\n",
      "[2022-07-07 17:05:14,097 INFO] Epoch: [3][520/566]\tLoss 4.82 (5.01)\tInvT  18.79 ( 18.84)\tAcc@1  36.35 ( 33.66)\tAcc@3  58.12 ( 54.94)\n",
      "[2022-07-07 17:05:43,837 INFO] Epoch: [3][540/566]\tLoss 4.787 (5.007)\tInvT  18.78 ( 18.84)\tAcc@1  35.31 ( 33.70)\tAcc@3  57.29 ( 54.99)\n",
      "[2022-07-07 17:06:13,677 INFO] Epoch: [3][560/566]\tLoss 5.041 (5.004)\tInvT  18.78 ( 18.84)\tAcc@1  32.40 ( 33.74)\tAcc@3  55.00 ( 55.03)\n",
      "[2022-07-07 17:06:21,088 INFO] Learning rate: 3.231163408616436e-05\n",
      "[2022-07-07 17:06:47,373 INFO] Epoch 3, valid metric: {\"Acc@1\": 28.206, \"Acc@3\": 41.722, \"loss\": 3.197}\n",
      "[2022-07-07 17:06:53,023 INFO] Epoch: [4][  0/566]\tLoss 4.45 (4.45)\tInvT  18.78 ( 18.78)\tAcc@1  40.52 ( 40.52)\tAcc@3  61.88 ( 61.88)\n",
      "[2022-07-07 17:07:22,657 INFO] Epoch: [4][ 20/566]\tLoss 4.462 (4.621)\tInvT  18.78 ( 18.78)\tAcc@1  39.58 ( 38.47)\tAcc@3  60.10 ( 59.74)\n",
      "[2022-07-07 17:07:52,305 INFO] Epoch: [4][ 40/566]\tLoss 4.725 (4.615)\tInvT  18.79 ( 18.78)\tAcc@1  34.58 ( 38.15)\tAcc@3  57.50 ( 59.77)\n",
      "[2022-07-07 17:08:22,032 INFO] Epoch: [4][ 60/566]\tLoss 4.564 (4.62)\tInvT  18.80 ( 18.79)\tAcc@1  36.67 ( 38.23)\tAcc@3  60.31 ( 59.82)\n",
      "[2022-07-07 17:08:51,639 INFO] Epoch: [4][ 80/566]\tLoss 4.727 (4.621)\tInvT  18.80 ( 18.79)\tAcc@1  38.85 ( 38.26)\tAcc@3  59.27 ( 59.89)\n",
      "[2022-07-07 17:09:21,412 INFO] Epoch: [4][100/566]\tLoss 4.639 (4.62)\tInvT  18.81 ( 18.79)\tAcc@1  36.98 ( 38.33)\tAcc@3  59.17 ( 59.93)\n",
      "[2022-07-07 17:09:51,067 INFO] Epoch: [4][120/566]\tLoss 4.793 (4.621)\tInvT  18.82 ( 18.80)\tAcc@1  37.29 ( 38.44)\tAcc@3  60.10 ( 59.94)\n",
      "[2022-07-07 17:10:20,805 INFO] Epoch: [4][140/566]\tLoss 4.616 (4.621)\tInvT  18.82 ( 18.80)\tAcc@1  37.71 ( 38.48)\tAcc@3  59.48 ( 59.91)\n",
      "[2022-07-07 17:10:50,251 INFO] Epoch: [4][160/566]\tLoss 4.68 (4.622)\tInvT  18.83 ( 18.80)\tAcc@1  38.96 ( 38.52)\tAcc@3  61.15 ( 59.93)\n",
      "[2022-07-07 17:11:20,000 INFO] Epoch: [4][180/566]\tLoss 4.457 (4.624)\tInvT  18.84 ( 18.81)\tAcc@1  39.90 ( 38.52)\tAcc@3  61.46 ( 59.87)\n",
      "[2022-07-07 17:11:49,792 INFO] Epoch: [4][200/566]\tLoss 4.613 (4.621)\tInvT  18.84 ( 18.81)\tAcc@1  38.75 ( 38.53)\tAcc@3  60.31 ( 59.87)\n",
      "[2022-07-07 17:12:19,559 INFO] Epoch: [4][220/566]\tLoss 4.61 (4.624)\tInvT  18.85 ( 18.81)\tAcc@1  38.96 ( 38.52)\tAcc@3  60.73 ( 59.84)\n",
      "[2022-07-07 17:12:49,349 INFO] Epoch: [4][240/566]\tLoss 4.411 (4.624)\tInvT  18.85 ( 18.82)\tAcc@1  41.88 ( 38.54)\tAcc@3  63.02 ( 59.86)\n",
      "[2022-07-07 17:13:19,064 INFO] Epoch: [4][260/566]\tLoss 4.632 (4.622)\tInvT  18.86 ( 18.82)\tAcc@1  37.92 ( 38.57)\tAcc@3  59.27 ( 59.90)\n",
      "[2022-07-07 17:13:48,754 INFO] Epoch: [4][280/566]\tLoss 4.638 (4.621)\tInvT  18.86 ( 18.82)\tAcc@1  39.79 ( 38.58)\tAcc@3  60.52 ( 59.94)\n",
      "[2022-07-07 17:14:18,237 INFO] Epoch: [4][300/566]\tLoss 4.451 (4.616)\tInvT  18.87 ( 18.82)\tAcc@1  40.42 ( 38.63)\tAcc@3  59.17 ( 59.99)\n",
      "[2022-07-07 17:14:47,907 INFO] Epoch: [4][320/566]\tLoss 4.688 (4.615)\tInvT  18.88 ( 18.83)\tAcc@1  37.50 ( 38.64)\tAcc@3  57.92 ( 60.02)\n",
      "[2022-07-07 17:15:17,446 INFO] Epoch: [4][340/566]\tLoss 4.684 (4.612)\tInvT  18.88 ( 18.83)\tAcc@1  38.96 ( 38.69)\tAcc@3  59.79 ( 60.06)\n",
      "[2022-07-07 17:15:47,180 INFO] Epoch: [4][360/566]\tLoss 4.684 (4.61)\tInvT  18.89 ( 18.83)\tAcc@1  38.85 ( 38.73)\tAcc@3  60.83 ( 60.08)\n",
      "[2022-07-07 17:16:17,000 INFO] Epoch: [4][380/566]\tLoss 4.624 (4.608)\tInvT  18.89 ( 18.84)\tAcc@1  37.08 ( 38.73)\tAcc@3  58.75 ( 60.09)\n",
      "[2022-07-07 17:16:46,854 INFO] Epoch: [4][400/566]\tLoss 4.416 (4.609)\tInvT  18.90 ( 18.84)\tAcc@1  40.21 ( 38.74)\tAcc@3  61.46 ( 60.09)\n",
      "[2022-07-07 17:17:16,693 INFO] Epoch: [4][420/566]\tLoss 4.756 (4.609)\tInvT  18.91 ( 18.84)\tAcc@1  35.94 ( 38.75)\tAcc@3  56.56 ( 60.08)\n",
      "[2022-07-07 17:17:46,417 INFO] Epoch: [4][440/566]\tLoss 4.509 (4.607)\tInvT  18.91 ( 18.85)\tAcc@1  41.04 ( 38.78)\tAcc@3  61.98 ( 60.11)\n",
      "[2022-07-07 17:18:16,191 INFO] Epoch: [4][460/566]\tLoss 4.607 (4.605)\tInvT  18.92 ( 18.85)\tAcc@1  38.54 ( 38.81)\tAcc@3  60.42 ( 60.13)\n",
      "[2022-07-07 17:18:46,039 INFO] Epoch: [4][480/566]\tLoss 4.435 (4.602)\tInvT  18.93 ( 18.85)\tAcc@1  39.48 ( 38.83)\tAcc@3  62.50 ( 60.17)\n",
      "[2022-07-07 17:19:15,944 INFO] Epoch: [4][500/566]\tLoss 4.533 (4.601)\tInvT  18.93 ( 18.85)\tAcc@1  39.06 ( 38.86)\tAcc@3  62.08 ( 60.19)\n",
      "[2022-07-07 17:19:45,655 INFO] Epoch: [4][520/566]\tLoss 4.269 (4.597)\tInvT  18.94 ( 18.86)\tAcc@1  41.67 ( 38.91)\tAcc@3  65.10 ( 60.23)\n",
      "[2022-07-07 17:20:15,409 INFO] Epoch: [4][540/566]\tLoss 4.715 (4.597)\tInvT  18.95 ( 18.86)\tAcc@1  36.77 ( 38.92)\tAcc@3  59.06 ( 60.24)\n",
      "[2022-07-07 17:20:44,998 INFO] Epoch: [4][560/566]\tLoss 4.504 (4.593)\tInvT  18.95 ( 18.86)\tAcc@1  40.83 ( 38.96)\tAcc@3  61.88 ( 60.29)\n",
      "[2022-07-07 17:20:52,435 INFO] Learning rate: 2.6940595938508257e-05\n",
      "[2022-07-07 17:21:18,628 INFO] Epoch 4, valid metric: {\"Acc@1\": 28.973, \"Acc@3\": 42.746, \"loss\": 3.145}\n",
      "[2022-07-07 17:21:24,187 INFO] Epoch: [5][  0/566]\tLoss 4.197 (4.197)\tInvT  18.96 ( 18.96)\tAcc@1  42.29 ( 42.29)\tAcc@3  64.79 ( 64.79)\n",
      "[2022-07-07 17:21:53,889 INFO] Epoch: [5][ 20/566]\tLoss 4.333 (4.255)\tInvT  18.97 ( 18.96)\tAcc@1  39.48 ( 42.65)\tAcc@3  65.21 ( 64.54)\n",
      "[2022-07-07 17:22:23,795 INFO] Epoch: [5][ 40/566]\tLoss 4.419 (4.284)\tInvT  18.99 ( 18.97)\tAcc@1  39.79 ( 42.35)\tAcc@3  61.67 ( 64.11)\n",
      "[2022-07-07 17:22:53,381 INFO] Epoch: [5][ 60/566]\tLoss 4.142 (4.272)\tInvT  19.00 ( 18.98)\tAcc@1  43.54 ( 42.61)\tAcc@3  66.25 ( 64.19)\n",
      "[2022-07-07 17:23:23,189 INFO] Epoch: [5][ 80/566]\tLoss 4.361 (4.274)\tInvT  19.02 ( 18.99)\tAcc@1  40.83 ( 42.56)\tAcc@3  63.44 ( 64.17)\n",
      "[2022-07-07 17:23:53,019 INFO] Epoch: [5][100/566]\tLoss 4.182 (4.269)\tInvT  19.03 ( 18.99)\tAcc@1  42.29 ( 42.70)\tAcc@3  64.90 ( 64.25)\n",
      "[2022-07-07 17:24:22,938 INFO] Epoch: [5][120/566]\tLoss 4.484 (4.276)\tInvT  19.05 ( 19.00)\tAcc@1  40.42 ( 42.69)\tAcc@3  62.08 ( 64.21)\n",
      "[2022-07-07 17:24:52,410 INFO] Epoch: [5][140/566]\tLoss 4.342 (4.276)\tInvT  19.06 ( 19.01)\tAcc@1  42.92 ( 42.79)\tAcc@3  62.71 ( 64.19)\n",
      "[2022-07-07 17:25:22,062 INFO] Epoch: [5][160/566]\tLoss 4.472 (4.277)\tInvT  19.07 ( 19.02)\tAcc@1  41.67 ( 42.81)\tAcc@3  62.29 ( 64.20)\n",
      "[2022-07-07 17:25:51,867 INFO] Epoch: [5][180/566]\tLoss 4.47 (4.284)\tInvT  19.08 ( 19.02)\tAcc@1  40.83 ( 42.76)\tAcc@3  62.50 ( 64.13)\n",
      "[2022-07-07 17:26:21,630 INFO] Epoch: [5][200/566]\tLoss 4.449 (4.284)\tInvT  19.09 ( 19.03)\tAcc@1  40.62 ( 42.80)\tAcc@3  60.73 ( 64.09)\n",
      "[2022-07-07 17:26:51,390 INFO] Epoch: [5][220/566]\tLoss 4.386 (4.286)\tInvT  19.11 ( 19.04)\tAcc@1  42.19 ( 42.81)\tAcc@3  63.54 ( 64.07)\n",
      "[2022-07-07 17:27:21,079 INFO] Epoch: [5][240/566]\tLoss 4.491 (4.288)\tInvT  19.12 ( 19.04)\tAcc@1  41.77 ( 42.80)\tAcc@3  61.04 ( 64.06)\n",
      "[2022-07-07 17:27:50,942 INFO] Epoch: [5][260/566]\tLoss 4.255 (4.286)\tInvT  19.13 ( 19.05)\tAcc@1  40.94 ( 42.81)\tAcc@3  64.58 ( 64.07)\n",
      "[2022-07-07 17:28:20,740 INFO] Epoch: [5][280/566]\tLoss 4.42 (4.286)\tInvT  19.14 ( 19.05)\tAcc@1  43.44 ( 42.83)\tAcc@3  60.21 ( 64.06)\n",
      "[2022-07-07 17:28:50,556 INFO] Epoch: [5][300/566]\tLoss 4.255 (4.287)\tInvT  19.15 ( 19.06)\tAcc@1  43.54 ( 42.84)\tAcc@3  64.58 ( 64.08)\n",
      "[2022-07-07 17:29:20,454 INFO] Epoch: [5][320/566]\tLoss 4.135 (4.286)\tInvT  19.16 ( 19.07)\tAcc@1  46.88 ( 42.85)\tAcc@3  65.94 ( 64.07)\n",
      "[2022-07-07 17:29:49,916 INFO] Epoch: [5][340/566]\tLoss 4.349 (4.282)\tInvT  19.17 ( 19.07)\tAcc@1  41.15 ( 42.89)\tAcc@3  64.38 ( 64.10)\n",
      "[2022-07-07 17:30:19,653 INFO] Epoch: [5][360/566]\tLoss 4.313 (4.28)\tInvT  19.19 ( 19.08)\tAcc@1  42.19 ( 42.95)\tAcc@3  64.17 ( 64.15)\n",
      "[2022-07-07 17:30:49,444 INFO] Epoch: [5][380/566]\tLoss 4.062 (4.277)\tInvT  19.19 ( 19.08)\tAcc@1  44.17 ( 42.96)\tAcc@3  65.52 ( 64.17)\n",
      "[2022-07-07 17:31:19,175 INFO] Epoch: [5][400/566]\tLoss 4.215 (4.276)\tInvT  19.21 ( 19.09)\tAcc@1  42.81 ( 42.98)\tAcc@3  65.31 ( 64.21)\n",
      "[2022-07-07 17:31:48,935 INFO] Epoch: [5][420/566]\tLoss 4.286 (4.277)\tInvT  19.22 ( 19.10)\tAcc@1  42.40 ( 42.98)\tAcc@3  61.46 ( 64.18)\n",
      "[2022-07-07 17:32:18,687 INFO] Epoch: [5][440/566]\tLoss 4.393 (4.277)\tInvT  19.23 ( 19.10)\tAcc@1  41.56 ( 43.01)\tAcc@3  63.54 ( 64.19)\n",
      "[2022-07-07 17:32:48,422 INFO] Epoch: [5][460/566]\tLoss 4.324 (4.274)\tInvT  19.24 ( 19.11)\tAcc@1  42.71 ( 43.04)\tAcc@3  62.71 ( 64.21)\n",
      "[2022-07-07 17:33:17,854 INFO] Epoch: [5][480/566]\tLoss 4.321 (4.276)\tInvT  19.25 ( 19.11)\tAcc@1  43.33 ( 43.05)\tAcc@3  63.96 ( 64.20)\n",
      "[2022-07-07 17:33:47,706 INFO] Epoch: [5][500/566]\tLoss 4.139 (4.275)\tInvT  19.26 ( 19.12)\tAcc@1  44.06 ( 43.08)\tAcc@3  66.98 ( 64.21)\n",
      "[2022-07-07 17:34:17,455 INFO] Epoch: [5][520/566]\tLoss 4.311 (4.275)\tInvT  19.26 ( 19.12)\tAcc@1  40.94 ( 43.09)\tAcc@3  63.33 ( 64.21)\n",
      "[2022-07-07 17:34:47,241 INFO] Epoch: [5][540/566]\tLoss 4.22 (4.275)\tInvT  19.27 ( 19.13)\tAcc@1  43.12 ( 43.10)\tAcc@3  63.65 ( 64.22)\n",
      "[2022-07-07 17:35:17,173 INFO] Epoch: [5][560/566]\tLoss 4.523 (4.275)\tInvT  19.28 ( 19.13)\tAcc@1  41.04 ( 43.12)\tAcc@3  62.19 ( 64.23)\n",
      "[2022-07-07 17:35:24,629 INFO] Learning rate: 2.1569557790852155e-05\n",
      "[2022-07-07 17:35:51,283 INFO] Epoch 5, valid metric: {\"Acc@1\": 29.173, \"Acc@3\": 43.393, \"loss\": 3.13}\n",
      "[2022-07-07 17:35:54,167 INFO] Delete old checkpoint ./checkpoint/fb15k237/checkpoint_epoch0.mdl\n",
      "[2022-07-07 17:35:57,079 INFO] Epoch: [6][  0/566]\tLoss 4.051 (4.051)\tInvT  19.29 ( 19.29)\tAcc@1  46.56 ( 46.56)\tAcc@3  66.25 ( 66.25)\n",
      "[2022-07-07 17:36:26,671 INFO] Epoch: [6][ 20/566]\tLoss 3.935 (3.999)\tInvT  19.30 ( 19.29)\tAcc@1  48.65 ( 46.77)\tAcc@3  68.02 ( 67.38)\n",
      "[2022-07-07 17:36:56,361 INFO] Epoch: [6][ 40/566]\tLoss 3.827 (3.987)\tInvT  19.31 ( 19.30)\tAcc@1  49.69 ( 46.98)\tAcc@3  70.73 ( 67.59)\n",
      "[2022-07-07 17:37:25,997 INFO] Epoch: [6][ 60/566]\tLoss 3.877 (3.987)\tInvT  19.33 ( 19.31)\tAcc@1  49.69 ( 46.90)\tAcc@3  69.48 ( 67.61)\n",
      "[2022-07-07 17:37:55,651 INFO] Epoch: [6][ 80/566]\tLoss 4.002 (3.987)\tInvT  19.34 ( 19.31)\tAcc@1  47.92 ( 46.99)\tAcc@3  69.58 ( 67.69)\n",
      "[2022-07-07 17:38:25,305 INFO] Epoch: [6][100/566]\tLoss 3.767 (3.998)\tInvT  19.36 ( 19.32)\tAcc@1  46.77 ( 46.86)\tAcc@3  70.31 ( 67.53)\n",
      "[2022-07-07 17:38:54,938 INFO] Epoch: [6][120/566]\tLoss 4.006 (4.0)\tInvT  19.37 ( 19.33)\tAcc@1  45.94 ( 46.79)\tAcc@3  65.73 ( 67.51)\n",
      "[2022-07-07 17:39:24,799 INFO] Epoch: [6][140/566]\tLoss 4.197 (4.009)\tInvT  19.38 ( 19.33)\tAcc@1  44.79 ( 46.74)\tAcc@3  64.58 ( 67.43)\n",
      "[2022-07-07 17:39:54,562 INFO] Epoch: [6][160/566]\tLoss 4.329 (4.009)\tInvT  19.39 ( 19.34)\tAcc@1  44.90 ( 46.70)\tAcc@3  64.58 ( 67.44)\n",
      "[2022-07-07 17:40:24,266 INFO] Epoch: [6][180/566]\tLoss 4.088 (4.013)\tInvT  19.40 ( 19.35)\tAcc@1  45.73 ( 46.69)\tAcc@3  66.46 ( 67.41)\n",
      "[2022-07-07 17:40:54,052 INFO] Epoch: [6][200/566]\tLoss 3.95 (4.011)\tInvT  19.41 ( 19.35)\tAcc@1  47.29 ( 46.67)\tAcc@3  67.81 ( 67.40)\n",
      "[2022-07-07 17:41:23,889 INFO] Epoch: [6][220/566]\tLoss 3.945 (4.013)\tInvT  19.43 ( 19.36)\tAcc@1  47.08 ( 46.62)\tAcc@3  68.75 ( 67.36)\n",
      "[2022-07-07 17:41:53,693 INFO] Epoch: [6][240/566]\tLoss 3.968 (4.012)\tInvT  19.44 ( 19.36)\tAcc@1  45.94 ( 46.66)\tAcc@3  69.06 ( 67.35)\n",
      "[2022-07-07 17:42:23,554 INFO] Epoch: [6][260/566]\tLoss 4.057 (4.012)\tInvT  19.45 ( 19.37)\tAcc@1  47.29 ( 46.66)\tAcc@3  65.83 ( 67.35)\n",
      "[2022-07-07 17:42:53,431 INFO] Epoch: [6][280/566]\tLoss 4.105 (4.014)\tInvT  19.46 ( 19.38)\tAcc@1  46.15 ( 46.65)\tAcc@3  66.56 ( 67.31)\n",
      "[2022-07-07 17:43:23,137 INFO] Epoch: [6][300/566]\tLoss 3.924 (4.014)\tInvT  19.47 ( 19.38)\tAcc@1  46.46 ( 46.68)\tAcc@3  67.08 ( 67.30)\n",
      "[2022-07-07 17:43:52,779 INFO] Epoch: [6][320/566]\tLoss 3.954 (4.013)\tInvT  19.48 ( 19.39)\tAcc@1  46.56 ( 46.71)\tAcc@3  68.44 ( 67.31)\n",
      "[2022-07-07 17:44:22,250 INFO] Epoch: [6][340/566]\tLoss 3.92 (4.01)\tInvT  19.49 ( 19.39)\tAcc@1  47.92 ( 46.72)\tAcc@3  66.88 ( 67.35)\n",
      "[2022-07-07 17:44:52,011 INFO] Epoch: [6][360/566]\tLoss 3.935 (4.011)\tInvT  19.49 ( 19.40)\tAcc@1  47.19 ( 46.70)\tAcc@3  69.27 ( 67.34)\n",
      "[2022-07-07 17:45:21,848 INFO] Epoch: [6][380/566]\tLoss 3.848 (4.011)\tInvT  19.50 ( 19.40)\tAcc@1  47.60 ( 46.72)\tAcc@3  68.33 ( 67.33)\n",
      "[2022-07-07 17:45:51,488 INFO] Epoch: [6][400/566]\tLoss 4.06 (4.011)\tInvT  19.51 ( 19.41)\tAcc@1  45.83 ( 46.72)\tAcc@3  66.98 ( 67.34)\n",
      "[2022-07-07 17:46:21,249 INFO] Epoch: [6][420/566]\tLoss 4.058 (4.014)\tInvT  19.52 ( 19.41)\tAcc@1  47.29 ( 46.73)\tAcc@3  67.19 ( 67.30)\n",
      "[2022-07-07 17:46:51,106 INFO] Epoch: [6][440/566]\tLoss 3.972 (4.012)\tInvT  19.53 ( 19.42)\tAcc@1  46.77 ( 46.73)\tAcc@3  66.35 ( 67.30)\n",
      "[2022-07-07 17:47:20,966 INFO] Epoch: [6][460/566]\tLoss 3.94 (4.012)\tInvT  19.54 ( 19.42)\tAcc@1  47.92 ( 46.74)\tAcc@3  67.29 ( 67.32)\n",
      "[2022-07-07 17:47:50,828 INFO] Epoch: [6][480/566]\tLoss 4.004 (4.012)\tInvT  19.55 ( 19.43)\tAcc@1  47.50 ( 46.74)\tAcc@3  68.44 ( 67.32)\n",
      "[2022-07-07 17:48:20,187 INFO] Epoch: [6][500/566]\tLoss 3.97 (4.011)\tInvT  19.55 ( 19.43)\tAcc@1  49.79 ( 46.79)\tAcc@3  69.06 ( 67.34)\n",
      "[2022-07-07 17:48:50,051 INFO] Epoch: [6][520/566]\tLoss 4.113 (4.011)\tInvT  19.56 ( 19.44)\tAcc@1  45.83 ( 46.81)\tAcc@3  66.35 ( 67.36)\n",
      "[2022-07-07 17:49:19,862 INFO] Epoch: [6][540/566]\tLoss 4.091 (4.012)\tInvT  19.57 ( 19.44)\tAcc@1  46.67 ( 46.80)\tAcc@3  65.73 ( 67.35)\n",
      "[2022-07-07 17:49:49,594 INFO] Epoch: [6][560/566]\tLoss 3.946 (4.01)\tInvT  19.58 ( 19.45)\tAcc@1  47.92 ( 46.82)\tAcc@3  69.06 ( 67.35)\n",
      "[2022-07-07 17:49:57,053 INFO] Learning rate: 1.6198519643196054e-05\n",
      "[2022-07-07 17:50:23,287 INFO] Epoch 6, valid metric: {\"Acc@1\": 29.287, \"Acc@3\": 43.49, \"loss\": 3.14}\n",
      "[2022-07-07 17:50:26,123 INFO] Delete old checkpoint ./checkpoint/fb15k237/checkpoint_epoch1.mdl\n",
      "[2022-07-07 17:50:29,034 INFO] Epoch: [7][  0/566]\tLoss 3.646 (3.646)\tInvT  19.58 ( 19.58)\tAcc@1  51.35 ( 51.35)\tAcc@3  71.56 ( 71.56)\n",
      "[2022-07-07 17:50:58,731 INFO] Epoch: [7][ 20/566]\tLoss 3.431 (3.77)\tInvT  19.59 ( 19.59)\tAcc@1  53.23 ( 49.85)\tAcc@3  74.17 ( 70.15)\n",
      "[2022-07-07 17:51:28,399 INFO] Epoch: [7][ 40/566]\tLoss 3.899 (3.783)\tInvT  19.60 ( 19.59)\tAcc@1  49.17 ( 49.85)\tAcc@3  70.62 ( 70.12)\n",
      "[2022-07-07 17:51:58,059 INFO] Epoch: [7][ 60/566]\tLoss 3.576 (3.783)\tInvT  19.61 ( 19.60)\tAcc@1  52.60 ( 49.84)\tAcc@3  72.60 ( 70.14)\n",
      "[2022-07-07 17:52:27,650 INFO] Epoch: [7][ 80/566]\tLoss 3.775 (3.783)\tInvT  19.62 ( 19.60)\tAcc@1  47.50 ( 49.77)\tAcc@3  69.58 ( 70.06)\n",
      "[2022-07-07 17:52:57,346 INFO] Epoch: [7][100/566]\tLoss 3.736 (3.787)\tInvT  19.63 ( 19.61)\tAcc@1  49.69 ( 49.69)\tAcc@3  70.52 ( 70.07)\n",
      "[2022-07-07 17:53:27,109 INFO] Epoch: [7][120/566]\tLoss 3.808 (3.789)\tInvT  19.64 ( 19.61)\tAcc@1  49.27 ( 49.64)\tAcc@3  68.65 ( 70.02)\n",
      "[2022-07-07 17:53:56,914 INFO] Epoch: [7][140/566]\tLoss 3.948 (3.791)\tInvT  19.65 ( 19.62)\tAcc@1  47.81 ( 49.63)\tAcc@3  68.33 ( 69.99)\n",
      "[2022-07-07 17:54:26,589 INFO] Epoch: [7][160/566]\tLoss 3.977 (3.796)\tInvT  19.66 ( 19.62)\tAcc@1  46.46 ( 49.55)\tAcc@3  68.54 ( 70.00)\n",
      "[2022-07-07 17:54:56,215 INFO] Epoch: [7][180/566]\tLoss 3.718 (3.792)\tInvT  19.66 ( 19.62)\tAcc@1  49.48 ( 49.58)\tAcc@3  69.06 ( 70.03)\n",
      "[2022-07-07 17:55:25,842 INFO] Epoch: [7][200/566]\tLoss 3.835 (3.795)\tInvT  19.67 ( 19.63)\tAcc@1  50.31 ( 49.61)\tAcc@3  69.17 ( 69.96)\n",
      "[2022-07-07 17:55:55,740 INFO] Epoch: [7][220/566]\tLoss 3.721 (3.796)\tInvT  19.68 ( 19.63)\tAcc@1  50.10 ( 49.64)\tAcc@3  71.46 ( 69.93)\n",
      "[2022-07-07 17:56:25,619 INFO] Epoch: [7][240/566]\tLoss 3.778 (3.796)\tInvT  19.69 ( 19.64)\tAcc@1  48.75 ( 49.66)\tAcc@3  70.21 ( 69.89)\n",
      "[2022-07-07 17:56:55,335 INFO] Epoch: [7][260/566]\tLoss 3.805 (3.796)\tInvT  19.70 ( 19.64)\tAcc@1  48.96 ( 49.66)\tAcc@3  69.79 ( 69.91)\n",
      "[2022-07-07 17:57:25,050 INFO] Epoch: [7][280/566]\tLoss 3.772 (3.797)\tInvT  19.70 ( 19.65)\tAcc@1  51.04 ( 49.66)\tAcc@3  71.56 ( 69.93)\n",
      "[2022-07-07 17:57:54,899 INFO] Epoch: [7][300/566]\tLoss 3.72 (3.794)\tInvT  19.71 ( 19.65)\tAcc@1  50.31 ( 49.67)\tAcc@3  69.06 ( 69.93)\n",
      "[2022-07-07 17:58:24,771 INFO] Epoch: [7][320/566]\tLoss 3.684 (3.793)\tInvT  19.72 ( 19.65)\tAcc@1  50.83 ( 49.69)\tAcc@3  71.04 ( 69.94)\n",
      "[2022-07-07 17:58:54,657 INFO] Epoch: [7][340/566]\tLoss 3.832 (3.793)\tInvT  19.72 ( 19.66)\tAcc@1  48.65 ( 49.70)\tAcc@3  69.27 ( 69.92)\n",
      "[2022-07-07 17:59:24,288 INFO] Epoch: [7][360/566]\tLoss 3.823 (3.791)\tInvT  19.73 ( 19.66)\tAcc@1  50.31 ( 49.72)\tAcc@3  70.94 ( 69.94)\n",
      "[2022-07-07 17:59:54,136 INFO] Epoch: [7][380/566]\tLoss 3.684 (3.791)\tInvT  19.74 ( 19.67)\tAcc@1  50.31 ( 49.73)\tAcc@3  71.15 ( 69.95)\n",
      "[2022-07-07 18:00:23,521 INFO] Epoch: [7][400/566]\tLoss 3.938 (3.791)\tInvT  19.74 ( 19.67)\tAcc@1  47.60 ( 49.73)\tAcc@3  67.29 ( 69.94)\n",
      "[2022-07-07 18:00:53,144 INFO] Epoch: [7][420/566]\tLoss 4.003 (3.792)\tInvT  19.75 ( 19.67)\tAcc@1  49.06 ( 49.75)\tAcc@3  67.50 ( 69.93)\n",
      "[2022-07-07 18:01:22,993 INFO] Epoch: [7][440/566]\tLoss 3.812 (3.793)\tInvT  19.76 ( 19.68)\tAcc@1  49.69 ( 49.74)\tAcc@3  68.65 ( 69.92)\n",
      "[2022-07-07 18:01:52,824 INFO] Epoch: [7][460/566]\tLoss 3.948 (3.793)\tInvT  19.76 ( 19.68)\tAcc@1  49.38 ( 49.75)\tAcc@3  67.40 ( 69.91)\n",
      "[2022-07-07 18:02:22,594 INFO] Epoch: [7][480/566]\tLoss 3.854 (3.792)\tInvT  19.77 ( 19.68)\tAcc@1  49.58 ( 49.77)\tAcc@3  70.00 ( 69.93)\n",
      "[2022-07-07 18:02:52,357 INFO] Epoch: [7][500/566]\tLoss 4.026 (3.791)\tInvT  19.77 ( 19.69)\tAcc@1  46.46 ( 49.80)\tAcc@3  66.77 ( 69.96)\n",
      "[2022-07-07 18:03:22,016 INFO] Epoch: [7][520/566]\tLoss 3.779 (3.792)\tInvT  19.78 ( 19.69)\tAcc@1  51.46 ( 49.80)\tAcc@3  69.90 ( 69.94)\n",
      "[2022-07-07 18:03:51,674 INFO] Epoch: [7][540/566]\tLoss 3.786 (3.793)\tInvT  19.78 ( 19.69)\tAcc@1  49.48 ( 49.80)\tAcc@3  69.90 ( 69.94)\n",
      "[2022-07-07 18:04:21,242 INFO] Epoch: [7][560/566]\tLoss 3.801 (3.792)\tInvT  19.79 ( 19.70)\tAcc@1  51.04 ( 49.82)\tAcc@3  69.58 ( 69.95)\n",
      "[2022-07-07 18:04:28,666 INFO] Learning rate: 1.0827481495539952e-05\n",
      "[2022-07-07 18:04:54,934 INFO] Epoch 7, valid metric: {\"Acc@1\": 29.655, \"Acc@3\": 43.941, \"loss\": 3.124}\n",
      "[2022-07-07 18:04:57,753 INFO] Delete old checkpoint ./checkpoint/fb15k237/checkpoint_epoch2.mdl\n",
      "[2022-07-07 18:05:00,664 INFO] Epoch: [8][  0/566]\tLoss 3.498 (3.498)\tInvT  19.79 ( 19.79)\tAcc@1  51.25 ( 51.25)\tAcc@3  72.92 ( 72.92)\n",
      "[2022-07-07 18:05:30,322 INFO] Epoch: [8][ 20/566]\tLoss 3.66 (3.612)\tInvT  19.80 ( 19.79)\tAcc@1  51.25 ( 52.00)\tAcc@3  70.00 ( 72.12)\n",
      "[2022-07-07 18:06:00,023 INFO] Epoch: [8][ 40/566]\tLoss 3.755 (3.602)\tInvT  19.80 ( 19.80)\tAcc@1  49.69 ( 52.12)\tAcc@3  69.79 ( 72.20)\n",
      "[2022-07-07 18:06:29,701 INFO] Epoch: [8][ 60/566]\tLoss 3.793 (3.595)\tInvT  19.81 ( 19.80)\tAcc@1  50.42 ( 52.22)\tAcc@3  70.73 ( 72.28)\n",
      "[2022-07-07 18:06:59,377 INFO] Epoch: [8][ 80/566]\tLoss 3.669 (3.597)\tInvT  19.81 ( 19.80)\tAcc@1  52.19 ( 52.30)\tAcc@3  71.04 ( 72.27)\n",
      "[2022-07-07 18:07:29,179 INFO] Epoch: [8][100/566]\tLoss 3.617 (3.602)\tInvT  19.82 ( 19.81)\tAcc@1  51.67 ( 52.20)\tAcc@3  69.79 ( 72.10)\n",
      "[2022-07-07 18:07:58,856 INFO] Epoch: [8][120/566]\tLoss 3.483 (3.603)\tInvT  19.83 ( 19.81)\tAcc@1  53.96 ( 52.21)\tAcc@3  73.54 ( 72.07)\n",
      "[2022-07-07 18:08:28,480 INFO] Epoch: [8][140/566]\tLoss 3.695 (3.61)\tInvT  19.83 ( 19.81)\tAcc@1  50.31 ( 52.20)\tAcc@3  70.62 ( 72.01)\n",
      "[2022-07-07 18:08:58,093 INFO] Epoch: [8][160/566]\tLoss 3.618 (3.61)\tInvT  19.84 ( 19.81)\tAcc@1  53.85 ( 52.24)\tAcc@3  73.02 ( 72.02)\n",
      "[2022-07-07 18:09:27,737 INFO] Epoch: [8][180/566]\tLoss 3.654 (3.617)\tInvT  19.84 ( 19.82)\tAcc@1  52.19 ( 52.16)\tAcc@3  72.71 ( 71.96)\n",
      "[2022-07-07 18:09:56,902 INFO] Epoch: [8][200/566]\tLoss 3.663 (3.617)\tInvT  19.85 ( 19.82)\tAcc@1  50.21 ( 52.15)\tAcc@3  70.21 ( 71.95)\n",
      "[2022-07-07 18:10:26,587 INFO] Epoch: [8][220/566]\tLoss 3.697 (3.616)\tInvT  19.85 ( 19.82)\tAcc@1  51.04 ( 52.14)\tAcc@3  71.67 ( 71.96)\n",
      "[2022-07-07 18:10:56,365 INFO] Epoch: [8][240/566]\tLoss 3.536 (3.616)\tInvT  19.85 ( 19.82)\tAcc@1  54.69 ( 52.14)\tAcc@3  72.81 ( 71.92)\n",
      "[2022-07-07 18:11:26,123 INFO] Epoch: [8][260/566]\tLoss 3.606 (3.616)\tInvT  19.86 ( 19.83)\tAcc@1  53.12 ( 52.16)\tAcc@3  73.02 ( 71.94)\n",
      "[2022-07-07 18:11:55,764 INFO] Epoch: [8][280/566]\tLoss 3.601 (3.615)\tInvT  19.86 ( 19.83)\tAcc@1  53.23 ( 52.16)\tAcc@3  72.40 ( 71.95)\n",
      "[2022-07-07 18:12:25,471 INFO] Epoch: [8][300/566]\tLoss 3.656 (3.614)\tInvT  19.87 ( 19.83)\tAcc@1  53.75 ( 52.17)\tAcc@3  71.67 ( 71.98)\n",
      "[2022-07-07 18:12:55,134 INFO] Epoch: [8][320/566]\tLoss 3.669 (3.616)\tInvT  19.87 ( 19.83)\tAcc@1  51.77 ( 52.18)\tAcc@3  70.52 ( 71.96)\n",
      "[2022-07-07 18:13:24,786 INFO] Epoch: [8][340/566]\tLoss 3.425 (3.615)\tInvT  19.88 ( 19.84)\tAcc@1  51.77 ( 52.21)\tAcc@3  72.81 ( 71.98)\n",
      "[2022-07-07 18:13:54,543 INFO] Epoch: [8][360/566]\tLoss 3.502 (3.614)\tInvT  19.88 ( 19.84)\tAcc@1  53.12 ( 52.22)\tAcc@3  74.06 ( 72.00)\n",
      "[2022-07-07 18:14:24,432 INFO] Epoch: [8][380/566]\tLoss 3.6 (3.616)\tInvT  19.88 ( 19.84)\tAcc@1  52.50 ( 52.21)\tAcc@3  73.12 ( 71.99)\n",
      "[2022-07-07 18:14:54,216 INFO] Epoch: [8][400/566]\tLoss 3.808 (3.618)\tInvT  19.89 ( 19.84)\tAcc@1  50.83 ( 52.17)\tAcc@3  70.31 ( 71.97)\n",
      "[2022-07-07 18:15:24,045 INFO] Epoch: [8][420/566]\tLoss 3.731 (3.617)\tInvT  19.89 ( 19.85)\tAcc@1  52.40 ( 52.18)\tAcc@3  70.42 ( 71.98)\n",
      "[2022-07-07 18:15:53,664 INFO] Epoch: [8][440/566]\tLoss 3.436 (3.615)\tInvT  19.89 ( 19.85)\tAcc@1  53.65 ( 52.19)\tAcc@3  73.02 ( 72.00)\n",
      "[2022-07-07 18:16:23,338 INFO] Epoch: [8][460/566]\tLoss 3.553 (3.615)\tInvT  19.90 ( 19.85)\tAcc@1  50.52 ( 52.19)\tAcc@3  72.50 ( 71.99)\n",
      "[2022-07-07 18:16:52,960 INFO] Epoch: [8][480/566]\tLoss 3.481 (3.614)\tInvT  19.90 ( 19.85)\tAcc@1  53.33 ( 52.22)\tAcc@3  72.19 ( 72.00)\n",
      "[2022-07-07 18:17:22,734 INFO] Epoch: [8][500/566]\tLoss 3.512 (3.612)\tInvT  19.90 ( 19.85)\tAcc@1  53.23 ( 52.24)\tAcc@3  73.54 ( 72.02)\n",
      "[2022-07-07 18:17:52,367 INFO] Epoch: [8][520/566]\tLoss 3.577 (3.612)\tInvT  19.91 ( 19.86)\tAcc@1  53.44 ( 52.24)\tAcc@3  72.50 ( 72.03)\n",
      "[2022-07-07 18:18:22,178 INFO] Epoch: [8][540/566]\tLoss 3.984 (3.612)\tInvT  19.91 ( 19.86)\tAcc@1  48.85 ( 52.25)\tAcc@3  69.58 ( 72.03)\n",
      "[2022-07-07 18:18:52,108 INFO] Epoch: [8][560/566]\tLoss 3.372 (3.613)\tInvT  19.91 ( 19.86)\tAcc@1  55.21 ( 52.27)\tAcc@3  73.75 ( 72.01)\n",
      "[2022-07-07 18:18:59,591 INFO] Learning rate: 5.45644334788385e-06\n",
      "[2022-07-07 18:19:25,784 INFO] Epoch 8, valid metric: {\"Acc@1\": 29.718, \"Acc@3\": 44.023, \"loss\": 3.122}\n",
      "[2022-07-07 18:19:28,571 INFO] Delete old checkpoint ./checkpoint/fb15k237/checkpoint_epoch3.mdl\n",
      "[2022-07-07 18:19:31,554 INFO] Epoch: [9][  0/566]\tLoss 3.589 (3.589)\tInvT  19.91 ( 19.91)\tAcc@1  53.02 ( 53.02)\tAcc@3  70.52 ( 70.52)\n",
      "[2022-07-07 18:20:01,104 INFO] Epoch: [9][ 20/566]\tLoss 3.519 (3.498)\tInvT  19.91 ( 19.91)\tAcc@1  54.79 ( 54.09)\tAcc@3  72.81 ( 72.99)\n",
      "[2022-07-07 18:20:30,543 INFO] Epoch: [9][ 40/566]\tLoss 3.599 (3.499)\tInvT  19.92 ( 19.91)\tAcc@1  52.60 ( 53.83)\tAcc@3  71.67 ( 73.29)\n",
      "[2022-07-07 18:21:00,171 INFO] Epoch: [9][ 60/566]\tLoss 3.404 (3.482)\tInvT  19.92 ( 19.92)\tAcc@1  53.96 ( 53.96)\tAcc@3  73.96 ( 73.42)\n",
      "[2022-07-07 18:21:29,840 INFO] Epoch: [9][ 80/566]\tLoss 3.618 (3.483)\tInvT  19.92 ( 19.92)\tAcc@1  50.94 ( 53.91)\tAcc@3  72.29 ( 73.52)\n",
      "[2022-07-07 18:21:59,180 INFO] Epoch: [9][100/566]\tLoss 3.338 (3.482)\tInvT  19.92 ( 19.92)\tAcc@1  55.94 ( 54.00)\tAcc@3  75.10 ( 73.46)\n",
      "[2022-07-07 18:22:28,825 INFO] Epoch: [9][120/566]\tLoss 3.613 (3.483)\tInvT  19.93 ( 19.92)\tAcc@1  51.77 ( 54.00)\tAcc@3  71.56 ( 73.41)\n",
      "[2022-07-07 18:22:58,473 INFO] Epoch: [9][140/566]\tLoss 3.326 (3.48)\tInvT  19.93 ( 19.92)\tAcc@1  57.92 ( 54.03)\tAcc@3  74.69 ( 73.47)\n",
      "[2022-07-07 18:23:28,060 INFO] Epoch: [9][160/566]\tLoss 3.694 (3.478)\tInvT  19.93 ( 19.92)\tAcc@1  51.35 ( 54.09)\tAcc@3  71.46 ( 73.50)\n",
      "[2022-07-07 18:23:57,896 INFO] Epoch: [9][180/566]\tLoss 3.652 (3.482)\tInvT  19.93 ( 19.92)\tAcc@1  53.85 ( 54.08)\tAcc@3  71.88 ( 73.49)\n",
      "[2022-07-07 18:24:27,640 INFO] Epoch: [9][200/566]\tLoss 3.406 (3.48)\tInvT  19.94 ( 19.92)\tAcc@1  55.10 ( 54.06)\tAcc@3  74.48 ( 73.50)\n",
      "[2022-07-07 18:24:57,254 INFO] Epoch: [9][220/566]\tLoss 3.385 (3.481)\tInvT  19.94 ( 19.93)\tAcc@1  55.10 ( 54.07)\tAcc@3  74.90 ( 73.50)\n",
      "[2022-07-07 18:25:26,926 INFO] Epoch: [9][240/566]\tLoss 3.501 (3.482)\tInvT  19.94 ( 19.93)\tAcc@1  53.44 ( 54.10)\tAcc@3  72.81 ( 73.50)\n",
      "[2022-07-07 18:25:56,843 INFO] Epoch: [9][260/566]\tLoss 3.583 (3.484)\tInvT  19.94 ( 19.93)\tAcc@1  52.60 ( 54.08)\tAcc@3  71.98 ( 73.48)\n",
      "[2022-07-07 18:26:26,662 INFO] Epoch: [9][280/566]\tLoss 3.373 (3.482)\tInvT  19.94 ( 19.93)\tAcc@1  55.52 ( 54.09)\tAcc@3  75.10 ( 73.48)\n",
      "[2022-07-07 18:26:56,268 INFO] Epoch: [9][300/566]\tLoss 3.636 (3.482)\tInvT  19.94 ( 19.93)\tAcc@1  51.67 ( 54.07)\tAcc@3  72.08 ( 73.49)\n",
      "[2022-07-07 18:27:25,782 INFO] Epoch: [9][320/566]\tLoss 3.356 (3.483)\tInvT  19.94 ( 19.93)\tAcc@1  54.90 ( 54.06)\tAcc@3  74.58 ( 73.46)\n",
      "[2022-07-07 18:27:55,525 INFO] Epoch: [9][340/566]\tLoss 3.447 (3.485)\tInvT  19.95 ( 19.93)\tAcc@1  55.52 ( 54.05)\tAcc@3  74.06 ( 73.45)\n",
      "[2022-07-07 18:28:25,249 INFO] Epoch: [9][360/566]\tLoss 3.378 (3.484)\tInvT  19.95 ( 19.93)\tAcc@1  54.38 ( 54.07)\tAcc@3  74.06 ( 73.47)\n",
      "[2022-07-07 18:28:54,917 INFO] Epoch: [9][380/566]\tLoss 3.316 (3.485)\tInvT  19.95 ( 19.93)\tAcc@1  55.62 ( 54.05)\tAcc@3  75.42 ( 73.46)\n",
      "[2022-07-07 18:29:24,633 INFO] Epoch: [9][400/566]\tLoss 3.506 (3.486)\tInvT  19.95 ( 19.93)\tAcc@1  53.33 ( 54.05)\tAcc@3  74.48 ( 73.46)\n",
      "[2022-07-07 18:29:54,280 INFO] Epoch: [9][420/566]\tLoss 3.606 (3.486)\tInvT  19.95 ( 19.93)\tAcc@1  53.96 ( 54.05)\tAcc@3  71.46 ( 73.47)\n",
      "[2022-07-07 18:30:24,121 INFO] Epoch: [9][440/566]\tLoss 3.442 (3.487)\tInvT  19.95 ( 19.93)\tAcc@1  55.31 ( 54.06)\tAcc@3  74.27 ( 73.45)\n",
      "[2022-07-07 18:30:53,787 INFO] Epoch: [9][460/566]\tLoss 3.565 (3.484)\tInvT  19.95 ( 19.94)\tAcc@1  51.77 ( 54.06)\tAcc@3  73.02 ( 73.47)\n",
      "[2022-07-07 18:31:23,571 INFO] Epoch: [9][480/566]\tLoss 3.487 (3.482)\tInvT  19.95 ( 19.94)\tAcc@1  52.81 ( 54.09)\tAcc@3  73.12 ( 73.49)\n",
      "[2022-07-07 18:31:53,334 INFO] Epoch: [9][500/566]\tLoss 3.631 (3.483)\tInvT  19.95 ( 19.94)\tAcc@1  51.77 ( 54.09)\tAcc@3  71.35 ( 73.49)\n",
      "[2022-07-07 18:32:22,744 INFO] Epoch: [9][520/566]\tLoss 3.461 (3.485)\tInvT  19.95 ( 19.94)\tAcc@1  54.17 ( 54.07)\tAcc@3  74.27 ( 73.47)\n",
      "[2022-07-07 18:32:52,381 INFO] Epoch: [9][540/566]\tLoss 3.654 (3.483)\tInvT  19.95 ( 19.94)\tAcc@1  52.92 ( 54.08)\tAcc@3  71.04 ( 73.47)\n",
      "[2022-07-07 18:33:22,181 INFO] Epoch: [9][560/566]\tLoss 3.301 (3.481)\tInvT  19.95 ( 19.94)\tAcc@1  56.25 ( 54.10)\tAcc@3  74.69 ( 73.50)\n",
      "[2022-07-07 18:33:29,891 INFO] Learning rate: 8.540520022774721e-08\n",
      "[2022-07-07 18:33:55,772 INFO] Epoch 9, valid metric: {\"Acc@1\": 29.989, \"Acc@3\": 44.212, \"loss\": 3.117}\n",
      "[2022-07-07 18:33:58,611 INFO] Delete old checkpoint ./checkpoint/fb15k237/checkpoint_epoch4.mdl\n"
     ]
    }
   ],
   "source": [
    "!OUTPUT_DIR=./checkpoint/fb15k237/ bash scripts/train_fb.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5208b4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ set -e\n",
      "+ model_path=bert\n",
      "+ task=WN18RR\n",
      "+ [[ 2 -ge 1 ]]\n",
      "+ [[ ! checkpoint/fb15k237/model_best.mdl == \\-\\-* ]]\n",
      "+ model_path=checkpoint/fb15k237/model_best.mdl\n",
      "+ shift\n",
      "+ [[ 1 -ge 1 ]]\n",
      "+ [[ ! FB15k237 == \\-\\-* ]]\n",
      "+ task=FB15k237\n",
      "+ shift\n",
      "+++ dirname scripts/eval.sh\n",
      "++ cd scripts\n",
      "++ cd ..\n",
      "++ pwd\n",
      "+ DIR=/workspace/SimKGC\n",
      "+ echo 'working directory: /workspace/SimKGC'\n",
      "working directory: /workspace/SimKGC\n",
      "+ '[' -z '' ']'\n",
      "+ DATA_DIR=/workspace/SimKGC/data/FB15k237\n",
      "+ test_path=/workspace/SimKGC/data/FB15k237/test.txt.json\n",
      "+ [[ 0 -ge 1 ]]\n",
      "+ neighbor_weight=0.05\n",
      "+ rerank_n_hop=2\n",
      "+ '[' FB15k237 = WN18RR ']'\n",
      "+ '[' FB15k237 = wiki5m_ind ']'\n",
      "+ python3 -u evaluate.py --task FB15k237 --is-test --eval-model-path checkpoint/fb15k237/model_best.mdl --neighbor-weight 0.05 --rerank-n-hop 2 --train-path /workspace/SimKGC/data/FB15k237/train.txt.json --valid-path /workspace/SimKGC/data/FB15k237/test.txt.json\n",
      "[2022-07-07 18:38:21,934 INFO] Load 14541 entities from /workspace/SimKGC/data/FB15k237/entities.json\n",
      "[2022-07-07 18:38:21,972 INFO] Triplets path: ['/workspace/SimKGC/data/FB15k237/train.txt.json', '/workspace/SimKGC/data/FB15k237/valid.txt.json', '/workspace/SimKGC/data/FB15k237/test.txt.json']\n",
      "[2022-07-07 18:38:23,806 INFO] Triplet statistics: 474 relations, 40932 triplets\n",
      "[2022-07-07 18:38:24,082 INFO] Args used in training: {\n",
      "    \"pretrained_model\": \"bert-base-uncased\",\n",
      "    \"task\": \"FB15k237\",\n",
      "    \"train_path\": \"/workspace/SimKGC/data/FB15k237/train.txt.json\",\n",
      "    \"valid_path\": \"/workspace/SimKGC/data/FB15k237/valid.txt.json\",\n",
      "    \"model_dir\": \"./checkpoint/fb15k237/\",\n",
      "    \"warmup\": 400,\n",
      "    \"max_to_keep\": 5,\n",
      "    \"grad_clip\": 10.0,\n",
      "    \"pooling\": \"mean\",\n",
      "    \"dropout\": 0.1,\n",
      "    \"use_amp\": true,\n",
      "    \"t\": 0.05,\n",
      "    \"use_link_graph\": true,\n",
      "    \"eval_every_n_step\": 10000,\n",
      "    \"pre_batch\": 2,\n",
      "    \"pre_batch_weight\": 0.5,\n",
      "    \"additive_margin\": 0.02,\n",
      "    \"finetune_t\": true,\n",
      "    \"max_num_tokens\": 50,\n",
      "    \"use_self_negative\": true,\n",
      "    \"workers\": 4,\n",
      "    \"epochs\": 10,\n",
      "    \"batch_size\": 960,\n",
      "    \"lr\": 5e-05,\n",
      "    \"lr_scheduler\": \"linear\",\n",
      "    \"weight_decay\": 0.0001,\n",
      "    \"print_freq\": 20,\n",
      "    \"seed\": null,\n",
      "    \"is_test\": false,\n",
      "    \"rerank_n_hop\": 2,\n",
      "    \"neighbor_weight\": 0.0,\n",
      "    \"eval_model_path\": \"\"\n",
      "}\n",
      "[2022-07-07 18:38:27,435 INFO] Build tokenizer from bert-base-uncased\n",
      "[2022-07-07 18:38:30,349 INFO] Load model from checkpoint/fb15k237/model_best.mdl successfully\n",
      "  0%|                                                    | 0/15 [00:00<?, ?it/s][2022-07-07 18:38:30,431 INFO] Start to build link graph from /workspace/SimKGC/data/FB15k237/train.txt.json\n",
      "[2022-07-07 18:38:30,448 INFO] Start to build link graph from /workspace/SimKGC/data/FB15k237/train.txt.json\n",
      "[2022-07-07 18:38:31,282 INFO] Done build link graph with 14505 nodes\n",
      "[2022-07-07 18:38:31,305 INFO] Done build link graph with 14505 nodes\n",
      "100%|███████████████████████████████████████████| 15/15 [00:09<00:00,  1.60it/s]\n",
      "[2022-07-07 18:38:39,762 INFO] In test mode: True\n",
      "[2022-07-07 18:38:39,800 INFO] Load 20466 examples from /workspace/SimKGC/data/FB15k237/test.txt.json\n",
      "[2022-07-07 18:38:39,910 INFO] Start to build link graph from /workspace/SimKGC/data/FB15k237/train.txt.json\n",
      "[2022-07-07 18:38:40,881 INFO] Done build link graph with 14505 nodes\n",
      "[2022-07-07 18:39:13,551 INFO] predict tensor done, compute metrics...\n",
      "  0%|                                                    | 0/80 [00:00<?, ?it/s][2022-07-07 18:39:14,265 INFO] Start to build link graph from /workspace/SimKGC/data/FB15k237/train.txt.json\n",
      "[2022-07-07 18:39:15,020 INFO] Done build link graph with 14505 nodes\n",
      "100%|███████████████████████████████████████████| 80/80 [02:11<00:00,  1.65s/it]\n",
      "[2022-07-07 18:41:26,175 INFO] forward metrics: {\"mean_rank\": 114.0947, \"mrr\": 0.3872, \"hit@1\": 0.2991, \"hit@3\": 0.4143, \"hit@10\": 0.571}\n",
      "[2022-07-07 18:41:27,266 INFO] Evaluation takes 167.504 seconds\n",
      "[2022-07-07 18:41:27,284 INFO] In test mode: True\n",
      "[2022-07-07 18:41:27,322 INFO] Load 20466 examples from /workspace/SimKGC/data/FB15k237/test.txt.json\n",
      "[2022-07-07 18:42:00,077 INFO] predict tensor done, compute metrics...\n",
      "100%|███████████████████████████████████████████| 80/80 [03:20<00:00,  2.51s/it]\n",
      "[2022-07-07 18:45:21,776 INFO] backward metrics: {\"mean_rank\": 222.1025, \"mrr\": 0.1986, \"hit@1\": 0.1167, \"hit@3\": 0.2178, \"hit@10\": 0.362}\n",
      "[2022-07-07 18:45:22,888 INFO] Evaluation takes 235.604 seconds\n",
      "[2022-07-07 18:45:22,906 INFO] Averaged metrics: {'mean_rank': 168.0986, 'mrr': 0.2929, 'hit@1': 0.2079, 'hit@3': 0.316, 'hit@10': 0.4665}\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/eval.sh checkpoint/fb15k237/model_best.mdl FB15k237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92418fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
