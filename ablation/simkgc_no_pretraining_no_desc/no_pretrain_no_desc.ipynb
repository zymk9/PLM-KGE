{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f18bb1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul  8 12:41:48 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 510.54       Driver Version: 510.54       CUDA Version: 11.6     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:19:00.0 Off |                  N/A |\r\n",
      "|  0%   37C    P8    22W / 370W |    257MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:1A:00.0 Off |                  N/A |\r\n",
      "|  0%   36C    P8    13W / 420W |    257MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  NVIDIA GeForce ...  On   | 00000000:65:00.0 Off |                  N/A |\r\n",
      "|  0%   34C    P8    26W / 350W |    257MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  NVIDIA GeForce ...  On   | 00000000:B3:00.0 Off |                  N/A |\r\n",
      "|  0%   38C    P8    17W / 420W |    257MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1623      C                                     255MiB |\r\n",
      "|    1   N/A  N/A     31542      C                                     255MiB |\r\n",
      "|    2   N/A  N/A     26554      C                                     255MiB |\r\n",
      "|    3   N/A  N/A      7829      C                                     255MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "919b1a53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 37.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 13.3 MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.12.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.6.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
      "\u001b[K     |████████████████████████████████| 749 kB 30.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.8 MB 22.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 33.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=1.1.0\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 38.1 MB 25.0 MB/s eta 0:00:01     |████████▉                       | 10.5 MB 25.0 MB/s eta 0:00:02\n",
      "\u001b[?25hBuilding wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=0cd0d7ac5e697b35b05adf91acb2b6052da637e218491eeb6bcc7adc403f9744\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, tokenizers, scikit-learn, regex, huggingface-hub, transformers, sklearn\n",
      "Successfully installed huggingface-hub-0.8.1 joblib-1.1.0 regex-2022.6.2 scikit-learn-1.0.2 scipy-1.7.3 sklearn-0.0 threadpoolctl-3.1.0 tokenizers-0.12.1 transformers-4.20.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Cloning into 'SimKGC'...\n",
      "remote: Enumerating objects: 156, done.\u001b[K\n",
      "remote: Counting objects: 100% (156/156), done.\u001b[K\n",
      "remote: Compressing objects: 100% (101/101), done.\u001b[K\n",
      "remote: Total 156 (delta 81), reused 129 (delta 54), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (156/156), 15.67 MiB | 14.12 MiB/s, done.\n",
      "Resolving deltas: 100% (81/81), done.\n",
      "Cloning into 'CAKE'...\n",
      "remote: Enumerating objects: 110, done.\u001b[K\n",
      "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
      "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
      "remote: Total 110 (delta 0), reused 0 (delta 0), pack-reused 107\u001b[K\n",
      "Receiving objects: 100% (110/110), 51.26 MiB | 14.08 MiB/s, done.\n",
      "Resolving deltas: 100% (23/23), done.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sklearn\n",
    "# !pip install -U adapter-transformers\n",
    "!git clone https://github.com/intfloat/SimKGC.git\n",
    "!git clone https://github.com/ngl567/CAKE.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccb4faa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/SimKGC\n",
      "+ set -e\n",
      "+ TASK=WN18RR\n",
      "+ [[ 1 -ge 1 ]]\n",
      "+ TASK=FB15k237\n",
      "+ shift\n",
      "+ python3 -u preprocess.py --task FB15k237 --train-path ./data/FB15k237/train.txt --valid-path ./data/FB15k237/valid.txt --test-path ./data/FB15k237/test.txt\n",
      "Process ./data/FB15k237/train.txt...\n",
      "Load 14904 entity descriptions from ./data/FB15k237/FB15k_mid2description.txt\n",
      "No desc found for /m/02vxfw_\n",
      "No desc found for /m/02jxk\n",
      "No desc found for /m/03m3nzf\n",
      "No desc found for /m/04_1l0v\n",
      "No desc found for /m/09x_r\n",
      "No desc found for /m/0bytsc\n",
      "No desc found for /m/07_bv_\n",
      "No desc found for /m/03lsz8h\n",
      "No desc found for /m/05xf75\n",
      "No desc found for /m/01dy7j\n",
      "No desc found for /m/09ly2r6\n",
      "No desc found for /m/015zql\n",
      "No desc found for /m/047vp20\n",
      "No desc found for /m/0hk18\n",
      "No desc found for /m/061zc_\n",
      "No desc found for /m/0cfywh\n",
      "No desc found for /m/03tp4\n",
      "No desc found for /m/029cpw\n",
      "No desc found for /m/0lmb5\n",
      "No desc found for /m/0m6x4\n",
      "No desc found for /m/09l65\n",
      "No desc found for /m/0147fv\n",
      "No desc found for /m/0kvrb\n",
      "No desc found for /m/03gwg4w\n",
      "No desc found for /m/0bm39zf\n",
      "No desc found for /m/08mbj32\n",
      "No desc found for /m/068bs\n",
      "No desc found for /m/01xzb6\n",
      "No desc found for /m/07djnx\n",
      "No desc found for /m/02q_plc\n",
      "No desc found for /m/02cjrp\n",
      "No desc found for /m/01xsbh\n",
      "No desc found for /m/03bx017\n",
      "No desc found for /m/01sy5c\n",
      "No desc found for /m/0854hr\n",
      "No desc found for /m/0h005\n",
      "No desc found for /m/01fkv0\n",
      "No desc found for /m/05h4fjx\n",
      "No desc found for /m/05ry0p\n",
      "No desc found for /m/01dvms\n",
      "No desc found for /m/04686_j\n",
      "No desc found for /m/08chdb\n",
      "No desc found for /m/05zvq6g\n",
      "No desc found for /m/0288crq\n",
      "No desc found for /m/01my929\n",
      "No desc found for /m/07t_l23\n",
      "No desc found for /m/07s4911\n",
      "Load 14951 entity names from ./data/FB15k237/FB15k_mid2name.txt\n",
      "Save 237 relations to ./data/FB15k237/relations.json\n",
      "Save 272115 examples to ./data/FB15k237/train.txt.json\n",
      "Process ./data/FB15k237/valid.txt...\n",
      "Save 17535 examples to ./data/FB15k237/valid.txt.json\n",
      "Process ./data/FB15k237/test.txt...\n",
      "Save 20466 examples to ./data/FB15k237/test.txt.json\n",
      "Get 14541 entities, 237 relations in total\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "%cd SimKGC\n",
    "!bash scripts/preprocess.sh FB15k237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "480d4d26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ set -e\n",
      "+ TASK=FB15k237\n",
      "+++ dirname scripts/train_fb.sh\n",
      "++ cd scripts\n",
      "++ cd ..\n",
      "++ pwd\n",
      "+ DIR=/workspace/SimKGC\n",
      "+ echo 'working directory: /workspace/SimKGC'\n",
      "working directory: /workspace/SimKGC\n",
      "+ '[' -z ./checkpoint/fb15k237/ ']'\n",
      "+ '[' -z '' ']'\n",
      "+ DATA_DIR=/workspace/SimKGC/data/FB15k237\n",
      "+ python3 -u main.py --model-dir ./checkpoint/fb15k237/ --pretrained-model bert-base-uncased --pooling mean --lr 5e-5 --use-link-graph --train-path /workspace/SimKGC/data/FB15k237/train.txt.json --valid-path /workspace/SimKGC/data/FB15k237/valid.txt.json --task FB15k237 --batch-size 1024 --print-freq 20 --additive-margin 0.02 --use-amp --use-self-negative --finetune-t --pre-batch 2 --epochs 10 --workers 4 --max-to-keep 5 --no-desc --no-pretraining\n",
      "[2022-07-08 12:45:40,411 INFO] Load 14541 entities from /workspace/SimKGC/data/FB15k237/entities.json\n",
      "[2022-07-08 12:45:40,411 INFO] Triplets path: ['/workspace/SimKGC/data/FB15k237/train.txt.json']\n",
      "[2022-07-08 12:45:42,732 INFO] Triplet statistics: 474 relations, 544230 triplets\n",
      "[2022-07-08 12:45:42,732 INFO] Start to build link graph from /workspace/SimKGC/data/FB15k237/train.txt.json\n",
      "[2022-07-08 12:45:43,795 INFO] Done build link graph with 14505 nodes\n",
      "[2022-07-08 12:45:43,915 INFO] Use 4 gpus for training\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 15.7kB/s]\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 321kB/s]\n",
      "Downloading: 100%|████████████████████████████| 226k/226k [00:00<00:00, 297kB/s]\n",
      "Downloading: 100%|████████████████████████████| 455k/455k [00:00<00:00, 600kB/s]\n",
      "[2022-07-08 12:45:56,221 INFO] Build tokenizer from bert-base-uncased\n",
      "[2022-07-08 12:45:56,221 INFO] => creating model\n",
      "[2022-07-08 12:45:58,926 INFO] CustomBertModel(\n",
      "  (hr_bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (tail_bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\r\n",
      "  FutureWarning,\r\n",
      "[2022-07-08 12:46:00,176 INFO] module.log_inv_t: 1.0\r\n",
      "[2022-07-08 12:46:00,177 INFO] module.hr_bert.embeddings.word_embeddings.weight: 23440896\r\n",
      "[2022-07-08 12:46:00,177 INFO] module.hr_bert.embeddings.position_embeddings.weight: 393216\r\n",
      "[2022-07-08 12:46:00,177 INFO] module.hr_bert.embeddings.token_type_embeddings.weight: 1536\r\n",
      "[2022-07-08 12:46:00,177 INFO] module.hr_bert.embeddings.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,177 INFO] module.hr_bert.embeddings.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,177 INFO] module.hr_bert.encoder.layer.0.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,177 INFO] module.hr_bert.encoder.layer.0.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,177 INFO] module.hr_bert.encoder.layer.0.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,177 INFO] module.hr_bert.encoder.layer.0.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,177 INFO] module.hr_bert.encoder.layer.0.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,177 INFO] module.hr_bert.encoder.layer.0.attention.self.value.bias: 768\r\n",
      "[2022-07-08 12:46:00,177 INFO] module.hr_bert.encoder.layer.0.attention.output.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,177 INFO] module.hr_bert.encoder.layer.0.attention.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,178 INFO] module.hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,178 INFO] module.hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,178 INFO] module.hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,178 INFO] module.hr_bert.encoder.layer.0.intermediate.dense.bias: 3072\r\n",
      "[2022-07-08 12:46:00,178 INFO] module.hr_bert.encoder.layer.0.output.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,178 INFO] module.hr_bert.encoder.layer.0.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,178 INFO] module.hr_bert.encoder.layer.0.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,178 INFO] module.hr_bert.encoder.layer.0.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,178 INFO] module.hr_bert.encoder.layer.1.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,178 INFO] module.hr_bert.encoder.layer.1.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,178 INFO] module.hr_bert.encoder.layer.1.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,178 INFO] module.hr_bert.encoder.layer.1.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,178 INFO] module.hr_bert.encoder.layer.1.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,178 INFO] module.hr_bert.encoder.layer.1.attention.self.value.bias: 768\r\n",
      "[2022-07-08 12:46:00,178 INFO] module.hr_bert.encoder.layer.1.attention.output.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,179 INFO] module.hr_bert.encoder.layer.1.attention.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,179 INFO] module.hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,179 INFO] module.hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,179 INFO] module.hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,179 INFO] module.hr_bert.encoder.layer.1.intermediate.dense.bias: 3072\r\n",
      "[2022-07-08 12:46:00,179 INFO] module.hr_bert.encoder.layer.1.output.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,179 INFO] module.hr_bert.encoder.layer.1.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,179 INFO] module.hr_bert.encoder.layer.1.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,179 INFO] module.hr_bert.encoder.layer.1.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,179 INFO] module.hr_bert.encoder.layer.2.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,179 INFO] module.hr_bert.encoder.layer.2.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,179 INFO] module.hr_bert.encoder.layer.2.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,179 INFO] module.hr_bert.encoder.layer.2.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,179 INFO] module.hr_bert.encoder.layer.2.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,179 INFO] module.hr_bert.encoder.layer.2.attention.self.value.bias: 768\r\n",
      "[2022-07-08 12:46:00,180 INFO] module.hr_bert.encoder.layer.2.attention.output.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,180 INFO] module.hr_bert.encoder.layer.2.attention.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,180 INFO] module.hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,180 INFO] module.hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,180 INFO] module.hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,180 INFO] module.hr_bert.encoder.layer.2.intermediate.dense.bias: 3072\r\n",
      "[2022-07-08 12:46:00,180 INFO] module.hr_bert.encoder.layer.2.output.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,180 INFO] module.hr_bert.encoder.layer.2.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,180 INFO] module.hr_bert.encoder.layer.2.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,180 INFO] module.hr_bert.encoder.layer.2.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,180 INFO] module.hr_bert.encoder.layer.3.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,180 INFO] module.hr_bert.encoder.layer.3.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,180 INFO] module.hr_bert.encoder.layer.3.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,180 INFO] module.hr_bert.encoder.layer.3.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,180 INFO] module.hr_bert.encoder.layer.3.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,181 INFO] module.hr_bert.encoder.layer.3.attention.self.value.bias: 768\r\n",
      "[2022-07-08 12:46:00,181 INFO] module.hr_bert.encoder.layer.3.attention.output.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,181 INFO] module.hr_bert.encoder.layer.3.attention.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,181 INFO] module.hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,181 INFO] module.hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,181 INFO] module.hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,181 INFO] module.hr_bert.encoder.layer.3.intermediate.dense.bias: 3072\r\n",
      "[2022-07-08 12:46:00,181 INFO] module.hr_bert.encoder.layer.3.output.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,181 INFO] module.hr_bert.encoder.layer.3.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,181 INFO] module.hr_bert.encoder.layer.3.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,181 INFO] module.hr_bert.encoder.layer.3.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,181 INFO] module.hr_bert.encoder.layer.4.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,181 INFO] module.hr_bert.encoder.layer.4.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,181 INFO] module.hr_bert.encoder.layer.4.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,182 INFO] module.hr_bert.encoder.layer.4.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,182 INFO] module.hr_bert.encoder.layer.4.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,182 INFO] module.hr_bert.encoder.layer.4.attention.self.value.bias: 768\r\n",
      "[2022-07-08 12:46:00,182 INFO] module.hr_bert.encoder.layer.4.attention.output.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,182 INFO] module.hr_bert.encoder.layer.4.attention.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,182 INFO] module.hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,182 INFO] module.hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,182 INFO] module.hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,182 INFO] module.hr_bert.encoder.layer.4.intermediate.dense.bias: 3072\r\n",
      "[2022-07-08 12:46:00,182 INFO] module.hr_bert.encoder.layer.4.output.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,182 INFO] module.hr_bert.encoder.layer.4.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,182 INFO] module.hr_bert.encoder.layer.4.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,182 INFO] module.hr_bert.encoder.layer.4.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,182 INFO] module.hr_bert.encoder.layer.5.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,182 INFO] module.hr_bert.encoder.layer.5.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,183 INFO] module.hr_bert.encoder.layer.5.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,183 INFO] module.hr_bert.encoder.layer.5.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,183 INFO] module.hr_bert.encoder.layer.5.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,183 INFO] module.hr_bert.encoder.layer.5.attention.self.value.bias: 768\r\n",
      "[2022-07-08 12:46:00,183 INFO] module.hr_bert.encoder.layer.5.attention.output.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,183 INFO] module.hr_bert.encoder.layer.5.attention.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,183 INFO] module.hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,183 INFO] module.hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,183 INFO] module.hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,183 INFO] module.hr_bert.encoder.layer.5.intermediate.dense.bias: 3072\r\n",
      "[2022-07-08 12:46:00,183 INFO] module.hr_bert.encoder.layer.5.output.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,183 INFO] module.hr_bert.encoder.layer.5.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,183 INFO] module.hr_bert.encoder.layer.5.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,183 INFO] module.hr_bert.encoder.layer.5.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,183 INFO] module.hr_bert.encoder.layer.6.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,184 INFO] module.hr_bert.encoder.layer.6.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,184 INFO] module.hr_bert.encoder.layer.6.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,184 INFO] module.hr_bert.encoder.layer.6.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,184 INFO] module.hr_bert.encoder.layer.6.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,184 INFO] module.hr_bert.encoder.layer.6.attention.self.value.bias: 768\r\n",
      "[2022-07-08 12:46:00,184 INFO] module.hr_bert.encoder.layer.6.attention.output.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,184 INFO] module.hr_bert.encoder.layer.6.attention.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,184 INFO] module.hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,184 INFO] module.hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,184 INFO] module.hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,184 INFO] module.hr_bert.encoder.layer.6.intermediate.dense.bias: 3072\r\n",
      "[2022-07-08 12:46:00,184 INFO] module.hr_bert.encoder.layer.6.output.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,184 INFO] module.hr_bert.encoder.layer.6.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,184 INFO] module.hr_bert.encoder.layer.6.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,184 INFO] module.hr_bert.encoder.layer.6.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,185 INFO] module.hr_bert.encoder.layer.7.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,185 INFO] module.hr_bert.encoder.layer.7.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,185 INFO] module.hr_bert.encoder.layer.7.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,185 INFO] module.hr_bert.encoder.layer.7.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,185 INFO] module.hr_bert.encoder.layer.7.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,185 INFO] module.hr_bert.encoder.layer.7.attention.self.value.bias: 768\r\n",
      "[2022-07-08 12:46:00,185 INFO] module.hr_bert.encoder.layer.7.attention.output.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,185 INFO] module.hr_bert.encoder.layer.7.attention.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,185 INFO] module.hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,185 INFO] module.hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,185 INFO] module.hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,185 INFO] module.hr_bert.encoder.layer.7.intermediate.dense.bias: 3072\r\n",
      "[2022-07-08 12:46:00,185 INFO] module.hr_bert.encoder.layer.7.output.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,185 INFO] module.hr_bert.encoder.layer.7.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,186 INFO] module.hr_bert.encoder.layer.7.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,186 INFO] module.hr_bert.encoder.layer.7.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,186 INFO] module.hr_bert.encoder.layer.8.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,186 INFO] module.hr_bert.encoder.layer.8.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,186 INFO] module.hr_bert.encoder.layer.8.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,186 INFO] module.hr_bert.encoder.layer.8.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,186 INFO] module.hr_bert.encoder.layer.8.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,186 INFO] module.hr_bert.encoder.layer.8.attention.self.value.bias: 768\r\n",
      "[2022-07-08 12:46:00,186 INFO] module.hr_bert.encoder.layer.8.attention.output.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,186 INFO] module.hr_bert.encoder.layer.8.attention.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,186 INFO] module.hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,186 INFO] module.hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,186 INFO] module.hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,186 INFO] module.hr_bert.encoder.layer.8.intermediate.dense.bias: 3072\r\n",
      "[2022-07-08 12:46:00,186 INFO] module.hr_bert.encoder.layer.8.output.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,187 INFO] module.hr_bert.encoder.layer.8.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,187 INFO] module.hr_bert.encoder.layer.8.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,187 INFO] module.hr_bert.encoder.layer.8.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,187 INFO] module.hr_bert.encoder.layer.9.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,187 INFO] module.hr_bert.encoder.layer.9.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,187 INFO] module.hr_bert.encoder.layer.9.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,187 INFO] module.hr_bert.encoder.layer.9.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,187 INFO] module.hr_bert.encoder.layer.9.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,187 INFO] module.hr_bert.encoder.layer.9.attention.self.value.bias: 768\r\n",
      "[2022-07-08 12:46:00,187 INFO] module.hr_bert.encoder.layer.9.attention.output.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,187 INFO] module.hr_bert.encoder.layer.9.attention.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,187 INFO] module.hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,187 INFO] module.hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,187 INFO] module.hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,187 INFO] module.hr_bert.encoder.layer.9.intermediate.dense.bias: 3072\r\n",
      "[2022-07-08 12:46:00,188 INFO] module.hr_bert.encoder.layer.9.output.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,188 INFO] module.hr_bert.encoder.layer.9.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,188 INFO] module.hr_bert.encoder.layer.9.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,188 INFO] module.hr_bert.encoder.layer.9.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,188 INFO] module.hr_bert.encoder.layer.10.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,188 INFO] module.hr_bert.encoder.layer.10.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,188 INFO] module.hr_bert.encoder.layer.10.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,188 INFO] module.hr_bert.encoder.layer.10.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,188 INFO] module.hr_bert.encoder.layer.10.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,188 INFO] module.hr_bert.encoder.layer.10.attention.self.value.bias: 768\r\n",
      "[2022-07-08 12:46:00,188 INFO] module.hr_bert.encoder.layer.10.attention.output.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,188 INFO] module.hr_bert.encoder.layer.10.attention.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,188 INFO] module.hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,188 INFO] module.hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,188 INFO] module.hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,189 INFO] module.hr_bert.encoder.layer.10.intermediate.dense.bias: 3072\r\n",
      "[2022-07-08 12:46:00,189 INFO] module.hr_bert.encoder.layer.10.output.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,189 INFO] module.hr_bert.encoder.layer.10.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,189 INFO] module.hr_bert.encoder.layer.10.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,189 INFO] module.hr_bert.encoder.layer.10.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,189 INFO] module.hr_bert.encoder.layer.11.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,189 INFO] module.hr_bert.encoder.layer.11.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,189 INFO] module.hr_bert.encoder.layer.11.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,189 INFO] module.hr_bert.encoder.layer.11.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,189 INFO] module.hr_bert.encoder.layer.11.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,189 INFO] module.hr_bert.encoder.layer.11.attention.self.value.bias: 768\r\n",
      "[2022-07-08 12:46:00,189 INFO] module.hr_bert.encoder.layer.11.attention.output.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,189 INFO] module.hr_bert.encoder.layer.11.attention.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,189 INFO] module.hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,190 INFO] module.hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,190 INFO] module.hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,190 INFO] module.hr_bert.encoder.layer.11.intermediate.dense.bias: 3072\r\n",
      "[2022-07-08 12:46:00,190 INFO] module.hr_bert.encoder.layer.11.output.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,190 INFO] module.hr_bert.encoder.layer.11.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,190 INFO] module.hr_bert.encoder.layer.11.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,190 INFO] module.hr_bert.encoder.layer.11.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,190 INFO] module.hr_bert.pooler.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,190 INFO] module.hr_bert.pooler.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,190 INFO] module.tail_bert.embeddings.word_embeddings.weight: 23440896\r\n",
      "[2022-07-08 12:46:00,190 INFO] module.tail_bert.embeddings.position_embeddings.weight: 393216\r\n",
      "[2022-07-08 12:46:00,190 INFO] module.tail_bert.embeddings.token_type_embeddings.weight: 1536\r\n",
      "[2022-07-08 12:46:00,190 INFO] module.tail_bert.embeddings.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,190 INFO] module.tail_bert.embeddings.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,190 INFO] module.tail_bert.encoder.layer.0.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,191 INFO] module.tail_bert.encoder.layer.0.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,191 INFO] module.tail_bert.encoder.layer.0.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,191 INFO] module.tail_bert.encoder.layer.0.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,191 INFO] module.tail_bert.encoder.layer.0.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,191 INFO] module.tail_bert.encoder.layer.0.attention.self.value.bias: 768\r\n",
      "[2022-07-08 12:46:00,191 INFO] module.tail_bert.encoder.layer.0.attention.output.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,191 INFO] module.tail_bert.encoder.layer.0.attention.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,191 INFO] module.tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,191 INFO] module.tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,191 INFO] module.tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,191 INFO] module.tail_bert.encoder.layer.0.intermediate.dense.bias: 3072\r\n",
      "[2022-07-08 12:46:00,191 INFO] module.tail_bert.encoder.layer.0.output.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,191 INFO] module.tail_bert.encoder.layer.0.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,191 INFO] module.tail_bert.encoder.layer.0.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,191 INFO] module.tail_bert.encoder.layer.0.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,192 INFO] module.tail_bert.encoder.layer.1.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,192 INFO] module.tail_bert.encoder.layer.1.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,192 INFO] module.tail_bert.encoder.layer.1.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,192 INFO] module.tail_bert.encoder.layer.1.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,192 INFO] module.tail_bert.encoder.layer.1.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,192 INFO] module.tail_bert.encoder.layer.1.attention.self.value.bias: 768\r\n",
      "[2022-07-08 12:46:00,192 INFO] module.tail_bert.encoder.layer.1.attention.output.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,192 INFO] module.tail_bert.encoder.layer.1.attention.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,192 INFO] module.tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,192 INFO] module.tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,192 INFO] module.tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,192 INFO] module.tail_bert.encoder.layer.1.intermediate.dense.bias: 3072\r\n",
      "[2022-07-08 12:46:00,192 INFO] module.tail_bert.encoder.layer.1.output.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,192 INFO] module.tail_bert.encoder.layer.1.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,192 INFO] module.tail_bert.encoder.layer.1.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,193 INFO] module.tail_bert.encoder.layer.1.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,193 INFO] module.tail_bert.encoder.layer.2.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,193 INFO] module.tail_bert.encoder.layer.2.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,193 INFO] module.tail_bert.encoder.layer.2.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,193 INFO] module.tail_bert.encoder.layer.2.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,193 INFO] module.tail_bert.encoder.layer.2.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,193 INFO] module.tail_bert.encoder.layer.2.attention.self.value.bias: 768\r\n",
      "[2022-07-08 12:46:00,193 INFO] module.tail_bert.encoder.layer.2.attention.output.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,193 INFO] module.tail_bert.encoder.layer.2.attention.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,193 INFO] module.tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,193 INFO] module.tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,193 INFO] module.tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,193 INFO] module.tail_bert.encoder.layer.2.intermediate.dense.bias: 3072\r\n",
      "[2022-07-08 12:46:00,193 INFO] module.tail_bert.encoder.layer.2.output.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,194 INFO] module.tail_bert.encoder.layer.2.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,194 INFO] module.tail_bert.encoder.layer.2.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,194 INFO] module.tail_bert.encoder.layer.2.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,194 INFO] module.tail_bert.encoder.layer.3.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,194 INFO] module.tail_bert.encoder.layer.3.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,194 INFO] module.tail_bert.encoder.layer.3.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,194 INFO] module.tail_bert.encoder.layer.3.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,194 INFO] module.tail_bert.encoder.layer.3.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,194 INFO] module.tail_bert.encoder.layer.3.attention.self.value.bias: 768\r\n",
      "[2022-07-08 12:46:00,194 INFO] module.tail_bert.encoder.layer.3.attention.output.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,194 INFO] module.tail_bert.encoder.layer.3.attention.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,194 INFO] module.tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,194 INFO] module.tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,194 INFO] module.tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,194 INFO] module.tail_bert.encoder.layer.3.intermediate.dense.bias: 3072\r\n",
      "[2022-07-08 12:46:00,195 INFO] module.tail_bert.encoder.layer.3.output.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,195 INFO] module.tail_bert.encoder.layer.3.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,195 INFO] module.tail_bert.encoder.layer.3.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,195 INFO] module.tail_bert.encoder.layer.3.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,195 INFO] module.tail_bert.encoder.layer.4.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,195 INFO] module.tail_bert.encoder.layer.4.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,195 INFO] module.tail_bert.encoder.layer.4.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,195 INFO] module.tail_bert.encoder.layer.4.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,195 INFO] module.tail_bert.encoder.layer.4.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,195 INFO] module.tail_bert.encoder.layer.4.attention.self.value.bias: 768\r\n",
      "[2022-07-08 12:46:00,195 INFO] module.tail_bert.encoder.layer.4.attention.output.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,195 INFO] module.tail_bert.encoder.layer.4.attention.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,195 INFO] module.tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,195 INFO] module.tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,195 INFO] module.tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,196 INFO] module.tail_bert.encoder.layer.4.intermediate.dense.bias: 3072\r\n",
      "[2022-07-08 12:46:00,196 INFO] module.tail_bert.encoder.layer.4.output.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,196 INFO] module.tail_bert.encoder.layer.4.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,196 INFO] module.tail_bert.encoder.layer.4.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,196 INFO] module.tail_bert.encoder.layer.4.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,196 INFO] module.tail_bert.encoder.layer.5.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,196 INFO] module.tail_bert.encoder.layer.5.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,196 INFO] module.tail_bert.encoder.layer.5.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,196 INFO] module.tail_bert.encoder.layer.5.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,196 INFO] module.tail_bert.encoder.layer.5.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,196 INFO] module.tail_bert.encoder.layer.5.attention.self.value.bias: 768\r\n",
      "[2022-07-08 12:46:00,196 INFO] module.tail_bert.encoder.layer.5.attention.output.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,196 INFO] module.tail_bert.encoder.layer.5.attention.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,196 INFO] module.tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,196 INFO] module.tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,197 INFO] module.tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,197 INFO] module.tail_bert.encoder.layer.5.intermediate.dense.bias: 3072\r\n",
      "[2022-07-08 12:46:00,197 INFO] module.tail_bert.encoder.layer.5.output.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,197 INFO] module.tail_bert.encoder.layer.5.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,197 INFO] module.tail_bert.encoder.layer.5.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,197 INFO] module.tail_bert.encoder.layer.5.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,197 INFO] module.tail_bert.encoder.layer.6.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,197 INFO] module.tail_bert.encoder.layer.6.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,197 INFO] module.tail_bert.encoder.layer.6.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,197 INFO] module.tail_bert.encoder.layer.6.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,197 INFO] module.tail_bert.encoder.layer.6.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,197 INFO] module.tail_bert.encoder.layer.6.attention.self.value.bias: 768\r\n",
      "[2022-07-08 12:46:00,197 INFO] module.tail_bert.encoder.layer.6.attention.output.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,197 INFO] module.tail_bert.encoder.layer.6.attention.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,198 INFO] module.tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,198 INFO] module.tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,198 INFO] module.tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,198 INFO] module.tail_bert.encoder.layer.6.intermediate.dense.bias: 3072\r\n",
      "[2022-07-08 12:46:00,198 INFO] module.tail_bert.encoder.layer.6.output.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,198 INFO] module.tail_bert.encoder.layer.6.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,198 INFO] module.tail_bert.encoder.layer.6.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,198 INFO] module.tail_bert.encoder.layer.6.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,198 INFO] module.tail_bert.encoder.layer.7.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,198 INFO] module.tail_bert.encoder.layer.7.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,198 INFO] module.tail_bert.encoder.layer.7.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,198 INFO] module.tail_bert.encoder.layer.7.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,198 INFO] module.tail_bert.encoder.layer.7.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,198 INFO] module.tail_bert.encoder.layer.7.attention.self.value.bias: 768\r\n",
      "[2022-07-08 12:46:00,198 INFO] module.tail_bert.encoder.layer.7.attention.output.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,199 INFO] module.tail_bert.encoder.layer.7.attention.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,199 INFO] module.tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,199 INFO] module.tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,199 INFO] module.tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,199 INFO] module.tail_bert.encoder.layer.7.intermediate.dense.bias: 3072\r\n",
      "[2022-07-08 12:46:00,199 INFO] module.tail_bert.encoder.layer.7.output.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,199 INFO] module.tail_bert.encoder.layer.7.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,199 INFO] module.tail_bert.encoder.layer.7.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,199 INFO] module.tail_bert.encoder.layer.7.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,199 INFO] module.tail_bert.encoder.layer.8.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,199 INFO] module.tail_bert.encoder.layer.8.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,199 INFO] module.tail_bert.encoder.layer.8.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,199 INFO] module.tail_bert.encoder.layer.8.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,199 INFO] module.tail_bert.encoder.layer.8.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,199 INFO] module.tail_bert.encoder.layer.8.attention.self.value.bias: 768\r\n",
      "[2022-07-08 12:46:00,200 INFO] module.tail_bert.encoder.layer.8.attention.output.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,200 INFO] module.tail_bert.encoder.layer.8.attention.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,200 INFO] module.tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,200 INFO] module.tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,200 INFO] module.tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,200 INFO] module.tail_bert.encoder.layer.8.intermediate.dense.bias: 3072\r\n",
      "[2022-07-08 12:46:00,200 INFO] module.tail_bert.encoder.layer.8.output.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,200 INFO] module.tail_bert.encoder.layer.8.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,200 INFO] module.tail_bert.encoder.layer.8.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,200 INFO] module.tail_bert.encoder.layer.8.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,200 INFO] module.tail_bert.encoder.layer.9.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,200 INFO] module.tail_bert.encoder.layer.9.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,200 INFO] module.tail_bert.encoder.layer.9.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,200 INFO] module.tail_bert.encoder.layer.9.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,200 INFO] module.tail_bert.encoder.layer.9.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,201 INFO] module.tail_bert.encoder.layer.9.attention.self.value.bias: 768\r\n",
      "[2022-07-08 12:46:00,201 INFO] module.tail_bert.encoder.layer.9.attention.output.dense.weight: 589824\r\n",
      "[2022-07-08 12:46:00,201 INFO] module.tail_bert.encoder.layer.9.attention.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,201 INFO] module.tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,201 INFO] module.tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,201 INFO] module.tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,201 INFO] module.tail_bert.encoder.layer.9.intermediate.dense.bias: 3072\r\n",
      "[2022-07-08 12:46:00,201 INFO] module.tail_bert.encoder.layer.9.output.dense.weight: 2359296\r\n",
      "[2022-07-08 12:46:00,201 INFO] module.tail_bert.encoder.layer.9.output.dense.bias: 768\r\n",
      "[2022-07-08 12:46:00,201 INFO] module.tail_bert.encoder.layer.9.output.LayerNorm.weight: 768\r\n",
      "[2022-07-08 12:46:00,201 INFO] module.tail_bert.encoder.layer.9.output.LayerNorm.bias: 768\r\n",
      "[2022-07-08 12:46:00,201 INFO] module.tail_bert.encoder.layer.10.attention.self.query.weight: 589824\r\n",
      "[2022-07-08 12:46:00,201 INFO] module.tail_bert.encoder.layer.10.attention.self.query.bias: 768\r\n",
      "[2022-07-08 12:46:00,201 INFO] module.tail_bert.encoder.layer.10.attention.self.key.weight: 589824\r\n",
      "[2022-07-08 12:46:00,201 INFO] module.tail_bert.encoder.layer.10.attention.self.key.bias: 768\r\n",
      "[2022-07-08 12:46:00,202 INFO] module.tail_bert.encoder.layer.10.attention.self.value.weight: 589824\r\n",
      "[2022-07-08 12:46:00,202 INFO] module.tail_bert.encoder.layer.10.attention.self.value.bias: 768\n",
      "[2022-07-08 12:46:00,202 INFO] module.tail_bert.encoder.layer.10.attention.output.dense.weight: 589824\n",
      "[2022-07-08 12:46:00,202 INFO] module.tail_bert.encoder.layer.10.attention.output.dense.bias: 768\n",
      "[2022-07-08 12:46:00,202 INFO] module.tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-08 12:46:00,202 INFO] module.tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-08 12:46:00,202 INFO] module.tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296\n",
      "[2022-07-08 12:46:00,202 INFO] module.tail_bert.encoder.layer.10.intermediate.dense.bias: 3072\n",
      "[2022-07-08 12:46:00,202 INFO] module.tail_bert.encoder.layer.10.output.dense.weight: 2359296\n",
      "[2022-07-08 12:46:00,202 INFO] module.tail_bert.encoder.layer.10.output.dense.bias: 768\n",
      "[2022-07-08 12:46:00,202 INFO] module.tail_bert.encoder.layer.10.output.LayerNorm.weight: 768\n",
      "[2022-07-08 12:46:00,202 INFO] module.tail_bert.encoder.layer.10.output.LayerNorm.bias: 768\n",
      "[2022-07-08 12:46:00,202 INFO] module.tail_bert.encoder.layer.11.attention.self.query.weight: 589824\n",
      "[2022-07-08 12:46:00,202 INFO] module.tail_bert.encoder.layer.11.attention.self.query.bias: 768\n",
      "[2022-07-08 12:46:00,202 INFO] module.tail_bert.encoder.layer.11.attention.self.key.weight: 589824\n",
      "[2022-07-08 12:46:00,203 INFO] module.tail_bert.encoder.layer.11.attention.self.key.bias: 768\n",
      "[2022-07-08 12:46:00,203 INFO] module.tail_bert.encoder.layer.11.attention.self.value.weight: 589824\n",
      "[2022-07-08 12:46:00,203 INFO] module.tail_bert.encoder.layer.11.attention.self.value.bias: 768\n",
      "[2022-07-08 12:46:00,203 INFO] module.tail_bert.encoder.layer.11.attention.output.dense.weight: 589824\n",
      "[2022-07-08 12:46:00,203 INFO] module.tail_bert.encoder.layer.11.attention.output.dense.bias: 768\n",
      "[2022-07-08 12:46:00,203 INFO] module.tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768\n",
      "[2022-07-08 12:46:00,203 INFO] module.tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768\n",
      "[2022-07-08 12:46:00,203 INFO] module.tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296\n",
      "[2022-07-08 12:46:00,203 INFO] module.tail_bert.encoder.layer.11.intermediate.dense.bias: 3072\n",
      "[2022-07-08 12:46:00,203 INFO] module.tail_bert.encoder.layer.11.output.dense.weight: 2359296\n",
      "[2022-07-08 12:46:00,203 INFO] module.tail_bert.encoder.layer.11.output.dense.bias: 768\n",
      "[2022-07-08 12:46:00,203 INFO] module.tail_bert.encoder.layer.11.output.LayerNorm.weight: 768\n",
      "[2022-07-08 12:46:00,203 INFO] module.tail_bert.encoder.layer.11.output.LayerNorm.bias: 768\n",
      "[2022-07-08 12:46:00,203 INFO] module.tail_bert.pooler.dense.weight: 589824\n",
      "[2022-07-08 12:46:00,203 INFO] module.tail_bert.pooler.dense.bias: 768\n",
      "[2022-07-08 12:46:00,204 INFO] Number of parameters: 218.0M\n",
      "[2022-07-08 12:46:00,204 INFO] In test mode: False\n",
      "[2022-07-08 12:46:00,866 INFO] Load 272115 examples from /workspace/SimKGC/data/FB15k237/train.txt.json\n",
      "[2022-07-08 12:46:03,066 INFO] In test mode: False\n",
      "[2022-07-08 12:46:03,105 INFO] Load 17535 examples from /workspace/SimKGC/data/FB15k237/valid.txt.json\n",
      "[2022-07-08 12:46:03,175 INFO] Total training steps: 5314, warmup steps: 400\n",
      "[2022-07-08 12:46:03,175 INFO] Args={\n",
      "    \"pretrained_model\": \"bert-base-uncased\",\n",
      "    \"task\": \"FB15k237\",\n",
      "    \"train_path\": \"/workspace/SimKGC/data/FB15k237/train.txt.json\",\n",
      "    \"valid_path\": \"/workspace/SimKGC/data/FB15k237/valid.txt.json\",\n",
      "    \"model_dir\": \"./checkpoint/fb15k237/\",\n",
      "    \"warmup\": 400,\n",
      "    \"max_to_keep\": 5,\n",
      "    \"grad_clip\": 10.0,\n",
      "    \"pooling\": \"mean\",\n",
      "    \"dropout\": 0.1,\n",
      "    \"use_amp\": true,\n",
      "    \"t\": 0.05,\n",
      "    \"use_link_graph\": true,\n",
      "    \"eval_every_n_step\": 10000,\n",
      "    \"pre_batch\": 2,\n",
      "    \"pre_batch_weight\": 0.5,\n",
      "    \"additive_margin\": 0.02,\n",
      "    \"finetune_t\": true,\n",
      "    \"max_num_tokens\": 50,\n",
      "    \"use_self_negative\": true,\n",
      "    \"workers\": 4,\n",
      "    \"epochs\": 10,\n",
      "    \"batch_size\": 1024,\n",
      "    \"lr\": 5e-05,\n",
      "    \"lr_scheduler\": \"linear\",\n",
      "    \"weight_decay\": 0.0001,\n",
      "    \"print_freq\": 20,\n",
      "    \"seed\": null,\n",
      "    \"concept_path\": \"\",\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"no_desc\": true,\n",
      "    \"no_pretraining\": true,\n",
      "    \"is_test\": false,\n",
      "    \"rerank_n_hop\": 2,\n",
      "    \"neighbor_weight\": 0.0,\n",
      "    \"eval_model_path\": \"\"\n",
      "}\n",
      "[2022-07-08 12:46:11,836 INFO] Epoch: [0][  0/531]\tLoss 15.83 (15.83)\tInvT  20.00 ( 20.00)\tAcc@1   0.00 (  0.00)\tAcc@3   0.00 (  0.00)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:46:46,521 INFO] Epoch: [0][ 20/531]\tLoss 14.64 (15.14)\tInvT  20.00 ( 20.00)\tAcc@1   0.00 (  0.01)\tAcc@3   0.00 (  0.02)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:47:21,292 INFO] Epoch: [0][ 40/531]\tLoss 14.42 (14.84)\tInvT  20.00 ( 20.00)\tAcc@1   0.00 (  0.00)\tAcc@3   0.00 (  0.01)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:47:56,425 INFO] Epoch: [0][ 60/531]\tLoss 14.15 (14.67)\tInvT  20.00 ( 20.00)\tAcc@1   0.00 (  0.00)\tAcc@3   0.00 (  0.01)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:48:31,579 INFO] Epoch: [0][ 80/531]\tLoss 13.64 (14.47)\tInvT  19.99 ( 20.00)\tAcc@1   1.17 (  0.10)\tAcc@3   2.05 (  0.22)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:49:06,955 INFO] Epoch: [0][100/531]\tLoss 13.25 (14.25)\tInvT  19.99 ( 20.00)\tAcc@1   3.42 (  0.58)\tAcc@3   6.05 (  1.05)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:49:42,139 INFO] Epoch: [0][120/531]\tLoss 12.7 (14.01)\tInvT  19.99 ( 19.99)\tAcc@1   4.59 (  1.24)\tAcc@3   7.81 (  2.09)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:50:17,354 INFO] Epoch: [0][140/531]\tLoss 12.22 (13.78)\tInvT  19.98 ( 19.99)\tAcc@1   3.61 (  1.77)\tAcc@3   6.35 (  2.97)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:50:52,844 INFO] Epoch: [0][160/531]\tLoss 11.84 (13.57)\tInvT  19.98 ( 19.99)\tAcc@1   6.45 (  2.24)\tAcc@3   9.96 (  3.72)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:51:28,297 INFO] Epoch: [0][180/531]\tLoss 11.41 (13.37)\tInvT  19.98 ( 19.99)\tAcc@1   6.05 (  2.63)\tAcc@3   9.28 (  4.35)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:52:03,714 INFO] Epoch: [0][200/531]\tLoss 11.37 (13.18)\tInvT  19.97 ( 19.99)\tAcc@1   6.84 (  2.97)\tAcc@3  10.55 (  4.92)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:52:38,929 INFO] Epoch: [0][220/531]\tLoss 11.12 (13.01)\tInvT  19.97 ( 19.99)\tAcc@1   7.62 (  3.27)\tAcc@3  13.18 (  5.44)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:53:14,342 INFO] Epoch: [0][240/531]\tLoss 10.83 (12.84)\tInvT  19.96 ( 19.99)\tAcc@1   6.25 (  3.56)\tAcc@3  10.45 (  5.94)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:53:49,805 INFO] Epoch: [0][260/531]\tLoss 10.52 (12.69)\tInvT  19.96 ( 19.98)\tAcc@1   8.98 (  3.85)\tAcc@3  14.55 (  6.43)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:54:25,074 INFO] Epoch: [0][280/531]\tLoss 10.35 (12.54)\tInvT  19.95 ( 19.98)\tAcc@1   9.18 (  4.10)\tAcc@3  14.26 (  6.87)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:55:00,517 INFO] Epoch: [0][300/531]\tLoss 10.19 (12.4)\tInvT  19.94 ( 19.98)\tAcc@1   7.81 (  4.32)\tAcc@3  13.87 (  7.26)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:55:35,745 INFO] Epoch: [0][320/531]\tLoss 10.2 (12.27)\tInvT  19.94 ( 19.98)\tAcc@1   8.11 (  4.55)\tAcc@3  12.89 (  7.66)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:56:10,844 INFO] Epoch: [0][340/531]\tLoss 10.05 (12.14)\tInvT  19.93 ( 19.97)\tAcc@1   9.57 (  4.76)\tAcc@3  13.77 (  8.03)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:56:46,216 INFO] Epoch: [0][360/531]\tLoss 9.669 (12.01)\tInvT  19.92 ( 19.97)\tAcc@1  10.16 (  4.98)\tAcc@3  14.75 (  8.38)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:57:21,951 INFO] Epoch: [0][380/531]\tLoss 9.7 (11.89)\tInvT  19.91 ( 19.97)\tAcc@1   9.57 (  5.18)\tAcc@3  14.84 (  8.74)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:57:57,502 INFO] Epoch: [0][400/531]\tLoss 9.5 (11.77)\tInvT  19.89 ( 19.96)\tAcc@1   8.40 (  5.39)\tAcc@3  15.72 (  9.09)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:58:32,569 INFO] Epoch: [0][420/531]\tLoss 9.427 (11.66)\tInvT  19.88 ( 19.96)\tAcc@1   7.81 (  5.57)\tAcc@3  15.72 (  9.42)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:59:08,370 INFO] Epoch: [0][440/531]\tLoss 9.326 (11.55)\tInvT  19.86 ( 19.96)\tAcc@1   9.67 (  5.77)\tAcc@3  16.99 (  9.76)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 12:59:43,567 INFO] Epoch: [0][460/531]\tLoss 8.981 (11.44)\tInvT  19.84 ( 19.95)\tAcc@1   9.57 (  5.94)\tAcc@3  17.58 ( 10.06)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:00:19,312 INFO] Epoch: [0][480/531]\tLoss 9.072 (11.34)\tInvT  19.82 ( 19.95)\tAcc@1   9.28 (  6.10)\tAcc@3  15.23 ( 10.35)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:00:54,264 INFO] Epoch: [0][500/531]\tLoss 8.877 (11.25)\tInvT  19.80 ( 19.94)\tAcc@1  10.16 (  6.24)\tAcc@3  18.46 ( 10.64)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:01:29,566 INFO] Epoch: [0][520/531]\tLoss 8.893 (11.15)\tInvT  19.77 ( 19.94)\tAcc@1   9.38 (  6.39)\tAcc@3  17.48 ( 10.92)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:01:47,143 INFO] Learning rate: 4.866707366707367e-05\n",
      "[2022-07-08 13:02:10,932 INFO] Epoch 0, valid metric: {\"Acc@1\": 15.03, \"Acc@3\": 21.956, \"loss\": 4.387}\n",
      "[2022-07-08 13:02:17,202 INFO] Epoch: [1][  0/531]\tLoss 8.566 (8.566)\tInvT  19.76 ( 19.76)\tAcc@1  11.04 ( 11.04)\tAcc@3  20.02 ( 20.02)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:02:52,226 INFO] Epoch: [1][ 20/531]\tLoss 8.638 (8.63)\tInvT  19.74 ( 19.75)\tAcc@1  10.94 ( 10.42)\tAcc@3  18.95 ( 18.55)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:03:27,391 INFO] Epoch: [1][ 40/531]\tLoss 8.508 (8.618)\tInvT  19.71 ( 19.74)\tAcc@1  10.64 ( 10.50)\tAcc@3  18.55 ( 18.48)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:04:02,929 INFO] Epoch: [1][ 60/531]\tLoss 8.447 (8.594)\tInvT  19.69 ( 19.73)\tAcc@1  10.45 ( 10.58)\tAcc@3  18.46 ( 18.59)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:04:37,914 INFO] Epoch: [1][ 80/531]\tLoss 8.353 (8.543)\tInvT  19.67 ( 19.71)\tAcc@1  11.72 ( 10.79)\tAcc@3  20.61 ( 18.87)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:05:12,955 INFO] Epoch: [1][100/531]\tLoss 8.285 (8.519)\tInvT  19.64 ( 19.70)\tAcc@1  10.94 ( 10.87)\tAcc@3  19.34 ( 18.99)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:05:48,670 INFO] Epoch: [1][120/531]\tLoss 8.416 (8.501)\tInvT  19.62 ( 19.69)\tAcc@1  11.23 ( 10.90)\tAcc@3  20.61 ( 19.09)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:06:23,915 INFO] Epoch: [1][140/531]\tLoss 8.156 (8.473)\tInvT  19.60 ( 19.68)\tAcc@1  11.82 ( 10.97)\tAcc@3  20.02 ( 19.21)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:06:59,092 INFO] Epoch: [1][160/531]\tLoss 8.283 (8.444)\tInvT  19.58 ( 19.67)\tAcc@1  12.60 ( 11.10)\tAcc@3  20.51 ( 19.34)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:07:34,096 INFO] Epoch: [1][180/531]\tLoss 7.983 (8.421)\tInvT  19.55 ( 19.66)\tAcc@1  12.01 ( 11.14)\tAcc@3  20.90 ( 19.42)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:08:09,071 INFO] Epoch: [1][200/531]\tLoss 8.039 (8.398)\tInvT  19.53 ( 19.64)\tAcc@1  11.91 ( 11.18)\tAcc@3  22.66 ( 19.50)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:08:44,756 INFO] Epoch: [1][220/531]\tLoss 7.864 (8.375)\tInvT  19.51 ( 19.63)\tAcc@1  12.60 ( 11.18)\tAcc@3  21.88 ( 19.57)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:09:20,149 INFO] Epoch: [1][240/531]\tLoss 8.236 (8.35)\tInvT  19.49 ( 19.62)\tAcc@1  10.06 ( 11.25)\tAcc@3  18.55 ( 19.67)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:09:55,341 INFO] Epoch: [1][260/531]\tLoss 7.889 (8.32)\tInvT  19.47 ( 19.61)\tAcc@1  12.50 ( 11.35)\tAcc@3  22.95 ( 19.85)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:10:30,535 INFO] Epoch: [1][280/531]\tLoss 7.874 (8.295)\tInvT  19.45 ( 19.60)\tAcc@1  12.70 ( 11.43)\tAcc@3  22.66 ( 19.98)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:11:05,669 INFO] Epoch: [1][300/531]\tLoss 8.015 (8.271)\tInvT  19.43 ( 19.59)\tAcc@1  12.11 ( 11.48)\tAcc@3  21.97 ( 20.09)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:11:41,032 INFO] Epoch: [1][320/531]\tLoss 8.025 (8.249)\tInvT  19.41 ( 19.58)\tAcc@1  13.87 ( 11.54)\tAcc@3  21.68 ( 20.22)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:12:16,282 INFO] Epoch: [1][340/531]\tLoss 7.92 (8.223)\tInvT  19.39 ( 19.57)\tAcc@1  12.89 ( 11.61)\tAcc@3  21.48 ( 20.34)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:12:51,541 INFO] Epoch: [1][360/531]\tLoss 7.581 (8.2)\tInvT  19.37 ( 19.56)\tAcc@1  13.18 ( 11.68)\tAcc@3  23.44 ( 20.46)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:13:26,904 INFO] Epoch: [1][380/531]\tLoss 7.598 (8.18)\tInvT  19.35 ( 19.55)\tAcc@1  13.38 ( 11.75)\tAcc@3  23.93 ( 20.57)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:14:02,209 INFO] Epoch: [1][400/531]\tLoss 7.834 (8.157)\tInvT  19.33 ( 19.54)\tAcc@1  12.01 ( 11.82)\tAcc@3  22.66 ( 20.71)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:14:37,024 INFO] Epoch: [1][420/531]\tLoss 7.755 (8.136)\tInvT  19.31 ( 19.53)\tAcc@1  13.77 ( 11.87)\tAcc@3  23.54 ( 20.82)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:15:12,315 INFO] Epoch: [1][440/531]\tLoss 7.451 (8.114)\tInvT  19.29 ( 19.52)\tAcc@1  14.94 ( 11.95)\tAcc@3  25.49 ( 20.96)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:15:47,751 INFO] Epoch: [1][460/531]\tLoss 7.736 (8.095)\tInvT  19.28 ( 19.51)\tAcc@1  11.23 ( 12.00)\tAcc@3  21.78 ( 21.07)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:16:23,227 INFO] Epoch: [1][480/531]\tLoss 7.79 (8.077)\tInvT  19.26 ( 19.50)\tAcc@1  11.52 ( 12.05)\tAcc@3  21.19 ( 21.20)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:16:58,479 INFO] Epoch: [1][500/531]\tLoss 7.48 (8.057)\tInvT  19.24 ( 19.49)\tAcc@1  13.09 ( 12.13)\tAcc@3  23.63 ( 21.33)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:17:34,125 INFO] Epoch: [1][520/531]\tLoss 7.688 (8.037)\tInvT  19.22 ( 19.48)\tAcc@1  11.91 ( 12.17)\tAcc@3  23.83 ( 21.47)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:17:51,460 INFO] Learning rate: 4.326414326414326e-05\n",
      "[2022-07-08 13:18:16,235 INFO] Epoch 1, valid metric: {\"Acc@1\": 18.061, \"Acc@3\": 27.054, \"loss\": 3.928}\n",
      "[2022-07-08 13:18:22,960 INFO] Epoch: [2][  0/531]\tLoss 7.279 (7.279)\tInvT  19.22 ( 19.22)\tAcc@1  15.23 ( 15.23)\tAcc@3  26.37 ( 26.37)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:18:58,235 INFO] Epoch: [2][ 20/531]\tLoss 7.133 (7.357)\tInvT  19.20 ( 19.21)\tAcc@1  16.41 ( 14.92)\tAcc@3  29.00 ( 26.70)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:19:33,724 INFO] Epoch: [2][ 40/531]\tLoss 7.546 (7.352)\tInvT  19.18 ( 19.20)\tAcc@1  14.06 ( 14.57)\tAcc@3  24.41 ( 26.49)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:20:09,251 INFO] Epoch: [2][ 60/531]\tLoss 7.167 (7.342)\tInvT  19.17 ( 19.19)\tAcc@1  14.26 ( 14.71)\tAcc@3  26.37 ( 26.59)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:20:44,415 INFO] Epoch: [2][ 80/531]\tLoss 7.117 (7.325)\tInvT  19.15 ( 19.18)\tAcc@1  16.80 ( 14.92)\tAcc@3  31.25 ( 26.91)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:21:19,723 INFO] Epoch: [2][100/531]\tLoss 7.537 (7.311)\tInvT  19.14 ( 19.18)\tAcc@1  14.75 ( 15.04)\tAcc@3  26.17 ( 27.05)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:21:54,774 INFO] Epoch: [2][120/531]\tLoss 7.064 (7.295)\tInvT  19.12 ( 19.17)\tAcc@1  17.38 ( 15.18)\tAcc@3  28.42 ( 27.25)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:22:30,069 INFO] Epoch: [2][140/531]\tLoss 7.064 (7.283)\tInvT  19.11 ( 19.16)\tAcc@1  17.29 ( 15.19)\tAcc@3  29.20 ( 27.33)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:23:05,549 INFO] Epoch: [2][160/531]\tLoss 7.262 (7.269)\tInvT  19.09 ( 19.15)\tAcc@1  15.43 ( 15.21)\tAcc@3  28.71 ( 27.48)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:23:40,781 INFO] Epoch: [2][180/531]\tLoss 7.036 (7.256)\tInvT  19.07 ( 19.14)\tAcc@1  16.11 ( 15.26)\tAcc@3  29.20 ( 27.60)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:24:15,852 INFO] Epoch: [2][200/531]\tLoss 6.897 (7.243)\tInvT  19.06 ( 19.14)\tAcc@1  17.68 ( 15.37)\tAcc@3  31.05 ( 27.77)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:24:51,193 INFO] Epoch: [2][220/531]\tLoss 6.886 (7.229)\tInvT  19.05 ( 19.13)\tAcc@1  17.68 ( 15.44)\tAcc@3  31.35 ( 27.90)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:25:26,371 INFO] Epoch: [2][240/531]\tLoss 7.041 (7.221)\tInvT  19.03 ( 19.12)\tAcc@1  15.62 ( 15.49)\tAcc@3  29.30 ( 28.04)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:26:01,327 INFO] Epoch: [2][260/531]\tLoss 7.117 (7.21)\tInvT  19.02 ( 19.11)\tAcc@1  16.41 ( 15.56)\tAcc@3  29.39 ( 28.17)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:26:36,779 INFO] Epoch: [2][280/531]\tLoss 7.224 (7.195)\tInvT  19.00 ( 19.11)\tAcc@1  16.02 ( 15.62)\tAcc@3  29.88 ( 28.33)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:27:12,273 INFO] Epoch: [2][300/531]\tLoss 7.023 (7.182)\tInvT  18.99 ( 19.10)\tAcc@1  16.50 ( 15.68)\tAcc@3  31.45 ( 28.48)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:27:47,543 INFO] Epoch: [2][320/531]\tLoss 6.905 (7.17)\tInvT  18.98 ( 19.09)\tAcc@1  18.55 ( 15.76)\tAcc@3  31.25 ( 28.63)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:28:23,215 INFO] Epoch: [2][340/531]\tLoss 6.781 (7.155)\tInvT  18.96 ( 19.08)\tAcc@1  17.87 ( 15.86)\tAcc@3  30.96 ( 28.78)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:28:58,471 INFO] Epoch: [2][360/531]\tLoss 6.94 (7.142)\tInvT  18.95 ( 19.08)\tAcc@1  16.50 ( 15.91)\tAcc@3  30.66 ( 28.92)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:29:34,140 INFO] Epoch: [2][380/531]\tLoss 6.78 (7.13)\tInvT  18.93 ( 19.07)\tAcc@1  16.99 ( 15.96)\tAcc@3  32.52 ( 29.06)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:30:09,881 INFO] Epoch: [2][400/531]\tLoss 6.916 (7.119)\tInvT  18.92 ( 19.06)\tAcc@1  18.16 ( 16.04)\tAcc@3  34.18 ( 29.20)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:30:45,010 INFO] Epoch: [2][420/531]\tLoss 6.95 (7.104)\tInvT  18.91 ( 19.06)\tAcc@1  16.99 ( 16.12)\tAcc@3  29.79 ( 29.35)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:31:20,100 INFO] Epoch: [2][440/531]\tLoss 6.885 (7.093)\tInvT  18.89 ( 19.05)\tAcc@1  18.26 ( 16.19)\tAcc@3  31.84 ( 29.49)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:31:55,531 INFO] Epoch: [2][460/531]\tLoss 6.894 (7.081)\tInvT  18.88 ( 19.04)\tAcc@1  17.48 ( 16.26)\tAcc@3  31.84 ( 29.63)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:32:31,073 INFO] Epoch: [2][480/531]\tLoss 6.761 (7.072)\tInvT  18.87 ( 19.04)\tAcc@1  17.97 ( 16.31)\tAcc@3  32.71 ( 29.73)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:33:06,493 INFO] Epoch: [2][500/531]\tLoss 6.886 (7.062)\tInvT  18.85 ( 19.03)\tAcc@1  16.11 ( 16.37)\tAcc@3  31.84 ( 29.85)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:33:42,178 INFO] Epoch: [2][520/531]\tLoss 6.768 (7.05)\tInvT  18.84 ( 19.02)\tAcc@1  18.07 ( 16.44)\tAcc@3  32.81 ( 29.97)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:33:59,298 INFO] Learning rate: 3.7861212861212864e-05\n",
      "[2022-07-08 13:34:23,562 INFO] Epoch 2, valid metric: {\"Acc@1\": 19.886, \"Acc@3\": 30.279, \"loss\": 3.742}\n",
      "[2022-07-08 13:34:30,277 INFO] Epoch: [3][  0/531]\tLoss 6.485 (6.485)\tInvT  18.84 ( 18.84)\tAcc@1  20.12 ( 20.12)\tAcc@3  36.23 ( 36.23)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:35:05,761 INFO] Epoch: [3][ 20/531]\tLoss 6.504 (6.531)\tInvT  18.82 ( 18.83)\tAcc@1  19.92 ( 19.55)\tAcc@3  36.62 ( 35.46)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:35:41,129 INFO] Epoch: [3][ 40/531]\tLoss 6.638 (6.55)\tInvT  18.81 ( 18.82)\tAcc@1  19.24 ( 19.52)\tAcc@3  35.55 ( 35.54)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:36:16,390 INFO] Epoch: [3][ 60/531]\tLoss 6.308 (6.539)\tInvT  18.80 ( 18.82)\tAcc@1  21.29 ( 19.61)\tAcc@3  36.33 ( 35.67)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:36:51,666 INFO] Epoch: [3][ 80/531]\tLoss 6.616 (6.527)\tInvT  18.79 ( 18.81)\tAcc@1  19.53 ( 19.70)\tAcc@3  34.57 ( 35.82)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:37:27,065 INFO] Epoch: [3][100/531]\tLoss 6.491 (6.518)\tInvT  18.78 ( 18.81)\tAcc@1  20.61 ( 19.85)\tAcc@3  36.62 ( 35.97)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:38:02,570 INFO] Epoch: [3][120/531]\tLoss 6.374 (6.504)\tInvT  18.77 ( 18.80)\tAcc@1  21.09 ( 19.94)\tAcc@3  37.99 ( 36.15)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:38:37,995 INFO] Epoch: [3][140/531]\tLoss 6.522 (6.501)\tInvT  18.76 ( 18.80)\tAcc@1  20.51 ( 20.01)\tAcc@3  36.82 ( 36.21)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:39:13,137 INFO] Epoch: [3][160/531]\tLoss 6.482 (6.494)\tInvT  18.75 ( 18.79)\tAcc@1  21.78 ( 20.15)\tAcc@3  35.74 ( 36.39)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:39:48,577 INFO] Epoch: [3][180/531]\tLoss 6.416 (6.494)\tInvT  18.74 ( 18.79)\tAcc@1  21.19 ( 20.16)\tAcc@3  38.18 ( 36.38)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:40:24,212 INFO] Epoch: [3][200/531]\tLoss 6.569 (6.495)\tInvT  18.73 ( 18.78)\tAcc@1  20.31 ( 20.19)\tAcc@3  36.33 ( 36.43)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:40:59,645 INFO] Epoch: [3][220/531]\tLoss 6.199 (6.486)\tInvT  18.72 ( 18.78)\tAcc@1  24.90 ( 20.30)\tAcc@3  42.09 ( 36.55)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:41:34,979 INFO] Epoch: [3][240/531]\tLoss 6.437 (6.485)\tInvT  18.71 ( 18.77)\tAcc@1  19.04 ( 20.33)\tAcc@3  36.62 ( 36.59)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:42:10,288 INFO] Epoch: [3][260/531]\tLoss 6.217 (6.48)\tInvT  18.70 ( 18.76)\tAcc@1  22.66 ( 20.39)\tAcc@3  38.96 ( 36.65)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:42:45,879 INFO] Epoch: [3][280/531]\tLoss 6.392 (6.475)\tInvT  18.69 ( 18.76)\tAcc@1  21.97 ( 20.42)\tAcc@3  38.67 ( 36.75)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:43:21,189 INFO] Epoch: [3][300/531]\tLoss 6.161 (6.468)\tInvT  18.68 ( 18.75)\tAcc@1  21.58 ( 20.47)\tAcc@3  40.14 ( 36.85)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:43:56,735 INFO] Epoch: [3][320/531]\tLoss 6.07 (6.463)\tInvT  18.67 ( 18.75)\tAcc@1  22.17 ( 20.51)\tAcc@3  38.96 ( 36.92)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:44:31,985 INFO] Epoch: [3][340/531]\tLoss 6.297 (6.456)\tInvT  18.66 ( 18.74)\tAcc@1  21.78 ( 20.56)\tAcc@3  38.38 ( 37.01)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:45:07,287 INFO] Epoch: [3][360/531]\tLoss 6.318 (6.447)\tInvT  18.65 ( 18.74)\tAcc@1  21.68 ( 20.65)\tAcc@3  39.45 ( 37.14)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:45:42,243 INFO] Epoch: [3][380/531]\tLoss 6.466 (6.439)\tInvT  18.64 ( 18.73)\tAcc@1  20.31 ( 20.72)\tAcc@3  36.72 ( 37.23)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:46:17,773 INFO] Epoch: [3][400/531]\tLoss 6.427 (6.432)\tInvT  18.63 ( 18.73)\tAcc@1  20.90 ( 20.77)\tAcc@3  38.18 ( 37.31)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:46:52,903 INFO] Epoch: [3][420/531]\tLoss 6.543 (6.424)\tInvT  18.62 ( 18.72)\tAcc@1  20.51 ( 20.81)\tAcc@3  35.35 ( 37.40)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:47:28,621 INFO] Epoch: [3][440/531]\tLoss 6.314 (6.417)\tInvT  18.61 ( 18.72)\tAcc@1  22.66 ( 20.87)\tAcc@3  39.26 ( 37.50)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:48:04,036 INFO] Epoch: [3][460/531]\tLoss 6.48 (6.412)\tInvT  18.60 ( 18.71)\tAcc@1  21.68 ( 20.92)\tAcc@3  38.48 ( 37.57)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:48:39,439 INFO] Epoch: [3][480/531]\tLoss 6.281 (6.406)\tInvT  18.59 ( 18.71)\tAcc@1  24.41 ( 20.99)\tAcc@3  40.82 ( 37.66)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:49:14,858 INFO] Epoch: [3][500/531]\tLoss 6.194 (6.398)\tInvT  18.58 ( 18.70)\tAcc@1  23.34 ( 21.06)\tAcc@3  40.23 ( 37.76)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:49:50,293 INFO] Epoch: [3][520/531]\tLoss 6.139 (6.392)\tInvT  18.57 ( 18.70)\tAcc@1  22.07 ( 21.11)\tAcc@3  38.96 ( 37.85)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:50:07,691 INFO] Learning rate: 3.245828245828246e-05\n",
      "[2022-07-08 13:50:31,925 INFO] Epoch 3, valid metric: {\"Acc@1\": 21.603, \"Acc@3\": 32.889, \"loss\": 3.621}\n",
      "[2022-07-08 13:50:38,655 INFO] Epoch: [4][  0/531]\tLoss 6.045 (6.045)\tInvT  18.57 ( 18.57)\tAcc@1  23.93 ( 23.93)\tAcc@3  43.65 ( 43.65)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:51:13,934 INFO] Epoch: [4][ 20/531]\tLoss 6.05 (5.987)\tInvT  18.56 ( 18.56)\tAcc@1  25.49 ( 24.78)\tAcc@3  42.29 ( 42.48)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:51:49,381 INFO] Epoch: [4][ 40/531]\tLoss 6.204 (5.984)\tInvT  18.55 ( 18.56)\tAcc@1  25.59 ( 24.91)\tAcc@3  41.02 ( 42.77)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:52:24,338 INFO] Epoch: [4][ 60/531]\tLoss 5.88 (5.978)\tInvT  18.55 ( 18.56)\tAcc@1  26.66 ( 24.91)\tAcc@3  44.14 ( 42.84)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:53:00,019 INFO] Epoch: [4][ 80/531]\tLoss 6.179 (5.98)\tInvT  18.54 ( 18.55)\tAcc@1  23.05 ( 24.83)\tAcc@3  40.43 ( 42.90)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:53:35,266 INFO] Epoch: [4][100/531]\tLoss 5.875 (5.983)\tInvT  18.53 ( 18.55)\tAcc@1  24.61 ( 24.87)\tAcc@3  42.87 ( 42.92)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:54:10,555 INFO] Epoch: [4][120/531]\tLoss 6.158 (5.979)\tInvT  18.53 ( 18.55)\tAcc@1  21.19 ( 24.81)\tAcc@3  39.55 ( 42.94)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:54:45,834 INFO] Epoch: [4][140/531]\tLoss 5.974 (5.983)\tInvT  18.52 ( 18.54)\tAcc@1  27.05 ( 24.80)\tAcc@3  43.85 ( 42.93)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:55:20,671 INFO] Epoch: [4][160/531]\tLoss 5.96 (5.983)\tInvT  18.51 ( 18.54)\tAcc@1  23.34 ( 24.78)\tAcc@3  42.29 ( 42.97)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:55:55,925 INFO] Epoch: [4][180/531]\tLoss 5.913 (5.976)\tInvT  18.51 ( 18.54)\tAcc@1  25.49 ( 24.82)\tAcc@3  44.73 ( 43.05)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:56:31,383 INFO] Epoch: [4][200/531]\tLoss 5.806 (5.975)\tInvT  18.50 ( 18.53)\tAcc@1  26.27 ( 24.86)\tAcc@3  43.85 ( 43.10)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:57:06,882 INFO] Epoch: [4][220/531]\tLoss 5.883 (5.978)\tInvT  18.49 ( 18.53)\tAcc@1  26.27 ( 24.85)\tAcc@3  43.07 ( 43.10)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:57:42,241 INFO] Epoch: [4][240/531]\tLoss 6.003 (5.971)\tInvT  18.49 ( 18.53)\tAcc@1  26.27 ( 24.92)\tAcc@3  45.12 ( 43.20)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:58:17,845 INFO] Epoch: [4][260/531]\tLoss 5.912 (5.967)\tInvT  18.48 ( 18.52)\tAcc@1  24.90 ( 24.95)\tAcc@3  44.63 ( 43.23)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:58:53,632 INFO] Epoch: [4][280/531]\tLoss 5.605 (5.961)\tInvT  18.47 ( 18.52)\tAcc@1  29.88 ( 25.04)\tAcc@3  48.05 ( 43.30)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 13:59:29,167 INFO] Epoch: [4][300/531]\tLoss 5.943 (5.96)\tInvT  18.47 ( 18.52)\tAcc@1  23.83 ( 25.03)\tAcc@3  41.89 ( 43.32)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:00:04,734 INFO] Epoch: [4][320/531]\tLoss 5.816 (5.957)\tInvT  18.46 ( 18.51)\tAcc@1  25.68 ( 25.08)\tAcc@3  44.63 ( 43.40)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:00:39,962 INFO] Epoch: [4][340/531]\tLoss 5.87 (5.953)\tInvT  18.46 ( 18.51)\tAcc@1  24.41 ( 25.12)\tAcc@3  44.04 ( 43.48)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:01:15,584 INFO] Epoch: [4][360/531]\tLoss 5.969 (5.946)\tInvT  18.45 ( 18.51)\tAcc@1  26.27 ( 25.21)\tAcc@3  44.24 ( 43.57)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:01:51,012 INFO] Epoch: [4][380/531]\tLoss 6.151 (5.943)\tInvT  18.44 ( 18.50)\tAcc@1  26.07 ( 25.25)\tAcc@3  43.36 ( 43.62)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:02:26,455 INFO] Epoch: [4][400/531]\tLoss 5.734 (5.94)\tInvT  18.44 ( 18.50)\tAcc@1  27.05 ( 25.29)\tAcc@3  46.88 ( 43.67)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:03:01,876 INFO] Epoch: [4][420/531]\tLoss 5.559 (5.934)\tInvT  18.43 ( 18.50)\tAcc@1  28.32 ( 25.33)\tAcc@3  46.09 ( 43.74)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:03:37,433 INFO] Epoch: [4][440/531]\tLoss 5.651 (5.931)\tInvT  18.43 ( 18.49)\tAcc@1  28.03 ( 25.39)\tAcc@3  46.88 ( 43.80)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:04:12,761 INFO] Epoch: [4][460/531]\tLoss 5.861 (5.927)\tInvT  18.42 ( 18.49)\tAcc@1  27.05 ( 25.43)\tAcc@3  45.12 ( 43.86)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:04:48,338 INFO] Epoch: [4][480/531]\tLoss 5.624 (5.922)\tInvT  18.42 ( 18.49)\tAcc@1  28.42 ( 25.47)\tAcc@3  47.75 ( 43.91)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:05:23,509 INFO] Epoch: [4][500/531]\tLoss 5.622 (5.916)\tInvT  18.41 ( 18.49)\tAcc@1  27.34 ( 25.52)\tAcc@3  46.19 ( 43.98)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:05:58,747 INFO] Epoch: [4][520/531]\tLoss 5.65 (5.913)\tInvT  18.41 ( 18.48)\tAcc@1  28.52 ( 25.57)\tAcc@3  47.95 ( 44.03)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:06:16,391 INFO] Learning rate: 2.7055352055352055e-05\n",
      "[2022-07-08 14:06:40,495 INFO] Epoch 4, valid metric: {\"Acc@1\": 22.985, \"Acc@3\": 34.728, \"loss\": 3.553}\n",
      "[2022-07-08 14:06:47,110 INFO] Epoch: [5][  0/531]\tLoss 5.461 (5.461)\tInvT  18.40 ( 18.40)\tAcc@1  30.37 ( 30.37)\tAcc@3  49.61 ( 49.61)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:07:22,298 INFO] Epoch: [5][ 20/531]\tLoss 5.78 (5.609)\tInvT  18.40 ( 18.40)\tAcc@1  28.12 ( 28.81)\tAcc@3  45.61 ( 47.85)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:07:57,691 INFO] Epoch: [5][ 40/531]\tLoss 5.559 (5.589)\tInvT  18.40 ( 18.40)\tAcc@1  29.88 ( 28.76)\tAcc@3  50.68 ( 48.15)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:08:32,821 INFO] Epoch: [5][ 60/531]\tLoss 5.645 (5.594)\tInvT  18.39 ( 18.40)\tAcc@1  28.32 ( 28.80)\tAcc@3  46.09 ( 48.03)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:09:08,230 INFO] Epoch: [5][ 80/531]\tLoss 5.611 (5.59)\tInvT  18.39 ( 18.40)\tAcc@1  29.00 ( 28.79)\tAcc@3  48.63 ( 48.05)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:09:43,575 INFO] Epoch: [5][100/531]\tLoss 5.44 (5.591)\tInvT  18.39 ( 18.39)\tAcc@1  30.66 ( 28.74)\tAcc@3  49.71 ( 48.06)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:10:18,760 INFO] Epoch: [5][120/531]\tLoss 5.632 (5.586)\tInvT  18.38 ( 18.39)\tAcc@1  29.49 ( 28.79)\tAcc@3  46.00 ( 48.09)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:10:54,148 INFO] Epoch: [5][140/531]\tLoss 5.638 (5.581)\tInvT  18.38 ( 18.39)\tAcc@1  27.25 ( 28.80)\tAcc@3  47.46 ( 48.16)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:11:29,423 INFO] Epoch: [5][160/531]\tLoss 5.737 (5.576)\tInvT  18.38 ( 18.39)\tAcc@1  27.64 ( 28.85)\tAcc@3  47.07 ( 48.23)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:12:04,644 INFO] Epoch: [5][180/531]\tLoss 5.523 (5.576)\tInvT  18.38 ( 18.39)\tAcc@1  31.45 ( 28.89)\tAcc@3  48.83 ( 48.26)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:12:39,909 INFO] Epoch: [5][200/531]\tLoss 5.438 (5.577)\tInvT  18.37 ( 18.39)\tAcc@1  31.05 ( 28.89)\tAcc@3  49.12 ( 48.27)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:13:15,210 INFO] Epoch: [5][220/531]\tLoss 5.524 (5.574)\tInvT  18.37 ( 18.39)\tAcc@1  29.49 ( 28.96)\tAcc@3  47.85 ( 48.33)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:13:50,671 INFO] Epoch: [5][240/531]\tLoss 5.514 (5.576)\tInvT  18.37 ( 18.38)\tAcc@1  30.86 ( 28.95)\tAcc@3  48.34 ( 48.32)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:14:25,655 INFO] Epoch: [5][260/531]\tLoss 5.472 (5.571)\tInvT  18.36 ( 18.38)\tAcc@1  28.91 ( 29.00)\tAcc@3  48.73 ( 48.39)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:15:00,896 INFO] Epoch: [5][280/531]\tLoss 5.407 (5.57)\tInvT  18.36 ( 18.38)\tAcc@1  29.49 ( 29.03)\tAcc@3  49.80 ( 48.40)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:15:36,487 INFO] Epoch: [5][300/531]\tLoss 5.68 (5.568)\tInvT  18.36 ( 18.38)\tAcc@1  29.79 ( 29.06)\tAcc@3  47.95 ( 48.43)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:16:12,068 INFO] Epoch: [5][320/531]\tLoss 5.58 (5.565)\tInvT  18.36 ( 18.38)\tAcc@1  28.61 ( 29.09)\tAcc@3  47.46 ( 48.47)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:16:47,657 INFO] Epoch: [5][340/531]\tLoss 5.415 (5.562)\tInvT  18.35 ( 18.38)\tAcc@1  31.54 ( 29.14)\tAcc@3  49.32 ( 48.50)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:17:23,179 INFO] Epoch: [5][360/531]\tLoss 5.503 (5.56)\tInvT  18.35 ( 18.38)\tAcc@1  29.79 ( 29.17)\tAcc@3  49.41 ( 48.52)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:17:58,651 INFO] Epoch: [5][380/531]\tLoss 5.541 (5.558)\tInvT  18.35 ( 18.37)\tAcc@1  30.76 ( 29.20)\tAcc@3  50.49 ( 48.57)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:18:34,020 INFO] Epoch: [5][400/531]\tLoss 5.518 (5.557)\tInvT  18.35 ( 18.37)\tAcc@1  30.18 ( 29.24)\tAcc@3  48.54 ( 48.60)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:19:09,136 INFO] Epoch: [5][420/531]\tLoss 5.504 (5.553)\tInvT  18.35 ( 18.37)\tAcc@1  30.47 ( 29.30)\tAcc@3  47.95 ( 48.63)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:19:44,690 INFO] Epoch: [5][440/531]\tLoss 5.42 (5.55)\tInvT  18.34 ( 18.37)\tAcc@1  31.05 ( 29.33)\tAcc@3  51.27 ( 48.67)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:20:20,250 INFO] Epoch: [5][460/531]\tLoss 5.644 (5.551)\tInvT  18.34 ( 18.37)\tAcc@1  29.30 ( 29.34)\tAcc@3  49.02 ( 48.68)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:20:56,031 INFO] Epoch: [5][480/531]\tLoss 5.544 (5.549)\tInvT  18.34 ( 18.37)\tAcc@1  28.81 ( 29.37)\tAcc@3  49.12 ( 48.71)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:21:31,366 INFO] Epoch: [5][500/531]\tLoss 5.598 (5.547)\tInvT  18.34 ( 18.37)\tAcc@1  28.52 ( 29.40)\tAcc@3  49.32 ( 48.74)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:22:06,932 INFO] Epoch: [5][520/531]\tLoss 5.478 (5.545)\tInvT  18.33 ( 18.37)\tAcc@1  29.30 ( 29.42)\tAcc@3  49.61 ( 48.75)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:22:24,170 INFO] Learning rate: 2.1652421652421653e-05\n",
      "[2022-07-08 14:22:48,457 INFO] Epoch 5, valid metric: {\"Acc@1\": 23.787, \"Acc@3\": 35.994, \"loss\": 3.484}\n",
      "[2022-07-08 14:22:52,042 INFO] Delete old checkpoint ./checkpoint/fb15k237/checkpoint_epoch0.mdl\n",
      "[2022-07-08 14:22:55,478 INFO] Epoch: [6][  0/531]\tLoss 5.086 (5.086)\tInvT  18.33 ( 18.33)\tAcc@1  32.13 ( 32.13)\tAcc@3  51.86 ( 51.86)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:23:30,823 INFO] Epoch: [6][ 20/531]\tLoss 5.043 (5.226)\tInvT  18.33 ( 18.33)\tAcc@1  33.98 ( 32.37)\tAcc@3  55.18 ( 52.23)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:24:06,383 INFO] Epoch: [6][ 40/531]\tLoss 5.189 (5.234)\tInvT  18.33 ( 18.33)\tAcc@1  31.54 ( 32.33)\tAcc@3  53.32 ( 52.57)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:24:42,010 INFO] Epoch: [6][ 60/531]\tLoss 5.627 (5.237)\tInvT  18.34 ( 18.33)\tAcc@1  28.81 ( 32.44)\tAcc@3  47.56 ( 52.64)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:25:17,465 INFO] Epoch: [6][ 80/531]\tLoss 5.474 (5.243)\tInvT  18.34 ( 18.33)\tAcc@1  29.10 ( 32.36)\tAcc@3  50.59 ( 52.59)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:25:52,694 INFO] Epoch: [6][100/531]\tLoss 5.183 (5.257)\tInvT  18.34 ( 18.33)\tAcc@1  32.42 ( 32.35)\tAcc@3  52.64 ( 52.42)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:26:27,826 INFO] Epoch: [6][120/531]\tLoss 5.45 (5.254)\tInvT  18.34 ( 18.34)\tAcc@1  32.03 ( 32.42)\tAcc@3  50.39 ( 52.46)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:27:03,608 INFO] Epoch: [6][140/531]\tLoss 5.204 (5.261)\tInvT  18.34 ( 18.34)\tAcc@1  33.30 ( 32.36)\tAcc@3  52.44 ( 52.40)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:27:39,013 INFO] Epoch: [6][160/531]\tLoss 5.414 (5.267)\tInvT  18.34 ( 18.34)\tAcc@1  30.86 ( 32.36)\tAcc@3  50.29 ( 52.34)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:28:14,157 INFO] Epoch: [6][180/531]\tLoss 5.324 (5.264)\tInvT  18.34 ( 18.34)\tAcc@1  33.89 ( 32.39)\tAcc@3  52.73 ( 52.37)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:28:49,544 INFO] Epoch: [6][200/531]\tLoss 5.371 (5.264)\tInvT  18.34 ( 18.34)\tAcc@1  32.62 ( 32.45)\tAcc@3  51.86 ( 52.37)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:29:24,952 INFO] Epoch: [6][220/531]\tLoss 5.237 (5.262)\tInvT  18.34 ( 18.34)\tAcc@1  33.50 ( 32.48)\tAcc@3  52.05 ( 52.38)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:30:00,362 INFO] Epoch: [6][240/531]\tLoss 5.524 (5.259)\tInvT  18.34 ( 18.34)\tAcc@1  32.32 ( 32.51)\tAcc@3  51.07 ( 52.42)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:30:35,353 INFO] Epoch: [6][260/531]\tLoss 5.203 (5.257)\tInvT  18.34 ( 18.34)\tAcc@1  32.91 ( 32.54)\tAcc@3  53.42 ( 52.45)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:31:10,820 INFO] Epoch: [6][280/531]\tLoss 5.27 (5.256)\tInvT  18.34 ( 18.34)\tAcc@1  33.20 ( 32.54)\tAcc@3  51.76 ( 52.43)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:31:46,010 INFO] Epoch: [6][300/531]\tLoss 5.153 (5.252)\tInvT  18.35 ( 18.34)\tAcc@1  34.67 ( 32.59)\tAcc@3  54.10 ( 52.47)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:32:21,703 INFO] Epoch: [6][320/531]\tLoss 5.269 (5.254)\tInvT  18.35 ( 18.34)\tAcc@1  32.52 ( 32.59)\tAcc@3  52.34 ( 52.47)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:32:56,686 INFO] Epoch: [6][340/531]\tLoss 5.376 (5.255)\tInvT  18.35 ( 18.34)\tAcc@1  31.15 ( 32.59)\tAcc@3  50.49 ( 52.47)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:33:32,209 INFO] Epoch: [6][360/531]\tLoss 5.288 (5.256)\tInvT  18.35 ( 18.34)\tAcc@1  34.08 ( 32.60)\tAcc@3  53.52 ( 52.47)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:34:08,110 INFO] Epoch: [6][380/531]\tLoss 5.162 (5.257)\tInvT  18.35 ( 18.34)\tAcc@1  32.13 ( 32.60)\tAcc@3  53.91 ( 52.46)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:34:43,642 INFO] Epoch: [6][400/531]\tLoss 5.253 (5.257)\tInvT  18.35 ( 18.34)\tAcc@1  33.11 ( 32.59)\tAcc@3  53.42 ( 52.48)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:35:19,463 INFO] Epoch: [6][420/531]\tLoss 5.163 (5.256)\tInvT  18.35 ( 18.34)\tAcc@1  33.50 ( 32.61)\tAcc@3  53.42 ( 52.48)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:35:54,852 INFO] Epoch: [6][440/531]\tLoss 5.228 (5.256)\tInvT  18.35 ( 18.34)\tAcc@1  31.93 ( 32.61)\tAcc@3  54.49 ( 52.51)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:36:30,141 INFO] Epoch: [6][460/531]\tLoss 5.106 (5.254)\tInvT  18.35 ( 18.34)\tAcc@1  31.64 ( 32.63)\tAcc@3  52.54 ( 52.54)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:37:05,423 INFO] Epoch: [6][480/531]\tLoss 5.193 (5.253)\tInvT  18.35 ( 18.34)\tAcc@1  32.91 ( 32.64)\tAcc@3  54.98 ( 52.54)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:37:40,445 INFO] Epoch: [6][500/531]\tLoss 5.145 (5.252)\tInvT  18.35 ( 18.34)\tAcc@1  34.38 ( 32.64)\tAcc@3  53.03 ( 52.56)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:38:16,067 INFO] Epoch: [6][520/531]\tLoss 5.194 (5.252)\tInvT  18.35 ( 18.34)\tAcc@1  33.89 ( 32.66)\tAcc@3  54.79 ( 52.57)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:38:33,855 INFO] Learning rate: 1.624949124949125e-05\n",
      "[2022-07-08 14:38:58,033 INFO] Epoch 6, valid metric: {\"Acc@1\": 24.194, \"Acc@3\": 36.593, \"loss\": 3.456}\n",
      "[2022-07-08 14:39:01,592 INFO] Delete old checkpoint ./checkpoint/fb15k237/checkpoint_epoch1.mdl\n",
      "[2022-07-08 14:39:05,062 INFO] Epoch: [7][  0/531]\tLoss 4.982 (4.982)\tInvT  18.35 ( 18.35)\tAcc@1  34.96 ( 34.96)\tAcc@3  57.32 ( 57.32)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:39:40,488 INFO] Epoch: [7][ 20/531]\tLoss 5.053 (5.038)\tInvT  18.36 ( 18.36)\tAcc@1  35.35 ( 34.82)\tAcc@3  53.22 ( 55.06)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:40:15,609 INFO] Epoch: [7][ 40/531]\tLoss 4.994 (5.023)\tInvT  18.36 ( 18.36)\tAcc@1  35.55 ( 35.17)\tAcc@3  56.35 ( 55.33)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:40:50,950 INFO] Epoch: [7][ 60/531]\tLoss 4.888 (5.014)\tInvT  18.37 ( 18.36)\tAcc@1  35.55 ( 35.23)\tAcc@3  58.98 ( 55.52)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:41:26,415 INFO] Epoch: [7][ 80/531]\tLoss 5.075 (5.015)\tInvT  18.37 ( 18.36)\tAcc@1  35.94 ( 35.19)\tAcc@3  55.08 ( 55.58)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:42:01,781 INFO] Epoch: [7][100/531]\tLoss 4.987 (5.013)\tInvT  18.37 ( 18.36)\tAcc@1  36.04 ( 35.27)\tAcc@3  57.32 ( 55.56)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:42:37,025 INFO] Epoch: [7][120/531]\tLoss 5.042 (5.018)\tInvT  18.38 ( 18.36)\tAcc@1  34.47 ( 35.30)\tAcc@3  53.71 ( 55.51)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:43:12,240 INFO] Epoch: [7][140/531]\tLoss 4.933 (5.022)\tInvT  18.38 ( 18.37)\tAcc@1  35.94 ( 35.27)\tAcc@3  57.03 ( 55.48)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:43:47,634 INFO] Epoch: [7][160/531]\tLoss 5.014 (5.023)\tInvT  18.38 ( 18.37)\tAcc@1  35.06 ( 35.28)\tAcc@3  54.39 ( 55.46)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:44:22,497 INFO] Epoch: [7][180/531]\tLoss 4.78 (5.023)\tInvT  18.39 ( 18.37)\tAcc@1  38.09 ( 35.30)\tAcc@3  58.89 ( 55.45)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:44:58,007 INFO] Epoch: [7][200/531]\tLoss 5.108 (5.021)\tInvT  18.39 ( 18.37)\tAcc@1  34.57 ( 35.29)\tAcc@3  53.32 ( 55.49)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:45:33,594 INFO] Epoch: [7][220/531]\tLoss 5.103 (5.022)\tInvT  18.39 ( 18.37)\tAcc@1  34.96 ( 35.29)\tAcc@3  52.93 ( 55.45)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:46:09,039 INFO] Epoch: [7][240/531]\tLoss 5.193 (5.023)\tInvT  18.40 ( 18.38)\tAcc@1  34.47 ( 35.29)\tAcc@3  53.42 ( 55.45)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:46:44,615 INFO] Epoch: [7][260/531]\tLoss 5.057 (5.022)\tInvT  18.40 ( 18.38)\tAcc@1  35.16 ( 35.29)\tAcc@3  55.47 ( 55.45)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:47:19,716 INFO] Epoch: [7][280/531]\tLoss 5.07 (5.023)\tInvT  18.40 ( 18.38)\tAcc@1  33.89 ( 35.29)\tAcc@3  54.39 ( 55.44)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:47:55,371 INFO] Epoch: [7][300/531]\tLoss 4.915 (5.021)\tInvT  18.41 ( 18.38)\tAcc@1  36.33 ( 35.32)\tAcc@3  56.74 ( 55.45)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:48:30,701 INFO] Epoch: [7][320/531]\tLoss 4.867 (5.019)\tInvT  18.41 ( 18.38)\tAcc@1  36.62 ( 35.32)\tAcc@3  57.23 ( 55.45)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:49:06,300 INFO] Epoch: [7][340/531]\tLoss 5.034 (5.02)\tInvT  18.41 ( 18.38)\tAcc@1  35.74 ( 35.28)\tAcc@3  54.79 ( 55.44)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:49:41,707 INFO] Epoch: [7][360/531]\tLoss 5.09 (5.02)\tInvT  18.42 ( 18.39)\tAcc@1  34.47 ( 35.29)\tAcc@3  55.27 ( 55.45)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:50:17,468 INFO] Epoch: [7][380/531]\tLoss 4.873 (5.021)\tInvT  18.42 ( 18.39)\tAcc@1  37.99 ( 35.28)\tAcc@3  58.20 ( 55.44)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:50:52,820 INFO] Epoch: [7][400/531]\tLoss 5.222 (5.022)\tInvT  18.42 ( 18.39)\tAcc@1  33.79 ( 35.28)\tAcc@3  51.46 ( 55.43)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:51:28,355 INFO] Epoch: [7][420/531]\tLoss 5.117 (5.023)\tInvT  18.42 ( 18.39)\tAcc@1  34.28 ( 35.28)\tAcc@3  55.96 ( 55.42)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:52:03,730 INFO] Epoch: [7][440/531]\tLoss 4.876 (5.019)\tInvT  18.43 ( 18.39)\tAcc@1  37.50 ( 35.33)\tAcc@3  56.64 ( 55.46)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:52:38,717 INFO] Epoch: [7][460/531]\tLoss 4.903 (5.018)\tInvT  18.43 ( 18.39)\tAcc@1  38.38 ( 35.34)\tAcc@3  58.01 ( 55.48)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:53:13,980 INFO] Epoch: [7][480/531]\tLoss 4.954 (5.017)\tInvT  18.43 ( 18.40)\tAcc@1  35.45 ( 35.35)\tAcc@3  55.66 ( 55.50)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:53:49,752 INFO] Epoch: [7][500/531]\tLoss 5.027 (5.017)\tInvT  18.43 ( 18.40)\tAcc@1  33.20 ( 35.34)\tAcc@3  55.37 ( 55.51)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:54:24,936 INFO] Epoch: [7][520/531]\tLoss 4.967 (5.016)\tInvT  18.44 ( 18.40)\tAcc@1  35.45 ( 35.35)\tAcc@3  56.64 ( 55.51)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:54:42,478 INFO] Learning rate: 1.0846560846560846e-05\n",
      "[2022-07-08 14:55:07,035 INFO] Epoch 7, valid metric: {\"Acc@1\": 24.562, \"Acc@3\": 37.012, \"loss\": 3.448}\n",
      "[2022-07-08 14:55:10,558 INFO] Delete old checkpoint ./checkpoint/fb15k237/checkpoint_epoch2.mdl\n",
      "[2022-07-08 14:55:13,948 INFO] Epoch: [8][  0/531]\tLoss 4.751 (4.751)\tInvT  18.44 ( 18.44)\tAcc@1  40.04 ( 40.04)\tAcc@3  59.96 ( 59.96)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:55:49,461 INFO] Epoch: [8][ 20/531]\tLoss 4.746 (4.79)\tInvT  18.44 ( 18.44)\tAcc@1  37.70 ( 38.19)\tAcc@3  58.50 ( 57.99)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:56:24,480 INFO] Epoch: [8][ 40/531]\tLoss 4.938 (4.803)\tInvT  18.45 ( 18.44)\tAcc@1  36.62 ( 37.90)\tAcc@3  57.81 ( 58.01)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:56:59,816 INFO] Epoch: [8][ 60/531]\tLoss 4.709 (4.804)\tInvT  18.45 ( 18.45)\tAcc@1  38.38 ( 37.90)\tAcc@3  58.11 ( 58.08)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:57:35,215 INFO] Epoch: [8][ 80/531]\tLoss 5.011 (4.807)\tInvT  18.46 ( 18.45)\tAcc@1  35.84 ( 37.77)\tAcc@3  55.96 ( 58.06)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:58:10,508 INFO] Epoch: [8][100/531]\tLoss 4.889 (4.815)\tInvT  18.46 ( 18.45)\tAcc@1  37.30 ( 37.72)\tAcc@3  58.30 ( 57.99)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:58:46,039 INFO] Epoch: [8][120/531]\tLoss 4.826 (4.821)\tInvT  18.47 ( 18.45)\tAcc@1  38.96 ( 37.66)\tAcc@3  58.59 ( 57.91)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:59:21,659 INFO] Epoch: [8][140/531]\tLoss 4.825 (4.83)\tInvT  18.47 ( 18.45)\tAcc@1  38.18 ( 37.59)\tAcc@3  57.62 ( 57.79)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 14:59:57,149 INFO] Epoch: [8][160/531]\tLoss 4.744 (4.824)\tInvT  18.47 ( 18.46)\tAcc@1  37.50 ( 37.69)\tAcc@3  57.81 ( 57.91)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:00:32,872 INFO] Epoch: [8][180/531]\tLoss 4.833 (4.83)\tInvT  18.48 ( 18.46)\tAcc@1  35.64 ( 37.60)\tAcc@3  57.62 ( 57.81)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:01:08,484 INFO] Epoch: [8][200/531]\tLoss 4.588 (4.822)\tInvT  18.48 ( 18.46)\tAcc@1  38.09 ( 37.67)\tAcc@3  59.67 ( 57.87)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:01:43,973 INFO] Epoch: [8][220/531]\tLoss 5.108 (4.821)\tInvT  18.48 ( 18.46)\tAcc@1  34.96 ( 37.67)\tAcc@3  54.88 ( 57.85)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:02:19,055 INFO] Epoch: [8][240/531]\tLoss 4.797 (4.822)\tInvT  18.49 ( 18.46)\tAcc@1  39.06 ( 37.66)\tAcc@3  57.71 ( 57.84)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:02:54,105 INFO] Epoch: [8][260/531]\tLoss 4.791 (4.821)\tInvT  18.49 ( 18.47)\tAcc@1  39.55 ( 37.67)\tAcc@3  59.77 ( 57.86)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:03:29,317 INFO] Epoch: [8][280/531]\tLoss 4.722 (4.826)\tInvT  18.49 ( 18.47)\tAcc@1  37.70 ( 37.60)\tAcc@3  58.20 ( 57.76)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:04:04,901 INFO] Epoch: [8][300/531]\tLoss 4.679 (4.827)\tInvT  18.50 ( 18.47)\tAcc@1  40.53 ( 37.59)\tAcc@3  58.79 ( 57.77)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:04:40,192 INFO] Epoch: [8][320/531]\tLoss 4.988 (4.831)\tInvT  18.50 ( 18.47)\tAcc@1  36.13 ( 37.53)\tAcc@3  57.71 ( 57.72)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:05:15,501 INFO] Epoch: [8][340/531]\tLoss 4.926 (4.833)\tInvT  18.50 ( 18.47)\tAcc@1  37.30 ( 37.53)\tAcc@3  57.03 ( 57.70)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:05:51,021 INFO] Epoch: [8][360/531]\tLoss 4.977 (4.833)\tInvT  18.50 ( 18.47)\tAcc@1  37.01 ( 37.54)\tAcc@3  56.25 ( 57.70)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:06:26,529 INFO] Epoch: [8][380/531]\tLoss 4.9 (4.832)\tInvT  18.51 ( 18.48)\tAcc@1  37.30 ( 37.54)\tAcc@3  57.32 ( 57.72)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:07:02,158 INFO] Epoch: [8][400/531]\tLoss 4.803 (4.833)\tInvT  18.51 ( 18.48)\tAcc@1  36.91 ( 37.56)\tAcc@3  57.13 ( 57.72)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:07:37,666 INFO] Epoch: [8][420/531]\tLoss 4.804 (4.831)\tInvT  18.51 ( 18.48)\tAcc@1  38.77 ( 37.55)\tAcc@3  59.96 ( 57.76)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:08:13,198 INFO] Epoch: [8][440/531]\tLoss 4.925 (4.831)\tInvT  18.51 ( 18.48)\tAcc@1  36.52 ( 37.57)\tAcc@3  57.52 ( 57.78)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:08:48,810 INFO] Epoch: [8][460/531]\tLoss 4.877 (4.83)\tInvT  18.52 ( 18.48)\tAcc@1  38.09 ( 37.59)\tAcc@3  58.89 ( 57.80)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:09:24,317 INFO] Epoch: [8][480/531]\tLoss 4.759 (4.827)\tInvT  18.52 ( 18.48)\tAcc@1  39.16 ( 37.60)\tAcc@3  57.23 ( 57.82)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:10:00,084 INFO] Epoch: [8][500/531]\tLoss 4.943 (4.828)\tInvT  18.52 ( 18.49)\tAcc@1  36.43 ( 37.61)\tAcc@3  57.32 ( 57.84)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:10:35,611 INFO] Epoch: [8][520/531]\tLoss 4.886 (4.829)\tInvT  18.52 ( 18.49)\tAcc@1  35.94 ( 37.60)\tAcc@3  55.86 ( 57.83)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:10:52,801 INFO] Learning rate: 5.4436304436304435e-06\n",
      "[2022-07-08 15:11:17,031 INFO] Epoch 8, valid metric: {\"Acc@1\": 24.75, \"Acc@3\": 37.556, \"loss\": 3.426}\n",
      "[2022-07-08 15:11:20,578 INFO] Delete old checkpoint ./checkpoint/fb15k237/checkpoint_epoch3.mdl\n",
      "[2022-07-08 15:11:24,121 INFO] Epoch: [9][  0/531]\tLoss 4.683 (4.683)\tInvT  18.53 ( 18.53)\tAcc@1  39.36 ( 39.36)\tAcc@3  60.94 ( 60.94)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:11:59,396 INFO] Epoch: [9][ 20/531]\tLoss 4.63 (4.683)\tInvT  18.53 ( 18.53)\tAcc@1  40.92 ( 39.55)\tAcc@3  59.57 ( 59.86)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:12:35,014 INFO] Epoch: [9][ 40/531]\tLoss 4.574 (4.688)\tInvT  18.53 ( 18.53)\tAcc@1  39.84 ( 39.29)\tAcc@3  59.57 ( 59.66)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:13:10,293 INFO] Epoch: [9][ 60/531]\tLoss 4.591 (4.679)\tInvT  18.53 ( 18.53)\tAcc@1  38.57 ( 39.26)\tAcc@3  60.64 ( 59.85)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:13:45,488 INFO] Epoch: [9][ 80/531]\tLoss 4.727 (4.679)\tInvT  18.54 ( 18.53)\tAcc@1  39.94 ( 39.30)\tAcc@3  59.38 ( 59.80)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:14:20,699 INFO] Epoch: [9][100/531]\tLoss 4.702 (4.684)\tInvT  18.54 ( 18.53)\tAcc@1  39.36 ( 39.32)\tAcc@3  58.40 ( 59.73)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:14:55,694 INFO] Epoch: [9][120/531]\tLoss 4.704 (4.687)\tInvT  18.54 ( 18.53)\tAcc@1  41.50 ( 39.28)\tAcc@3  59.67 ( 59.69)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:15:31,020 INFO] Epoch: [9][140/531]\tLoss 4.754 (4.681)\tInvT  18.54 ( 18.53)\tAcc@1  38.38 ( 39.35)\tAcc@3  58.20 ( 59.74)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:16:06,847 INFO] Epoch: [9][160/531]\tLoss 4.895 (4.683)\tInvT  18.54 ( 18.53)\tAcc@1  37.30 ( 39.36)\tAcc@3  55.96 ( 59.72)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:16:42,254 INFO] Epoch: [9][180/531]\tLoss 4.485 (4.68)\tInvT  18.55 ( 18.54)\tAcc@1  40.04 ( 39.40)\tAcc@3  61.43 ( 59.76)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:17:17,735 INFO] Epoch: [9][200/531]\tLoss 4.758 (4.688)\tInvT  18.55 ( 18.54)\tAcc@1  37.79 ( 39.30)\tAcc@3  60.16 ( 59.69)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:17:53,470 INFO] Epoch: [9][220/531]\tLoss 4.538 (4.688)\tInvT  18.55 ( 18.54)\tAcc@1  40.62 ( 39.28)\tAcc@3  61.33 ( 59.69)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:18:28,419 INFO] Epoch: [9][240/531]\tLoss 4.577 (4.69)\tInvT  18.55 ( 18.54)\tAcc@1  41.11 ( 39.25)\tAcc@3  58.79 ( 59.67)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:19:04,073 INFO] Epoch: [9][260/531]\tLoss 4.694 (4.688)\tInvT  18.55 ( 18.54)\tAcc@1  42.09 ( 39.28)\tAcc@3  59.47 ( 59.70)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:19:39,438 INFO] Epoch: [9][280/531]\tLoss 4.634 (4.686)\tInvT  18.55 ( 18.54)\tAcc@1  39.26 ( 39.30)\tAcc@3  61.23 ( 59.71)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:20:15,243 INFO] Epoch: [9][300/531]\tLoss 4.658 (4.685)\tInvT  18.55 ( 18.54)\tAcc@1  40.14 ( 39.29)\tAcc@3  58.30 ( 59.70)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:20:50,742 INFO] Epoch: [9][320/531]\tLoss 4.604 (4.688)\tInvT  18.55 ( 18.54)\tAcc@1  41.31 ( 39.28)\tAcc@3  59.86 ( 59.65)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:21:25,798 INFO] Epoch: [9][340/531]\tLoss 4.759 (4.689)\tInvT  18.56 ( 18.54)\tAcc@1  39.55 ( 39.30)\tAcc@3  59.67 ( 59.64)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:22:01,280 INFO] Epoch: [9][360/531]\tLoss 4.86 (4.69)\tInvT  18.56 ( 18.54)\tAcc@1  37.89 ( 39.31)\tAcc@3  56.25 ( 59.62)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:22:36,864 INFO] Epoch: [9][380/531]\tLoss 4.323 (4.69)\tInvT  18.56 ( 18.54)\tAcc@1  41.31 ( 39.33)\tAcc@3  65.14 ( 59.64)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:23:12,288 INFO] Epoch: [9][400/531]\tLoss 4.682 (4.691)\tInvT  18.56 ( 18.54)\tAcc@1  39.26 ( 39.32)\tAcc@3  59.18 ( 59.63)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:23:47,527 INFO] Epoch: [9][420/531]\tLoss 4.539 (4.69)\tInvT  18.56 ( 18.55)\tAcc@1  41.70 ( 39.34)\tAcc@3  61.62 ( 59.61)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:24:23,139 INFO] Epoch: [9][440/531]\tLoss 4.736 (4.688)\tInvT  18.56 ( 18.55)\tAcc@1  39.45 ( 39.33)\tAcc@3  57.42 ( 59.61)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:24:58,821 INFO] Epoch: [9][460/531]\tLoss 4.657 (4.687)\tInvT  18.56 ( 18.55)\tAcc@1  40.14 ( 39.34)\tAcc@3  60.45 ( 59.63)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:25:34,578 INFO] Epoch: [9][480/531]\tLoss 4.765 (4.686)\tInvT  18.56 ( 18.55)\tAcc@1  40.53 ( 39.37)\tAcc@3  60.16 ( 59.63)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:26:09,675 INFO] Epoch: [9][500/531]\tLoss 4.728 (4.686)\tInvT  18.56 ( 18.55)\tAcc@1  38.77 ( 39.38)\tAcc@3  58.50 ( 59.63)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:26:45,003 INFO] Epoch: [9][520/531]\tLoss 4.676 (4.687)\tInvT  18.56 ( 18.55)\tAcc@1  38.77 ( 39.37)\tAcc@3  59.18 ( 59.62)\tHR mean norm   1.00 (  1.00)\tTail mean norm   1.00 (  1.00)\n",
      "[2022-07-08 15:27:02,565 INFO] Learning rate: 4.07000407000407e-08\n",
      "[2022-07-08 15:27:26,803 INFO] Epoch 9, valid metric: {\"Acc@1\": 24.876, \"Acc@3\": 37.482, \"loss\": 3.425}\n",
      "[2022-07-08 15:27:30,342 INFO] Delete old checkpoint ./checkpoint/fb15k237/checkpoint_epoch4.mdl\n"
     ]
    }
   ],
   "source": [
    "!OUTPUT_DIR=./checkpoint/fb15k237/ bash scripts/train_fb.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25062fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ set -e\n",
      "+ model_path=bert\n",
      "+ task=WN18RR\n",
      "+ [[ 2 -ge 1 ]]\n",
      "+ [[ ! checkpoint/fb15k237/model_best.mdl == \\-\\-* ]]\n",
      "+ model_path=checkpoint/fb15k237/model_best.mdl\n",
      "+ shift\n",
      "+ [[ 1 -ge 1 ]]\n",
      "+ [[ ! FB15k237 == \\-\\-* ]]\n",
      "+ task=FB15k237\n",
      "+ shift\n",
      "+++ dirname scripts/eval.sh\n",
      "++ cd scripts\n",
      "++ cd ..\n",
      "++ pwd\n",
      "+ DIR=/workspace/SimKGC\n",
      "+ echo 'working directory: /workspace/SimKGC'\n",
      "working directory: /workspace/SimKGC\n",
      "+ '[' -z '' ']'\n",
      "+ DATA_DIR=/workspace/SimKGC/data/FB15k237\n",
      "+ test_path=/workspace/SimKGC/data/FB15k237/test.txt.json\n",
      "+ [[ 0 -ge 1 ]]\n",
      "+ neighbor_weight=0.05\n",
      "+ rerank_n_hop=2\n",
      "+ '[' FB15k237 = WN18RR ']'\n",
      "+ '[' FB15k237 = wiki5m_ind ']'\n",
      "+ python3 -u evaluate.py --task FB15k237 --is-test --eval-model-path checkpoint/fb15k237/model_best.mdl --neighbor-weight 0.05 --rerank-n-hop 2 --train-path /workspace/SimKGC/data/FB15k237/train.txt.json --valid-path /workspace/SimKGC/data/FB15k237/test.txt.json --no-desc --no-pretraining\n",
      "[2022-07-08 15:28:22,446 INFO] Load 14541 entities from /workspace/SimKGC/data/FB15k237/entities.json\n",
      "[2022-07-08 15:28:22,501 INFO] Triplets path: ['/workspace/SimKGC/data/FB15k237/train.txt.json', '/workspace/SimKGC/data/FB15k237/valid.txt.json', '/workspace/SimKGC/data/FB15k237/test.txt.json']\n",
      "[2022-07-08 15:28:25,113 INFO] Triplet statistics: 474 relations, 40932 triplets\n",
      "[2022-07-08 15:28:25,535 INFO] Args used in training: {\n",
      "    \"pretrained_model\": \"bert-base-uncased\",\n",
      "    \"task\": \"FB15k237\",\n",
      "    \"train_path\": \"/workspace/SimKGC/data/FB15k237/train.txt.json\",\n",
      "    \"valid_path\": \"/workspace/SimKGC/data/FB15k237/valid.txt.json\",\n",
      "    \"model_dir\": \"./checkpoint/fb15k237/\",\n",
      "    \"warmup\": 400,\n",
      "    \"max_to_keep\": 5,\n",
      "    \"grad_clip\": 10.0,\n",
      "    \"pooling\": \"mean\",\n",
      "    \"dropout\": 0.1,\n",
      "    \"use_amp\": true,\n",
      "    \"t\": 0.05,\n",
      "    \"use_link_graph\": true,\n",
      "    \"eval_every_n_step\": 10000,\n",
      "    \"pre_batch\": 2,\n",
      "    \"pre_batch_weight\": 0.5,\n",
      "    \"additive_margin\": 0.02,\n",
      "    \"finetune_t\": true,\n",
      "    \"max_num_tokens\": 50,\n",
      "    \"use_self_negative\": true,\n",
      "    \"workers\": 4,\n",
      "    \"epochs\": 10,\n",
      "    \"batch_size\": 1024,\n",
      "    \"lr\": 5e-05,\n",
      "    \"lr_scheduler\": \"linear\",\n",
      "    \"weight_decay\": 0.0001,\n",
      "    \"print_freq\": 20,\n",
      "    \"seed\": null,\n",
      "    \"concept_path\": \"\",\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"no_desc\": true,\n",
      "    \"no_pretraining\": true,\n",
      "    \"is_test\": false,\n",
      "    \"rerank_n_hop\": 2,\n",
      "    \"neighbor_weight\": 0.0,\n",
      "    \"eval_model_path\": \"\"\n",
      "}\n",
      "[2022-07-08 15:28:32,846 INFO] Build tokenizer from bert-base-uncased\n",
      "[2022-07-08 15:28:36,922 INFO] Load model from checkpoint/fb15k237/model_best.mdl successfully\n",
      "  0%|                                                    | 0/15 [00:00<?, ?it/s][2022-07-08 15:28:37,072 INFO] Start to build link graph from /workspace/SimKGC/data/FB15k237/train.txt.json\n",
      "[2022-07-08 15:28:37,091 INFO] Start to build link graph from /workspace/SimKGC/data/FB15k237/train.txt.json\n",
      "[2022-07-08 15:28:38,511 INFO] Done build link graph with 14505 nodes\n",
      "[2022-07-08 15:28:38,524 INFO] Done build link graph with 14505 nodes\n",
      "100%|███████████████████████████████████████████| 15/15 [00:05<00:00,  2.76it/s]\n",
      "[2022-07-08 15:28:42,394 INFO] In test mode: True\n",
      "[2022-07-08 15:28:42,449 INFO] Load 20466 examples from /workspace/SimKGC/data/FB15k237/test.txt.json\n",
      "[2022-07-08 15:28:42,581 INFO] Start to build link graph from /workspace/SimKGC/data/FB15k237/train.txt.json\n",
      "[2022-07-08 15:28:43,768 INFO] Done build link graph with 14505 nodes\n",
      "[2022-07-08 15:28:59,543 INFO] predict tensor done, compute metrics...\n",
      "  0%|                                                    | 0/80 [00:00<?, ?it/s][2022-07-08 15:28:59,769 INFO] Start to build link graph from /workspace/SimKGC/data/FB15k237/train.txt.json\n",
      "[2022-07-08 15:29:00,815 INFO] Done build link graph with 14505 nodes\n",
      "100%|███████████████████████████████████████████| 80/80 [03:00<00:00,  2.26s/it]\n",
      "[2022-07-08 15:32:00,187 INFO] forward metrics: {\"mean_rank\": 147.5595, \"mrr\": 0.3424, \"hit@1\": 0.2591, \"hit@3\": 0.366, \"hit@10\": 0.5158}\n",
      "[2022-07-08 15:32:01,710 INFO] Evaluation takes 199.317 seconds\n",
      "[2022-07-08 15:32:01,727 INFO] In test mode: True\n",
      "[2022-07-08 15:32:01,775 INFO] Load 20466 examples from /workspace/SimKGC/data/FB15k237/test.txt.json\n",
      "[2022-07-08 15:32:18,057 INFO] predict tensor done, compute metrics...\n",
      "100%|███████████████████████████████████████████| 80/80 [04:12<00:00,  3.15s/it]\n",
      "[2022-07-08 15:36:30,466 INFO] backward metrics: {\"mean_rank\": 260.4799, \"mrr\": 0.1674, \"hit@1\": 0.0942, \"hit@3\": 0.1786, \"hit@10\": 0.3133}\n",
      "[2022-07-08 15:36:32,188 INFO] Evaluation takes 270.461 seconds\n",
      "[2022-07-08 15:36:32,204 INFO] Averaged metrics: {'mean_rank': 204.0197, 'mrr': 0.2549, 'hit@1': 0.1767, 'hit@3': 0.2723, 'hit@10': 0.4146}\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/eval.sh checkpoint/fb15k237/model_best.mdl FB15k237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5099c2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
